{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3cf7c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"logistic_regression_assignment_ling_cs_581_2026_submission_eight.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3c3cd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"logistic_regression_assignment_ling_cs_581_2026.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe73e27",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q1:**  The code  given in the code help notebook constructs a toy corpus derived from the Brown corpus and trains a logistic regression language model `lrc` on that corpus.   \n",
    "\n",
    "Use the trained model `lrc` to predict the probabilities of the entire training set and use those to to **evaluate** the  model by computing its **perplexity** with respect to the training data.  \n",
    "\n",
    "You should use the definition of perplexity given in our textbook ngrams chapter (see the slides).  Assign the perplexity value to `perp1.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35c60e6b",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "perp1 = 37.206005418592554"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d59dbf26",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1</pre></strong> passed! üíØ</p>"
      ],
      "text/plain": [
       "q1 results: All test cases passed!"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5de6eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q2**:  How many features does the model evaluated in Q1 use?  Assign the answer to `num_features1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e46645ab",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "num_features1 = 508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3706d2e0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2</pre></strong> passed! üåà</p>"
      ],
      "text/plain": [
       "q2 results: All test cases passed!"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01396a78",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q3**: The code  given in the code help notebook calls a function named `prepare_korpus` to construct the 2D array and predicted word sequence (`Y`) used to train the classifier `lrc`.  Modify the call to that function to build a new `korpus` and use it to train a model which does not use trigger words. (Note:  **You do not have to modify the function, just change how it is called.**) The resulting model only uses the previous word to predict the next word.  It is in effect a classic bigram model. Use the bigram model to predict the probabilities of the entire training set and use those to to **evaluate** this new model by computing its **perplexity** with respect to the training data.  Assign the perplexity value to `perp_bigram`. If the model that uses trigger words is better than the bigram model, assign the value `True` to `trigger_model_is_better`.  Otherwise assign the value `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6198bfcd",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "perp_bigram = 44.168512432090985\n",
    "trigger_model_is_better = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5734c4d3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3</pre></strong> passed! üåà</p>"
      ],
      "text/plain": [
       "q3 results: All test cases passed!"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b0a6c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q4**: How many features does the model evaluated in Q3 use? Assign the answer to num_features_bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06464696",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "num_features_bigram = 358 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "466e356d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q4</pre></strong> passed! üçÄ</p>"
      ],
      "text/plain": [
       "q4 results: All test cases passed!"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9abc4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q5**:  Compute the perplexity the model would assign to the corpus if it had no information, that is, if it predicted every  word, including the correct word, with probability `p`, where `p` is uniformly distributed among the target words.   Assign this perplexity to `perp_uniform`. Two notes.  First: the set of **target words**, words that are candidate predictions,  is not the same as the vocabulary, since only a subset of the vocabulary is found in the training data as a possible target word.  Find the `targets` subset and compute your answer accordingly.  Second:  You do not need to train another model to answer this question (and you shouldn't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68cc1ea2",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "perp_uniform = 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7d3332a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q5</pre></strong> passed! üåü</p>"
      ],
      "text/plain": [
       "q5 results: All test cases passed!"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db60aa8a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q6**:  Of all the target words in the data, which is predicted with the greatest **precision**? Use the model with trigger words.  You will need to look at the example using `sklearn.metrics.precision_score` in the code help notebook. You are interested in computing per-class precision scores. Then find the target word with the maximal precision score and assign it to `max_precision_word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99199437",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "max_precision_word = \"time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "020e3dc8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q6</pre></strong> passed! üçÄ</p>"
      ],
      "text/plain": [
       "q6 results: All test cases passed!"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f79f81",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    " This is a gradescope assignment.\n",
    " Write your code and compute your values in the code help NB.\n",
    " Then assign those values to the appropriate variables in the solution cell of each question in this NB.\n",
    " Upload this notebook file ('.ipynb') with your solutions to Canvas.\n",
    " Save your code help NB for discussion, but you do not need to hand it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b8bf1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082bccb6",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 37 < perp1 < 38\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 500 < num_features1 < 550\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 44 < perp_bigram < 45\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> trigger_model_is_better\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 300 < num_features_bigram < 400\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 90 < perp_uniform\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(max_precision_word) == 4\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
