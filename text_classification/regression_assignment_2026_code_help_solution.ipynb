{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8efbf227",
   "metadata": {},
   "source": [
    "# Logistic Regression assignment\n",
    "\n",
    "\n",
    "This is **Code help Notebook** for the Logistic Regression assignment.\n",
    "\n",
    "The goal of this assignment is to introduce you to the idea of a **Maximum Entropy Language Model**,\n",
    "that is, a Logistic Regression (LR) model that tries to predict the next word on the basis of features of the word's linguistic context.  We will look at two versions of the model.  One is a simple bigram model formulated as an LR model (the only context feature is the word immediately preceding the word we are predicting,\n",
    "(which we will call the **target**).  The other is a bigram model augmented with **trigger word features**, words that are known triggers for other words, which may be found arbitrarily far from the target.  The idea of building an LR language model is due to [Rosenfeld (1994)](https://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/roni/papers/me-csl-revised.pdf) and we follow his method for finding trigger words.\n",
    "\n",
    "The code in the section entitled **Preparing the filtered corpus** all needs to run in order for\n",
    "this notebook to work.\n",
    "\n",
    "The code in the section entitled  **Finding triggers  using Mutual Information (Rosenfeld 1994)**\n",
    "does not need to run (it takes a while).  You can circumvent it by using the value assigned to\n",
    "`triggers` at the end of that section. The correct value  for `triggers`(a set of 150 words) is spelled out at the end of that section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9cae7",
   "metadata": {},
   "source": [
    "## Preparing the filtered corpus\n",
    "\n",
    "To restrict the number of parameters in our model and yet demonstarte the ideas on reealistic data, we are going to build an LR language model on a **filtered** data set.  We will model onl noun co-occurrences and we will limit our model to nouns occurring over 100 times in the Brown corpus, That is, gioven that Brpwn is about 1.2 million words,\n",
    "we will look at nounds with a relative fdrequency greater than about:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b844fc",
   "metadata": {},
   "source": [
    "This gives us a vocabulary of between 600 and 700 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae0b9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a42da5",
   "metadata": {},
   "source": [
    "Corpus prep,  Vocab filter (freq thresholkd and noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debceb1b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{rcrrr}\n",
    "k & pos & \\# sents & V & N\\\\\n",
    "\\hline\n",
    "200 & \\text{N} & 28,468  & 258 & 21,709\\\\\n",
    "\\mathbf{100} &  \\text{N}  & \\mathbf{19,243} & \\mathbf{615} & \\mathbf{55, 792} \\\\\n",
    " 1 &        & 57,340  & 49,815 & 1,116,192\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d0867e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161192\n",
      "Filtered info:  Sents 19243 Vocab 615 N 55792\n",
      "Base info:  Sents 57013 Vocab 49815 N 1160865\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "\n",
    "def relevant(w, tag, fd=None, k=100, pos_chars=None):\n",
    "    \"\"\"\n",
    "    pos_char is a usually 'N' or 'V' or 'NV' or to select the nominal or verbal pos sets of Brown\n",
    "    \"\"\"\n",
    "    if pos_chars is not None and not tag[0] in pos_chars:\n",
    "        return False\n",
    "    if fd is not None:\n",
    "        return fd[w] > k\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "bw = [w.lower() for (w,t) in brown.tagged_words()]\n",
    "print(len(bw))\n",
    "fd = FreqDist(bw)\n",
    "tagged_sents = brown.tagged_sents()\n",
    "\n",
    "###############   Filtered vocab #####################################################################\n",
    "\n",
    "#k=200,pos_chars=\"N\"\n",
    "#k=100, pos_chars = \"N\"\n",
    "k, pos_chars, filtered_vocab = 100, \"N\", True\n",
    "\n",
    "if filtered_vocab:\n",
    "    f_bw = [w.lower() for (w,t) in brown.tagged_words() if relevant(w,t,fd,k=k,pos_chars=pos_chars)]\n",
    "    f_fd = FreqDist(f_bw)\n",
    "    #print(len(bw))\n",
    "    f_sents = [[w.lower() for (w,t) in sent if relevant(w, t,f_fd,k=k,pos_chars=pos_chars)] for sent in tagged_sents]\n",
    "    f_sents = [s for s in f_sents if len(s)>1]\n",
    "    f_vocab=set(f_fd.keys())\n",
    "    f_V = len(f_vocab)\n",
    "    print(\"Filtered info: \",\"Sents\", len(f_sents),\"Vocab\", f_V, \"N\", sum(len(s) for s in f_sents))\n",
    "\n",
    "###############   End filtered vocab ###################################################################\n",
    "\n",
    "sents = [[w.lower() for (w,t) in sent] for sent in tagged_sents]\n",
    "vocab=set(fd.keys())\n",
    "sents = [s for s in sents if len(s)>1]\n",
    "num_events = len(sents)\n",
    "V = len(vocab)\n",
    "num_tokens = sum(len(s) for s in sents)\n",
    "#print(num_events,V)\n",
    "print(\"Base info: \",\"Sents\", num_events,\"Vocab\", V, \"N\", num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab97cfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49815"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c37c823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3cbf838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1598"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eed1e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd[\"language\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7707f",
   "metadata": {},
   "source": [
    "These are the words tp try to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aed99ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "class_list = f_fd.most_common(100)\n",
    "class_set = {w for (w,ct) in class_list}\n",
    "print(len(class_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e54213f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['evidence', 'place'],\n",
       " ['charge', 'manner'],\n",
       " ['interest', 'number', 'size', 'city'],\n",
       " ['number', 'interest'],\n",
       " ['cost', 'administration']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6c4cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wd=\"work\"\n",
    "#sent = class_sents[wd][0]\n",
    "#if not wd== sent[0]:\n",
    "#   print(sent[:sent.index(wd)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3facd34e",
   "metadata": {},
   "source": [
    "Make a dictionary such that each key is a classword and\n",
    "the corresponding value is a list of brown \"sentences\" containing that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bfa8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def make_class_sents(f_sents,class_set):\n",
    "    class_sents0 = defaultdict(list)\n",
    "    for sent in f_sents:\n",
    "        wds = class_set.intersection(sent)\n",
    "        for wd in wds:\n",
    "            if not wd == sent[0]:\n",
    "                class_sents0[wd].append(sent[:sent.index(wd)])\n",
    "    return {wd:sents for (wd,sents) in class_sents0.items() if len(sents)>100}\n",
    "\n",
    "#{wd:sents for (wd,sents) in class_sents0.items() if len(sents)>100}\n",
    "class_sents = make_class_sents(f_sents,class_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb4c6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(class_sents[\"work\"])  276\n",
    "# len(class_sents) 91  \n",
    "# so we actually only have 91 lcasses to predict\n",
    "# total_num_class_sents = sum(1 for sents in class_sents.values() for sent in sents)\n",
    "# total_num_class_sents 16026\n",
    "# so we have 16K  histories to split into "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bee85f",
   "metadata": {},
   "source": [
    "Wehave 91 target words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e920c3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b79ab341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents 16026\n",
      "words 28714\n"
     ]
    }
   ],
   "source": [
    "# number of sentences/wds in the filtered corpus\n",
    "\n",
    "print(\"sents\", sum(1 for sents in class_sents.values() for s in sents))\n",
    "print(\"words\", sum(1 for sents in class_sents.values() for s in sents for wd in s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3e3b0",
   "metadata": {},
   "source": [
    "For ease of computation, the corpus has been filtered to include only nouns with frequency greater than 100 in the Brown corpus:\n",
    "\n",
    "Freq threshold for f_vocab, part of speech for f_vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94fc22a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 'N')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k,pos_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25a21025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for s in sents[:25]:\n",
    "#    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53e46a",
   "metadata": {},
   "source": [
    "## Finding triggers  using Mutual Information (Rosenfeld 1994)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef878e8c",
   "metadata": {},
   "source": [
    "## Mutual information calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191acd46",
   "metadata": {},
   "source": [
    "This code takes a while to run.  You can run it  if you like, or you can just use the value\n",
    "of `triggers`, the set of words being computed ion this section, which is assigned to the right set at the end of the **Trigger Vocab calculation** section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9d4b5",
   "metadata": {},
   "source": [
    "The comcept of a **trigger word**.\n",
    "\n",
    "For MI_ calculation: Each word gets assigned avector representing what sentences it has occurred in,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "908e7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f_encoder = {wd:i for (i,wd) in enumerate(f_vocab)}\n",
    "\n",
    "\n",
    "def get_mi_word_vecs (sents,V,encoder):\n",
    "    vecs = np.zeros((V,len(sents)),dtype=int)\n",
    "    for (j,sent) in enumerate(sents):\n",
    "        cts = FreqDist(sent)\n",
    "        for (wd,ct) in cts.items():\n",
    "            vecs[encoder[wd],j] = ct\n",
    "    return vecs\n",
    "            \n",
    "f_vecs = get_mi_word_vecs (f_sents,f_V,f_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66d60d",
   "metadata": {},
   "source": [
    "Shape is V x len(f_sents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b204de10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615, 19243)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918188d2",
   "metadata": {},
   "source": [
    "Now we can get the mutual information of two words by taking the mutual information score\n",
    "of their two word vectors.  So we compute all the pairwird MI scores for the filtered\n",
    "vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e1409bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 30 17:48:24 2026\n",
      "Processing word 0 Fri Jan 30 17:48:24 2026\n",
      "Processing word 50 Fri Jan 30 17:48:59 2026\n",
      "Processing word 100 Fri Jan 30 17:49:31 2026\n",
      "Processing word 150 Fri Jan 30 17:50:00 2026\n",
      "Processing word 200 Fri Jan 30 17:50:27 2026\n",
      "Processing word 250 Fri Jan 30 17:50:51 2026\n",
      "Processing word 300 Fri Jan 30 17:51:11 2026\n",
      "Processing word 350 Fri Jan 30 17:51:29 2026\n",
      "Processing word 400 Fri Jan 30 17:51:43 2026\n",
      "Processing word 450 Fri Jan 30 17:51:55 2026\n",
      "Processing word 500 Fri Jan 30 17:52:03 2026\n",
      "Processing word 550 Fri Jan 30 17:52:09 2026\n",
      "Processing word 600 Fri Jan 30 17:52:11 2026\n",
      "Fri Jan 30 17:52:11 2026\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "f_enc_pairs = list(f_encoder.items())\n",
    "#mis0 = np.zeros((V,V))\n",
    "#mis = Triangular2DArray(mis0)\n",
    "#vecs[0]\n",
    "\n",
    "def get_mis (V,enc_pairs,vecs,batch_sz=50):\n",
    "    mis = np.zeros((V,V))\n",
    "    vecs_b = (vecs > 0)\n",
    "    for (idx,(wd,i)) in enumerate(enc_pairs):\n",
    "        if idx%batch_sz == 0:\n",
    "            print(f\"Processing word {idx} {time.ctime()}\")\n",
    "        for (wd2,j) in enc_pairs[idx:]:\n",
    "            mis[i,j] = mutual_info_score(vecs_b[i],vecs_b[j])\n",
    "    return mis\n",
    "\n",
    "print(time.ctime())\n",
    "mis = get_mis (f_V,f_enc_pairs,f_vecs)\n",
    "print(time.ctime())\n",
    "#word_vecs = vecs.sum(axis=0)\n",
    "#del vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7770800d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04332267751869658"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis[:,0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e09ab9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_max_vals = mis.max(axis=0)\n",
    "#mi_max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "0668c4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis[f_encoder[\"man\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caeafe0",
   "metadata": {},
   "source": [
    "##  End MI calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3513ea",
   "metadata": {},
   "source": [
    "## Trigger vocab calculation\n",
    "\n",
    "Now for each vocab word find exactly one \"trigger\", another word strongly associated according to\n",
    "mutual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f764cfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "def print_triggers (trigger_pairs, top_n=None):\n",
    "    if top_n is not None:\n",
    "        trigger_pairs = trigger_pairs[:top_n]\n",
    "    for (wd,trig,score) in trigger_pairs:\n",
    "        print(f\"{wd} {trig} {score:.5f}\")\n",
    "\n",
    "def get_triggers (mis, decoder, threshhold=.0002,verbose=False):\n",
    "    # Zero out self triggers for now\n",
    "    for i in range(mis.shape[0]):\n",
    "        mis[i,i] = 0\n",
    "    # Find the best trigger of word in V\n",
    "    trigger_pairs = [(decoder[idx],decoder[mis[idx].argmax()]) for idx in range(mis.shape[0])]\n",
    "    # as well as the MI scores\n",
    "    trigger_scores = [(decoder[idx],mis[idx].max()) for idx in range(mis.shape[0])]\n",
    "    # Apply threshhold\n",
    "    filtered_trigger_pairs= []\n",
    "    for (i,(wd, trig)) in enumerate(trigger_pairs):\n",
    "        score = trigger_scores[i][1]\n",
    "        if score > threshhold:\n",
    "            filtered_trigger_pairs.append((wd,trig,score))\n",
    "    # Sort by score\n",
    "    filtered_trigger_pairs.sort(key=lambda x: x[2])\n",
    "    if verbose:\n",
    "        print_triggers (filtered_trigger_pairs)\n",
    "    return filtered_trigger_pairs\n",
    "\n",
    "threshhold,verbose = .0002,False\n",
    "f_decoder = {i:wd for (wd,i) in f_encoder.items()}\n",
    "filtered_trigger_pairs = get_triggers (mis, f_decoder,threshhold=threshhold,verbose=verbose)\n",
    "(wds,triggers,scores) = zip(*filtered_trigger_pairs)\n",
    "triggers = set(triggers)\n",
    "print(len(triggers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "1b95b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "men                       women                     0.00613\n",
      "girls                     boys                      0.00565\n",
      "wife                      husband                   0.00388\n",
      "view                      point                     0.00249\n",
      "eyes                      face                      0.00235\n",
      "year                      tax                       0.00211\n",
      "father                    mother                    0.00195\n",
      "costs                     cost                      0.00193\n",
      "stock                     market                    0.00188\n",
      "aid                       countries                 0.00187\n",
      "development               research                  0.00183\n",
      "sales                     tax                       0.00181\n",
      "room                      door                      0.00170\n",
      "students                  college                   0.00170\n",
      "children                  school                    0.00167\n",
      "college                   school                    0.00163\n",
      "ball                      game                      0.00162\n",
      "list                      forms                     0.00158\n",
      "education                 schools                   0.00151\n",
      "plane                     image                     0.00148\n",
      "property                  tax                       0.00146\n",
      "temperature               surface                   0.00144\n",
      "death                     life                      0.00139\n",
      "weeks                     couple                    0.00139\n",
      "image                     line                      0.00137\n",
      "schools                   school                    0.00132\n",
      "return                    tax                       0.00132\n",
      "staff                     member                    0.00125\n",
      "question                  answer                    0.00125\n",
      "mother                    son                       0.00124\n",
      "points                    line                      0.00121\n",
      "road                      car                       0.00116\n",
      "rate                      industry                  0.00115\n",
      "matter                    fact                      0.00113\n",
      "teeth                     mouth                     0.00112\n",
      "members                   board                     0.00112\n",
      "nations                   world                     0.00112\n",
      "peace                     world                     0.00112\n",
      "boys                      school                    0.00107\n",
      "use                       land                      0.00106\n",
      "activity                  business                  0.00102\n",
      "board                     school                    0.00098\n",
      "police                    car                       0.00097\n",
      "information               form                      0.00096\n",
      "words                     meaning                   0.00095\n",
      "farm                      equipment                 0.00095\n",
      "spirit                    community                 0.00094\n",
      "family                    members                   0.00094\n",
      "order                     court                     0.00094\n",
      "freedom                   nations                   0.00093\n",
      "trial                     court                     0.00089\n",
      "total                     number                    0.00089\n",
      "hall                      door                      0.00088\n",
      "law                       nations                   0.00086\n",
      "woman                     man                       0.00086\n",
      "cost                      analysis                  0.00086\n",
      "forms                     form                      0.00085\n",
      "service                   cost                      0.00085\n",
      "months                    cent                      0.00085\n",
      "system                    population                0.00083\n",
      "girl                      boy                       0.00082\n",
      "equipment                 sales                     0.00082\n",
      "center                    growth                    0.00082\n",
      "head                      hair                      0.00082\n",
      "stage                     policy                    0.00081\n",
      "plant                     equipment                 0.00080\n",
      "program                   aid                       0.00079\n",
      "lot                       money                     0.00076\n",
      "spring                    summer                    0.00075\n",
      "child                     school                    0.00075\n",
      "water                     level                     0.00074\n",
      "industry                  increase                  0.00073\n",
      "ideas                     literature                0.00073\n",
      "administration            tax                       0.00072\n",
      "back                      head                      0.00072\n",
      "face                      hair                      0.00071\n",
      "policy                    state                     0.00070\n",
      "house                     front                     0.00070\n",
      "hours                     month                     0.00069\n",
      "miles                     distance                  0.00069\n",
      "lines                     image                     0.00069\n",
      "business                  sales                     0.00068\n",
      "art                       form                      0.00068\n",
      "tax                       state                     0.00066\n",
      "feet                      water                     0.00064\n",
      "life                      way                       0.00062\n",
      "control                   factors                   0.00060\n",
      "programs                  community                 0.00060\n",
      "night                     morning                   0.00060\n",
      "meaning                   language                  0.00059\n",
      "age                       years                     0.00059\n",
      "word                      meaning                   0.00059\n",
      "power                     influence                 0.00059\n",
      "responsibility            government                0.00059\n",
      "support                   program                   0.00057\n",
      "person                    persons                   0.00056\n",
      "cities                    schools                   0.00056\n",
      "degree                    factors                   0.00056\n",
      "pressure                  temperature               0.00056\n",
      "plan                      tax                       0.00055\n",
      "study                     literature                0.00055\n",
      "period                    years                     0.00055\n",
      "science                   future                    0.00054\n",
      "difference                function                  0.00054\n",
      "party                     leaders                   0.00054\n",
      "picture                   plane                     0.00053\n",
      "story                     mother                    0.00053\n",
      "function                  value                     0.00053\n",
      "heart                     blood                     0.00053\n",
      "experience                spirit                    0.00052\n",
      "country                   parts                     0.00052\n",
      "light                     reaction                  0.00052\n",
      "top                       paper                     0.00052\n",
      "door                      front                     0.00052\n",
      "figure                    temperature               0.00052\n",
      "side                      door                      0.00052\n",
      "thought                   ideas                     0.00052\n",
      "corner                    street                    0.00051\n",
      "volume                    sales                     0.00050\n",
      "letter                    son                       0.00050\n",
      "town                      meeting                   0.00050\n",
      "name                      company                   0.00049\n",
      "day                       month                     0.00049\n",
      "week                      hours                     0.00048\n",
      "activities                programs                  0.00048\n",
      "areas                     labor                     0.00048\n",
      "morning                   sun                       0.00048\n",
      "boy                       hair                      0.00048\n",
      "point                     line                      0.00048\n",
      "job                       school                    0.00048\n",
      "game                      season                    0.00048\n",
      "president                 company                   0.00046\n",
      "operation                 pool                      0.00046\n",
      "literature                history                   0.00046\n",
      "reaction                  temperature               0.00046\n",
      "car                       corner                    0.00045\n",
      "data                      values                    0.00045\n",
      "friend                    boy                       0.00045\n",
      "population                cent                      0.00045\n",
      "defense                   forces                    0.00045\n",
      "mind                      ideas                     0.00045\n",
      "sound                     water                     0.00045\n",
      "analysis                  systems                   0.00045\n",
      "effect                    reaction                  0.00044\n",
      "government                state                     0.00043\n",
      "course                    student                   0.00042\n",
      "approach                  problem                   0.00042\n",
      "student                   school                    0.00041\n",
      "efforts                   aid                       0.00041\n",
      "production                industry                  0.00040\n",
      "steps                     door                      0.00040\n",
      "systems                   defense                   0.00040\n",
      "right                     street                    0.00040\n",
      "problems                  attention                 0.00040\n",
      "state                     court                     0.00040\n",
      "sort                      thing                     0.00039\n",
      "nature                    man                       0.00039\n",
      "years                     others                    0.00039\n",
      "world                     progress                  0.00039\n",
      "table                     head                      0.00039\n",
      "growth                    population                0.00038\n",
      "countries                 world                     0.00038\n",
      "color                     hair                      0.00038\n",
      "love                      faith                     0.00038\n",
      "air                       surface                   0.00038\n",
      "opportunity               freedom                   0.00038\n",
      "research                  increase                  0.00038\n",
      "paper                     piece                     0.00037\n",
      "end                       line                      0.00037\n",
      "treatment                 child                     0.00037\n",
      "radio                     surface                   0.00036\n",
      "work                      school                    0.00036\n",
      "moment                    mind                      0.00035\n",
      "force                     means                     0.00035\n",
      "hand                      mouth                     0.00035\n",
      "food                      production                0.00035\n",
      "methods                   research                  0.00035\n",
      "series                    reaction                  0.00035\n",
      "effects                   reaction                  0.00034\n",
      "need                      community                 0.00034\n",
      "market                    months                    0.00034\n",
      "interest                  months                    0.00034\n",
      "body                      face                      0.00034\n",
      "values                    value                     0.00033\n",
      "answer                    community                 0.00033\n",
      "afternoon                 morning                   0.00033\n",
      "number                    value                     0.00033\n",
      "time                      statement                 0.00033\n",
      "sun                       summer                    0.00032\n",
      "earth                     corner                    0.00032\n",
      "evidence                  fact                      0.00032\n",
      "area                      man                       0.00032\n",
      "questions                 science                   0.00032\n",
      "class                     cent                      0.00032\n",
      "land                      areas                     0.00032\n",
      "pattern                   treatment                 0.00031\n",
      "window                    bed                       0.00031\n",
      "ground                    floor                     0.00031\n",
      "leaders                   government                0.00030\n",
      "respect                   principle                 0.00030\n",
      "days                      state                     0.00030\n",
      "ways                      systems                   0.00030\n",
      "amount                    reaction                  0.00030\n",
      "states                    state                     0.00030\n",
      "distance                  range                     0.00030\n",
      "audience                  effect                    0.00029\n",
      "religion                  community                 0.00029\n",
      "increase                  extent                    0.00029\n",
      "existence                 form                      0.00029\n",
      "field                     world                     0.00029\n",
      "size                      length                    0.00029\n",
      "summer                    town                      0.00029\n",
      "parts                     water                     0.00028\n",
      "decision                  letter                    0.00028\n",
      "knowledge                 language                  0.00028\n",
      "types                     property                  0.00028\n",
      "issue                     labor                     0.00028\n",
      "report                    issue                     0.00028\n",
      "statement                 truth                     0.00027\n",
      "public                    increase                  0.00027\n",
      "century                   literature                0.00027\n",
      "home                      case                      0.00027\n",
      "case                      court                     0.00027\n",
      "section                   respect                   0.00027\n",
      "manner                    ways                      0.00026\n",
      "nation                    nations                   0.00026\n",
      "forces                    war                       0.00026\n",
      "sense                     day                       0.00026\n",
      "cases                     court                     0.00026\n",
      "people                    man                       0.00026\n",
      "minutes                   couple                    0.00025\n",
      "floor                     bed                       0.00025\n",
      "feeling                   heart                     0.00025\n",
      "man                       terms                     0.00025\n",
      "surface                   material                  0.00025\n",
      "services                  child                     0.00025\n",
      "basis                     research                  0.00025\n",
      "husband                   friends                   0.00025\n",
      "space                     science                   0.00025\n",
      "conditions                way                       0.00024\n",
      "strength                  forces                    0.00024\n",
      "change                    pressure                  0.00024\n",
      "language                  form                      0.00024\n",
      "theory                    law                       0.00024\n",
      "groups                    members                   0.00024\n",
      "role                      society                   0.00023\n",
      "factors                   situation                 0.00023\n",
      "school                    water                     0.00023\n",
      "horse                     sound                     0.00023\n",
      "money                     amount                    0.00023\n",
      "season                    home                      0.00023\n",
      "thing                     state                     0.00022\n",
      "city                      meeting                   0.00022\n",
      "type                      blood                     0.00022\n",
      "voice                     hands                     0.00022\n",
      "kind                      man                       0.00022\n",
      "performance               season                    0.00022\n",
      "letters                   numbers                   0.00021\n",
      "evening                   hours                     0.00021\n",
      "place                     action                    0.00021\n",
      "importance                religion                  0.00021\n",
      "step                      progress                  0.00021\n",
      "man's                     religion                  0.00021\n",
      "movement                  man                       0.00020\n",
      "position                  day                       0.00020\n",
      "trouble                   couple                    0.00020\n",
      "chance                    method                    0.00020\n",
      "organization              party                     0.00020\n",
      "effort                    way                       0.00020\n",
      "problem                   child                     0.00020\n"
     ]
    }
   ],
   "source": [
    "#(wds00,triggers00,scores00) = zip(*filtered_trigger_pairs)\n",
    "for (w,t,s) in filtered_trigger_pairs[::-1]:\n",
    "    print(f\"{w:<25} {t:<25} {s:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd18392",
   "metadata": {},
   "source": [
    "An important limitation of **mutual information**:  These words have been\n",
    "discovered because the occurrence of one of them in a sentence increases ythe likelihood oif its partner occurring in a sentence.  So they're here bnecause they ocurred in the **same** sentence often,\n",
    "not because they occurred in **similar** sentences often.  We will return to this issue and offer a solution when we consider embeddings models of word meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f07046",
   "metadata": {},
   "source": [
    "You can run the code to find the correct value for `triggers`, or you can just use the value for `triggers` set in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f0d474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "triggers = {'action',\n",
    " 'aid',\n",
    " 'amount',\n",
    " 'analysis',\n",
    " 'answer',\n",
    " 'areas',\n",
    " 'attention',\n",
    " 'bed',\n",
    " 'blood',\n",
    " 'board',\n",
    " 'boy',\n",
    " 'boys',\n",
    " 'business',\n",
    " 'car',\n",
    " 'case',\n",
    " 'cent',\n",
    " 'child',\n",
    " 'college',\n",
    " 'community',\n",
    " 'company',\n",
    " 'corner',\n",
    " 'cost',\n",
    " 'countries',\n",
    " 'couple',\n",
    " 'court',\n",
    " 'day',\n",
    " 'defense',\n",
    " 'distance',\n",
    " 'door',\n",
    " 'effect',\n",
    " 'equipment',\n",
    " 'extent',\n",
    " 'face',\n",
    " 'fact',\n",
    " 'factors',\n",
    " 'faith',\n",
    " 'floor',\n",
    " 'forces',\n",
    " 'form',\n",
    " 'forms',\n",
    " 'freedom',\n",
    " 'friends',\n",
    " 'front',\n",
    " 'function',\n",
    " 'future',\n",
    " 'game',\n",
    " 'government',\n",
    " 'growth',\n",
    " 'hair',\n",
    " 'hands',\n",
    " 'head',\n",
    " 'heart',\n",
    " 'history',\n",
    " 'home',\n",
    " 'hours',\n",
    " 'husband',\n",
    " 'ideas',\n",
    " 'image',\n",
    " 'increase',\n",
    " 'industry',\n",
    " 'influence',\n",
    " 'issue',\n",
    " 'labor',\n",
    " 'land',\n",
    " 'language',\n",
    " 'law',\n",
    " 'leaders',\n",
    " 'length',\n",
    " 'letter',\n",
    " 'level',\n",
    " 'life',\n",
    " 'line',\n",
    " 'literature',\n",
    " 'man',\n",
    " 'market',\n",
    " 'material',\n",
    " 'meaning',\n",
    " 'means',\n",
    " 'meeting',\n",
    " 'member',\n",
    " 'members',\n",
    " 'method',\n",
    " 'mind',\n",
    " 'money',\n",
    " 'month',\n",
    " 'months',\n",
    " 'morning',\n",
    " 'mother',\n",
    " 'mouth',\n",
    " 'nations',\n",
    " 'number',\n",
    " 'numbers',\n",
    " 'others',\n",
    " 'paper',\n",
    " 'parts',\n",
    " 'party',\n",
    " 'persons',\n",
    " 'piece',\n",
    " 'plane',\n",
    " 'point',\n",
    " 'policy',\n",
    " 'pool',\n",
    " 'population',\n",
    " 'pressure',\n",
    " 'principle',\n",
    " 'problem',\n",
    " 'production',\n",
    " 'program',\n",
    " 'programs',\n",
    " 'progress',\n",
    " 'property',\n",
    " 'range',\n",
    " 'reaction',\n",
    " 'religion',\n",
    " 'research',\n",
    " 'respect',\n",
    " 'sales',\n",
    " 'school',\n",
    " 'schools',\n",
    " 'science',\n",
    " 'season',\n",
    " 'situation',\n",
    " 'society',\n",
    " 'son',\n",
    " 'sound',\n",
    " 'spirit',\n",
    " 'state',\n",
    " 'statement',\n",
    " 'street',\n",
    " 'student',\n",
    " 'summer',\n",
    " 'sun',\n",
    " 'surface',\n",
    " 'systems',\n",
    " 'tax',\n",
    " 'temperature',\n",
    " 'terms',\n",
    " 'thing',\n",
    " 'town',\n",
    " 'treatment',\n",
    " 'truth',\n",
    " 'value',\n",
    " 'values',\n",
    " 'war',\n",
    " 'water',\n",
    " 'way',\n",
    " 'ways',\n",
    " 'women',\n",
    " 'world',\n",
    " 'years'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93d9d1",
   "metadata": {},
   "source": [
    "This should evaluate to 150 to get the results I want you to reproduce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d585fe4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triggers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597fca2f",
   "metadata": {},
   "source": [
    "## Make the Korpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a182c89d",
   "metadata": {},
   "source": [
    "To make a corpus you will call the function `prepare_korpus` defined and called in the few code cells.\n",
    "\n",
    "The first thing it will do is convert the filtered corpus into a set of history, predicted_word\n",
    "pairs.  This is done in the function `make_class_sents`, which takes as its arguments\n",
    "the filtered corpus and the set of class words (`class_set`) \n",
    "which are all frequent words \n",
    "selected to guarantee there would be enough examples of each predicted word to make reasonable training possible, even in  this small dataset.  \n",
    "\n",
    "The function `make_class_sents` returns  a dictionary `class_sents` which has t he following structure:\n",
    "\n",
    "```python\n",
    "class_wd |-> histories\n",
    "```\n",
    "\n",
    "where each `class_wd` is a word to be predicted and `histories` is a list\n",
    "of (filtered) histories for which_class_wd is the next word.\n",
    "\n",
    "Here's an example of the contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0668798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['law'],\n",
       " ['police', 'trial'],\n",
       " ['today', 'business'],\n",
       " ['administration', 'policy'],\n",
       " ['city'],\n",
       " ['night', 'study', 'changes'],\n",
       " ['group', 'mind'],\n",
       " ['sales', 'state', 'tax'],\n",
       " ['right'],\n",
       " ['scene'],\n",
       " ['defense'],\n",
       " ['game'],\n",
       " ['sun'],\n",
       " ['points', 'years', 'school'],\n",
       " ['boy'],\n",
       " ['efforts'],\n",
       " ['party'],\n",
       " ['home'],\n",
       " ['evening'],\n",
       " ['market', 'years']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_sents = make_class_sents(f_sents,class_set)\n",
    "# Histories followed by the word \"time\"\n",
    "class_sents[\"time\"][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb111a94",
   "metadata": {},
   "source": [
    "If we mush all the histories associated with all the target words, there are 16,026 histories.  That's how many histories we will train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "c9e23825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16026"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(sents) for sents in class_sents.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd0b15",
   "metadata": {},
   "source": [
    "The `class_sents` dictionary is then used to create `korpus`, the array representation of all 16,026\n",
    "histories,  and `Y` the corresponding 16,026 words to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e051773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_korpus (f_sents, class_set, active_triggers):\n",
    "    # Do creation of class_sents here because this code destructively modifies the sent lists\n",
    "    # class sents is a dictionary:  class_wd |-> histories\n",
    "    # where each eclass_d is a wd to be predicted and hsitories is a list\n",
    "    # (filetered) histories for which_class_wd is the next word.\n",
    "    class_sents = make_class_sents(f_sents,class_set)\n",
    "    #active_triggers = triggers\n",
    "    #active_triggers = set()\n",
    "    korpus0 = []\n",
    "    final_vocab0 = set()\n",
    "    for cls_wd,sents in class_sents.items():\n",
    "        for sent in sents:\n",
    "            sent[-1] = sent[-1] + \"_b\"\n",
    "            final_vocab0.add(sent[-1])\n",
    "            korpus0.append((sent,cls_wd))\n",
    "\n",
    "    final_vocab = final_vocab0 | active_triggers\n",
    "\n",
    "    #########   FINAL  PASS: Vectorize; Create korpus and Y   ########################\n",
    "    final_sample_sz, final_V = (len(korpus0),len(final_vocab))\n",
    "    korpus = np.zeros((final_sample_sz, final_V))\n",
    "    #final_dim = final_V + len(active_triggers)\n",
    "    final_encoder = {wd:i for (i,wd) in enumerate(final_vocab)}\n",
    "\n",
    "    trigger_ct = 0\n",
    "    Y = []    #np.array((final_sample_sz,))\n",
    "    for (i,(sent,cls_wd)) in enumerate(korpus0):\n",
    "        bigram_wd = sent[-1]\n",
    "        #print(bigram_wd,final_encoder[bigram_wd])\n",
    "        korpus[i,final_encoder[bigram_wd]] += 1\n",
    "        these_triggers = active_triggers.intersection(sent)\n",
    "        for trig in these_triggers:\n",
    "            trigger_ct += 1\n",
    "            korpus[i,final_encoder[trig]] += 1\n",
    "        Y.append(cls_wd)\n",
    "    Y=np.array(Y)\n",
    "    ##########  END FINAL  PASS  #######################\n",
    "    print(f\"Kropus created:: korpus shape: {korpus.shape}  Y shape: {Y.shape} triggers used: {trigger_ct}\")\n",
    "    return korpus, korpus0, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b80b73b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kropus created:: korpus shape: (16026, 508)  Y shape: (16026,) triggers used: 5511\n"
     ]
    }
   ],
   "source": [
    "# Make the corpus.  Must execute this cell.\n",
    "korpus, korpus0, Y = prepare_korpus (f_sents, class_set, triggers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64e911",
   "metadata": {},
   "source": [
    "##  Number of features for the base model (with triggers)\n",
    "\n",
    "The number of features for this model is 508."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd80d0b",
   "metadata": {},
   "source": [
    "The `prepare_corpus` function returns three things:\n",
    "\n",
    "1. `korpus`:  16,026 histories from the Brown corpus encoded as a 16,026x508 array, where 508 is the number of dimensions in a history vector, the encoded representation of a history.\n",
    "2. `Y`:  the 16,026 target words for those history vectors. These are the words our language model will try to predict. A target word is always a word that occurred later in the same sentence as the words in its history in the orginal Brown corpus.\n",
    "2.  `big_korpus0`: a sequence of history, target pairs represented as words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af9d23",
   "metadata": {},
   "source": [
    "The histories in `korpus0` differ from the histories in `class_sents` only in that they're in a flat list\n",
    "and the last words have been modified to have \"_b\" on them.  This is because one of the best trigger words for any word is itself:  once a content word with unigram probability $p$ occurs in any document the likelihood of its occurring in the rest of the document is higher than $p$. This property is sometimes called **burstiness.**\n",
    "To allow for the possibility for the same word to occur in\n",
    "the history both as the last word (the bigram prefix) and earlier (as a trigger), trigger words\n",
    "and words in final position in a history have different features; `korpus0` was a convenience\n",
    "in building the training data, but will play no role in training.  It does however\n",
    "help when we have questions about how a particular histopry gave rise to its feature reprersentation in \n",
    "`korpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c3fd8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['food', 'family_b'], 'place')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korpus0[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad804ea",
   "metadata": {},
   "source": [
    "Here is part of a row from `korpus`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b0429",
   "metadata": {},
   "source": [
    "It  is a sparse matrix, mostly 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b938fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korpus[12,:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7335d3",
   "metadata": {},
   "source": [
    "But each row will have at least one non-zero value in there because the bigram word (with a \"_b\" at\n",
    "the end) will always be an encoded feature of the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "fb32828e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korpus[12].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f98e39b",
   "metadata": {},
   "source": [
    "Consider how the word sequence in `korpus0[12]` is related to the 1D array `korpus[12]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "58f9f66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['food', 'family_b'], 'place')"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korpus0[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d774ef6",
   "metadata": {},
   "source": [
    "There is only one active feature in row 12, because \"food\" is not a trigger word. Since tt's not in bigram possition\n",
    "and it's not a trigger, we ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "2f5cbe5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"food\"  in triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe6254",
   "metadata": {},
   "source": [
    "Let's find a more interesting example.  The row with the greatest number of active features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2bd3ee12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1484"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korpus.sum(axis=1).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f6a34",
   "metadata": {},
   "source": [
    "When there is a no trigger word the history for the\n",
    "word prediction has only one non zero feature, the feature\n",
    "for the immediately preceding word (the bigram feature).\n",
    "When there is also a single trigger word, there are two nonzero features\n",
    "features.  History `1484` has 6 trigger words in additionto the bigram word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "eaaf797d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korpus[1484].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d5f756",
   "metadata": {},
   "source": [
    "This history and target word (word-to-predict, or class) of sample 1484:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "5d8ce1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['case',\n",
       "  'pressure',\n",
       "  'principle',\n",
       "  'use',\n",
       "  'means',\n",
       "  'years',\n",
       "  'state',\n",
       "  'state',\n",
       "  'program_b'],\n",
       " 'man')"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korpus0[1484]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "a73cf970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'case', 'means', 'pressure', 'principle', 'state', 'years'}"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_1484 = triggers.intersection(korpus0[1484][0])\n",
    "ts_1484"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9797034",
   "metadata": {},
   "source": [
    "Alas none of these trigger words has much of an association  with the given target word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "e5aea988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0.0\n",
      "pressure 0.0\n",
      "years 0.0\n",
      "principle 5.23414303311813e-06\n",
      "means 0.0\n",
      "case 0.00013002590661195888\n"
     ]
    }
   ],
   "source": [
    "#  Attn:  Re-evaluate this cell only if you have computed themi scores in thsi notebook session.\n",
    "man_vec = mis[f_encoder[\"man\"]]\n",
    "for t in ts_1484:\n",
    "    print(f\"{t} {man_vec[f_encoder[t]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f95a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [[w.lower() for (w,t) in sent] for sent in tagged_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d660c1f",
   "metadata": {},
   "source": [
    "Here's the original sentence from the corpus.  The target word `man` is the last word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "8144e112",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in any case , anyone who fails to make significant distinction between primary and secondary applications of economic pressure would in principle already have justified that use of economic boycott as a means which broke out a few years ago or was skillfully organized by white citizens' councils in the entire state of mississippi against every local philco dealer in that state , in protest against a philco-sponsored program over a national tv network on which was presented a drama showing , it seemed , a `` high yellow gal '' smooching with a white man .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([w for s in sents for w in s if len(ts_1484.intersection(s))==6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42a59a",
   "metadata": {},
   "source": [
    "The average row sum is approximately 1.34,\n",
    "which means there are a significant number of trigger words in `korpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e20f1527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3438786971171846"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ss = korpus.sum(axis=1)\n",
    "Ss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f25645b",
   "metadata": {},
   "source": [
    "##  Training the Logistic Regression classifier (with triggers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26dd5b8",
   "metadata": {},
   "source": [
    "This takes a little time, see the wall time printouts below from my Mac.  Your mileage may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06567839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb  3 23:07:24 2026\n",
      "Tue Feb  3 23:08:31 2026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "    \n",
    "#     LogisticRegression(penalty='deprecated', \n",
    "#                        C=1.0, l1_ratio=0.0, dual=False, \n",
    "#                         tol=0.0001, fit_intercept=True, \n",
    "#                         intercept_scaling=1, class_weight=None, \n",
    "#                         random_state=None, solver='lbfgs', \n",
    "#                         max_iter=100, verbose=0, warm_start=False, n_jobs=None)\n",
    "\n",
    "# we do want solver - 'saga' (sag wil also work also gd for large datasets) \n",
    "# and l1_ratio = 0, Also it's a good multiclass algorithm.\n",
    "lrc = LogisticRegression(solver=\"saga\")\n",
    "print(time.ctime())\n",
    "lrc.fit(korpus,Y)\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcffe96",
   "metadata": {},
   "source": [
    "##  Training the Logistic Regression classifier (without triggers: bigrams only model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e2634d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kropus created:: korpus shape: (16026, 358)  Y shape: (16026,) triggers used: 0\n"
     ]
    }
   ],
   "source": [
    "big_korpus, big_korpus0, Y = prepare_korpus (f_sents, class_set, set())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e35a3a0",
   "metadata": {},
   "source": [
    "####  The number of features for the bigrams model\n",
    "\n",
    "The number of features for this model is 358."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f96b51e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb  3 23:14:56 2026\n",
      "Tue Feb  3 23:15:21 2026\n"
     ]
    }
   ],
   "source": [
    "# Training the no-trigger words (or bigram) model\n",
    "big_lrc = LogisticRegression(solver=\"saga\")\n",
    "print(time.ctime())\n",
    "big_lrc.fit(big_korpus,Y)\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750efb64",
   "metadata": {},
   "source": [
    "##  Perplexity computation\n",
    "\n",
    "Your task is to compute the perplexity that the trained language model `lrc` assigns to the given corpus (or `korpus`), using  the formula for perplexity given in the ngram slides.  For the questions\n",
    "on this assignment you will find the `lrc` methods `.predict()`, `.predict_proba` and `.predict_leg_proba`\n",
    "useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26ac75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(lrc,korpus,Y):\n",
    "    probs = lrc.predict_proba(korpus)\n",
    "    (nsamples,nclasses) = korpus.shape\n",
    "    row_selection = list(range(nsamples))\n",
    "    #  Col selection\n",
    "    # Y is sequence if words.   We need to convert that\n",
    "    # to a sequence of their indices.\n",
    "    class_encoder = {w:i for (i,w) in enumerate(lrc.classes_)}\n",
    "    Y_idxs  = np.array([class_encoder[y] for y in Y])\n",
    "    # End col selection\n",
    "    probs_for_target_words = probs[row_selection,Y_idxs]\n",
    "    return probs_to_perplexity (probs_for_target_words)\n",
    "\n",
    "def probs_to_perplexity (probs):\n",
    "    \"\"\"\n",
    "    probs is an array of probabilities.  \n",
    "    \n",
    "    Let H = 1/N sum log_{2} probs\n",
    "    [log of the nth root of the product]\n",
    "    return 2**H\n",
    "    (map back off the log scale)\n",
    "    \"\"\"\n",
    "    nsamples = probs.shape[0]\n",
    "    #log_two_probs = np.log(probs)/np.log(2)\n",
    "    log_two_probs = np.log2(probs)\n",
    "    return 2**((-log_two_probs.sum())/nsamples)\n",
    "\n",
    "def perplexity_to_prob (perp):\n",
    "    \"\"\"\n",
    "    perp is a perplexity number (an average branching score)\n",
    "    \n",
    "    Convert it to the corresponding probability.\n",
    "    \"\"\"\n",
    "    #return np.exp(-perp * np.log(2))\n",
    "    return 2**(-perp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1fa9b",
   "metadata": {},
   "source": [
    "Get the perplexity for the trigger word model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae381d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(37.206003735660275)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with triggers: 5.217463600316301\n",
    "# w/o triggers:  5.464946456044086\n",
    "# w no info: probs_to_perplexity (np.array([1/91])) = 6.507794640198696 =\n",
    "get_perplexity(lrc,korpus,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e3e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56bf1cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(32.0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "probs_to_perplexity(np.array([1/32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988830e",
   "metadata": {},
   "source": [
    "### Perplexity for the bigrams only model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524cbba",
   "metadata": {},
   "source": [
    "Get the perplexity for the model with no triggers (the bigram model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1b92079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(44.168512432090985)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_perplexity(big_lrc,big_korpus,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28573437",
   "metadata": {},
   "source": [
    "### Perplexity for unigram probability model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64ef65",
   "metadata": {},
   "source": [
    "Get the perplexity for the model that assigns equal probability to each of the 91 target words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ed97187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_targets = len(set(Y))\n",
    "num_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64fa4e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(91.0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_to_perplexity (np.array([1/num_targets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cf25a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(91.0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_to_perplexity (np.array([1/91]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca6829",
   "metadata": {},
   "source": [
    "###  Maximum precision word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "92899e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df63a2",
   "metadata": {},
   "source": [
    "The predicted target words compared to the actual target words using `sklearn.metrics.precision_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "daa8491e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time'"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = lrc.predict(korpus)\n",
    "lrc.classes_[precision_score(predictions,Y,average=None).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "02cc2bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"time\" in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7f84eee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wife'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc.classes_[recall_score(predictions,Y,average=None,labels=lrc.classes_,zero_division=0).argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102e352",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642eb2af",
   "metadata": {},
   "source": [
    "## Example of using classifiers in scikit learn\n",
    "\n",
    "Sprinkled liberally with hints for the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "190e8960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9893bd71",
   "metadata": {},
   "source": [
    "### The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "84234db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups = fetch_20newsgroups()\n",
    "newsgroups['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "2e11f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a multiclass problem, so pick three of the 20 categories\n",
    "categories = ['alt.atheism', 'sci.space','comp.graphics']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                     categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "6d69d09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: degroff@netcom.com (21012d)\n",
      "Subject: Re: Venus Lander for Venus Conditions.\n",
      "Organization: Netcom Online Communications Services (408-241-9760 login: guest)\n",
      "Lines: 8\n",
      "\n",
      "\n",
      "  I doubt there are good prospects for  a self armoring system\n",
      "for venus surface conditions (several hundred degrees, very high\n",
      "pressure of CO2, possibly sulfuric and nitric acids or oxides\n",
      "but it is a notion to consider for outer planets rs where you might\n",
      "pick up ices under less extream upper atmosphere conditions buying\n",
      "deeper penetration.  A nice creative idea, unlikly but worthy of\n",
      "thinking about.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "439ebe49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9349ab53",
   "metadata": {},
   "source": [
    "The classnames we will pass to the classifier in training are integers.\n",
    "The integers are aligned with the class names in the order in target_names.\n",
    "Therefore we can set up a simple decoder dictionary that maps from class indices to class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "fd8f7772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sci.space'"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_class = newsgroups_train.target[0]\n",
    "print(first_class)\n",
    "decoder = np.array(newsgroups_train.target_names)\n",
    "decoder[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e08833",
   "metadata": {},
   "source": [
    "###  Training\n",
    "\n",
    "Train a logistic regression classifier on this multiclass problem.\n",
    "Also test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "ed95c751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1657, 29663)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.941016333938294"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not usually prize-winning with language data\n",
    "# vectorizer = CountVectorizer()\n",
    "\n",
    "##########  Mapping from a sequence of texts to a feature representation of the data\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "# (1657, 29663)\n",
    "# 1,657 documents. 29663 features.  Why so many features?  That's how many\n",
    "# distinct vocab items cropped up in these 1,657 documents.  We use words\n",
    "# as features in our language model as well but there are way fewer\n",
    "# features because our training data consists of filtered document texts and therefore a filtered vocab.\n",
    "print(vectors_train.shape)\n",
    "########## End of feature  mapping #####################################\n",
    "\n",
    "clf = LogisticRegression(solver=\"saga\")\n",
    "# targets are [0,1,2]  aligned with newsgroups_train.target_names\n",
    "clf.fit(vectors_train, newsgroups_train.target)\n",
    "#  The usual thing we do with trained classifiers\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average=\"micro\")\n",
    "#  f1 score average = \"micro\"  0.9407548825982005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f663bd",
   "metadata": {},
   "source": [
    "##  Predicting probabilities\n",
    "\n",
    "For this assignment we're more interested in having the clasifier produce probabilities:\n",
    "\n",
    "We classify a fresh example using `predict_proba`.\n",
    "\n",
    "Notice there are three probabilities.  That's because there are three classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "0db54e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0606451 , 0.17867197, 0.76068294]])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_text = [\"Space is the final frontier.\"]\n",
    "probs = clf.predict_proba(vectorizer.transform(space_text))\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "13cb803f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dfc594",
   "metadata": {},
   "source": [
    "Class with the highest prob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "b4de3dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "9ab59117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sci.space'"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = np.array(newsgroups_train.target_names)\n",
    "decoder[probs.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcce0ee",
   "metadata": {},
   "source": [
    "Which is the same answer I could have gotten through `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "ec560a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sci.space'"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[clf.predict(vectorizer.transform(space_text))[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9950bde",
   "metadata": {},
   "source": [
    "Note  that in our language modeling example you didn't need to call `vectorizer.transform`.\n",
    "I wrote `prepare_korpus` to do that job, because I wanted some custom \"vectorizing\".\n",
    "But the output was still a 2D array with the same number of rows as there were histories\n",
    "to classify and the same number of columns as there were features to classify with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b07c8bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probs shape:  (2, 3)\n",
      "Predicted class indices:  [2 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sci.space', 'comp.graphics'], dtype='<U13')"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a sequence of n inputs predict_proba will produce a  nx3 array.  Here n=2\n",
    "texts = [\"Space is the final frontier.\", \"Rasters are common in computer images.\"]\n",
    "probs = clf.predict_proba(vectorizer.transform(texts))\n",
    "print(\"Probs shape: \", probs.shape)\n",
    "cls_names = newsgroups_train.target_names\n",
    "cls_idxs = probs.argmax(axis=1)\n",
    "print(\"Predicted class indices: \",cls_idxs)\n",
    "decoder = np.array(cls_names)\n",
    "decoder[cls_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9701581d",
   "metadata": {},
   "source": [
    "##  Retrieving the predicted probabilities for a sequence of classes\n",
    "\n",
    "Hint:  This discussion relates to computing the perplexity of the data:\n",
    "\n",
    "Now suppose in the interest of finding the **hard** examples in the test set,  I want to know the probability \n",
    "my trained classifier assigns to the **correct** class for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "f3038393",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = clf.predict_proba(vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "edb0dc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1102, 3)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5721ed2e",
   "metadata": {},
   "source": [
    "I need to retrieve a different column index from each row of `pred_probs`, as dictated by the correct classes\n",
    "for the test data (`newsgroups_test.target`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "55fa9d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, ..., 0, 1, 2])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f49fa",
   "metadata": {},
   "source": [
    "This can be done via **fancy indexing** of the probs array. For a 1D array we pass a list containing\n",
    "a sequence of the indices we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "37578beb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61]\n",
      "(20,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([10, 28, 37, 49])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(4,62,3)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "a[[2,8,11,15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288700c4",
   "metadata": {},
   "source": [
    "For a 2D array, we pass two sequences, one for the row indices we want, the other for the column indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "3a894862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  7 10 13]\n",
      " [16 19 22 25]\n",
      " [28 31 34 37]\n",
      " [40 43 46 49]\n",
      " [52 55 58 61]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([22, 37, 55])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = a.reshape((5,4))\n",
    "print(aa)\n",
    "# The second element retrieved is aa[2,3]\n",
    "aa[[1,2,4],[2,3,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf0354",
   "metadata": {},
   "source": [
    "Back to our original problem.  We want an array consisting of one element from each row,\n",
    "the element corresponding to the correct class for that row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "bfcc48ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88644291, 0.91605077, 0.68185055, ..., 0.90199975, 0.68808646,\n",
       "       0.8170256 ])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rows = len(newsgroups_test.target)\n",
    "all_rows_idxs = list(range(num_rows))\n",
    "column_idxs = list(newsgroups_test.target)\n",
    "\n",
    "prediction_probs_for_correct_classes = pred_probs[all_rows_idxs,column_idxs]\n",
    "prediction_probs_for_correct_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90526b",
   "metadata": {},
   "source": [
    "To find the lowest prob assigned to a correct class on the test set we first find its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "3af09986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "548"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_idx = prediction_probs_for_correct_classes.argmin()\n",
    "example_idx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f16018",
   "metadata": {},
   "source": [
    "Here's the probability assigned to the correct class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "a2259f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1416095974675527"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_probs_for_correct_classes[example_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c18ffe",
   "metadata": {},
   "source": [
    "And here is that example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "7c7fe9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: gkm@wampyr.cc.uow.edu.au (Glen K Moore)\n",
      "Subject: Fax/email wanted for Louis Friedman/Planetary Society\n",
      "Organization: University of Wollongong, NSW, Australia.\n",
      "Lines: 7\n",
      "NNTP-Posting-Host: wampyr.cc.uow.edu.au\n",
      "Summary: Want to obtain fax/email address for Planetary Society\n",
      "Keywords: Planetary Friedman\n",
      "\n",
      "If available please send to\n",
      "Glen Moore\n",
      "Director\n",
      "Science Centre\n",
      "Wollongong, Australia\n",
      "fax: 61 42 213151   email: gkm@cc.uow.edu.au\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc548 = newsgroups_test.data[548]\n",
    "print(doc548)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44a865",
   "metadata": {},
   "source": [
    "Confirming the probs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "5479c0e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06956561, 0.78882479, 0.1416096 ]])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = clf.predict_proba(vectorizer.transform([doc548]))\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d76ee",
   "metadata": {},
   "source": [
    "##  Precision by class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9d3d1",
   "metadata": {},
   "source": [
    "Here `clf` is the classifier trained above. Note the use of `average=None`.  This gets the class\n",
    "by class precision results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "38f6c8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98006645, 0.90510949, 0.95128205])"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(vectors_test)\n",
    "metrics.precision_score(newsgroups_test.target, pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d166e048",
   "metadata": {},
   "source": [
    "Note the order of the arguments matters for precision.  Swapping predicted and true labelings changes the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "ba3e1199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92476489, 0.9562982 , 0.94162437])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(pred,newsgroups_test.target, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b60c64",
   "metadata": {},
   "source": [
    "The first order given is correct, as the documentation shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "35358df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the F1 score, also known as balanced F-score or F-measure.\n",
      "\n",
      "    The F1 score can be interpreted as a harmonic mean of the precision and\n",
      "    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
      "    The relative contribution of precision and recall to the F1 score are\n",
      "    equal. The formula for the F1 score is::\n",
      "\n",
      "        F1 = 2 * (precision * recall) / (precision + recall)\n",
      "\n",
      "    In the multi-class and multi-label case, this is the average of\n",
      "    the F1 score of each class with weighting depending on the ``average``\n",
      "    parameter.\n",
      "\n",
      "    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "        Ground truth (correct) target values.\n",
      "\n",
      "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "        Estimated targets as returned by a classifier.\n",
      "\n",
      "    labels : array-like, default=None\n",
      "        The set of labels to include when ``average != 'binary'``, and their\n",
      "        order if ``average is None``. Labels present in the data can be\n",
      "        excluded, for example to calculate a multiclass average ignoring a\n",
      "        majority negative class, while labels not present in the data will\n",
      "        result in 0 components in a macro average. For multilabel targets,\n",
      "        labels are column indices. By default, all labels in ``y_true`` and\n",
      "        ``y_pred`` are used in sorted order.\n",
      "\n",
      "        .. versionchanged:: 0.17\n",
      "           Parameter `labels` improved for multiclass problem.\n",
      "\n",
      "    pos_label : str or int, default=1\n",
      "        The class to report if ``average='binary'`` and the data is binary.\n",
      "        If the data are multiclass or multilabel, this will be ignored;\n",
      "        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "        scores for that label only.\n",
      "\n",
      "    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "        This parameter is required for multiclass/multilabel targets.\n",
      "        If ``None``, the scores for each class are returned. Otherwise, this\n",
      "        determines the type of averaging performed on the data:\n",
      "\n",
      "        ``'binary'``:\n",
      "            Only report results for the class specified by ``pos_label``.\n",
      "            This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "        ``'micro'``:\n",
      "            Calculate metrics globally by counting the total true positives,\n",
      "            false negatives and false positives.\n",
      "        ``'macro'``:\n",
      "            Calculate metrics for each label, and find their unweighted\n",
      "            mean.  This does not take label imbalance into account.\n",
      "        ``'weighted'``:\n",
      "            Calculate metrics for each label, and find their average weighted\n",
      "            by support (the number of true instances for each label). This\n",
      "            alters 'macro' to account for label imbalance; it can result in an\n",
      "            F-score that is not between precision and recall.\n",
      "        ``'samples'``:\n",
      "            Calculate metrics for each instance, and find their average (only\n",
      "            meaningful for multilabel classification where this differs from\n",
      "            :func:`accuracy_score`).\n",
      "\n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "\n",
      "    zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "        Sets the value to return when there is a zero division, i.e. when all\n",
      "        predictions and labels are negative. If set to \"warn\", this acts as 0,\n",
      "        but warnings are also raised.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    f1_score : float or array of float, shape = [n_unique_labels]\n",
      "        F1 score of the positive class in binary classification or weighted\n",
      "        average of the F1 scores of each class for the multiclass task.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    fbeta_score : Compute the F-beta score.\n",
      "    precision_recall_fscore_support : Compute the precision, recall, F-score,\n",
      "        and support.\n",
      "    jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "    multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "        sample.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    When ``true positive + false positive == 0``, precision is undefined.\n",
      "    When ``true positive + false negative == 0``, recall is undefined.\n",
      "    In such cases, by default the metric will be set to 0, as will f-score,\n",
      "    and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
      "    modified with ``zero_division``.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] `Wikipedia entry for the F1-score\n",
      "           <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import f1_score\n",
      "    >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "    >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "    >>> f1_score(y_true, y_pred, average='macro')\n",
      "    0.26...\n",
      "    >>> f1_score(y_true, y_pred, average='micro')\n",
      "    0.33...\n",
      "    >>> f1_score(y_true, y_pred, average='weighted')\n",
      "    0.26...\n",
      "    >>> f1_score(y_true, y_pred, average=None)\n",
      "    array([0.8, 0. , 0. ])\n",
      "    >>> y_true = [0, 0, 0, 0, 0, 0]\n",
      "    >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "    >>> f1_score(y_true, y_pred, zero_division=1)\n",
      "    1.0...\n",
      "    >>> # multilabel classification\n",
      "    >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "    >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "    >>> f1_score(y_true, y_pred, average=None)\n",
      "    array([0.66666667, 1.        , 0.66666667])\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086ed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
