{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhtQXWFRrWSd"
   },
   "source": [
    "# Text Classification:  Insults with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MejilF82-rRZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.naive_bayes as nb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "# Load libraries\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook we dive into the general question of feature selection.  In the regression\n",
    "and classification module notebook **Dimensionality Reduction** we looked at the specific\n",
    "\n",
    "$$\n",
    "\\begin{array}[t]{lll}\n",
    "1. &  \\text{fclassif} &  \\text{ANOVA F-value between label/feature for classification tasks.}\\\\\n",
    "2. &  \\text{chi2} & \\text{Chi-squared stat for non-negative features for classification tasks.}\\\\\n",
    "3. &  \\text{mutual_info_classif} &  \\text{Mutual information for a discrete target.}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Mutual Information, written $\\text{I}$, is defined to be:\n",
    "\n",
    "$$\n",
    "\\text{I}\\,(X;\\,Y) = \\text{D}_{\\text{KL}}\\,(\\, \\text{P}_{X,Y} \\mid\\mid \\text{P}_{X}\\otimes \\text{P}_{Y}\\, )\n",
    "$$\n",
    "\n",
    "\n",
    "where $D_{\\text{KL}}$ is the Kullback-Leibler Divergence (KL-Divergence) of two distributions and\n",
    "$\\text{P}_{X,Y}$ is the joint distribution of X and Y and $\\text{P}_{X}\\otimes \\text{P}_{Y}$ is \n",
    "the distribution that gives $P(X) \\times P(Y)$ as the joint probability of X and Y.\n",
    "KL Divergence is an Information Theoretic measure of the divergence\n",
    "between two distributions, $\\text{I}\\,(X;\\,Y)$ measures the distance between\n",
    "the joint distribution and the distribution that would obtain\n",
    "if X and Y were completely independent.  If X and Y are independent,\n",
    "$\\text{I}\\,(X;\\,Y)$ is 0.  If X conditionally depends on Y that\n",
    "increases the difference between the joint distribution and independence ($\\text{P}_{X}\\otimes \\text{P}_{Y}$).\n",
    "\n",
    "Thus $\\text{I}\\,(x;\\,y)$ measures the amount of information you gain about the outcome of $Y$\n",
    "if you know the outcome $X$ \n",
    "\n",
    "A Chi-square test is a hypothesis testing method. The one relevant to our problem\n",
    "is using a Chi-square test to check if the observed frequencies of some category match expected frequencies. In our setting:  For each feature value v for a feature F and for each class c we want to \n",
    "see if the number of members of class c exhibiting value v for F is greater than expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a toy example to demonstrate the central idea that all 3 feature selectors\n",
    "implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "        n_samples=100, n_features=10, n_informative=2, n_clusters_per_class=1, \n",
    "        shuffle=False, random_state=42)\n",
    "chi2_stats, p_values = chi2(X, y)\n",
    "chi2_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers show that all but 2 of the features have little connection with the classification\n",
    "problem.  Of course we set it up this way when we created `X, y` with `make_classification`  by\n",
    "setting the parameter `n_informative` to 2.\n",
    "\n",
    "From Wikipedia: ``Pearson's chi-squared test is used to determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more categories of a contingency table. For contingency tables with smaller sample sizes, a Fisher's exact test is used instead.''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qildTjvw-rRb"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28jZSTW_-rRb"
   },
   "source": [
    "Let's open the CSV file with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MtGQlB1q-rRc"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "site = 'https://raw.githubusercontent.com/gawron/python-for-social-science/master/'\\\n",
    "'text_classification/'\n",
    "#site = 'https://gawron.sdsu.edu/python_for_ss/course_core/book_draft/_static/'\n",
    "df = pd.read_csv(os.path.join(site,\"troll.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3ncLUYx-rRc"
   },
   "source": [
    "Each row is a comment  taken from a blog or online forum. There are three columns: whether the comment is insulting (1) or not (0), the data, and the unicode-encoded contents of the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pFeNaW-m-rRd",
    "outputId": "0d5e5e36-697f-4fd8-ff00-ef8d164a3c35"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>\"you are both morons and that is never happening\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>\"You're all upset, defending this hipster band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult                                            Comment\n",
       "3942       1  \"you are both morons and that is never happening\"\n",
       "3943       0  \"Many toolbars include spell check, like Yahoo...\n",
       "3944       0  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...\n",
       "3945       0  \"How about Felix? He is sure turning into one ...\n",
       "3946       0  \"You're all upset, defending this hipster band..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Insult', 'Comment']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H-fPVjYV-rRl"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets FIRST\n",
    "T_train,T_test, y_train,y_test = train_test_split(df['Comment'],df['Insult'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imported the `chi2` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.feature_selection._univariate_selection.chi2(X, y)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's ine way to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "4JLT1QylrWSn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2960 13829\n"
     ]
    }
   ],
   "source": [
    "#tf = text.TfidfVectorizer(min_df=2,max_df=.8)\n",
    "sublinear_tf = True\n",
    "#subliner_tf = False\n",
    "tf = text.TfidfVectorizer(sublinear_tf=sublinear_tf)\n",
    "# Train your vectorizer oNLY on the trainingh data.\n",
    "X_train = tf.fit_transform(T_train)\n",
    "print(*X_train.shape)\n",
    "# N features with highest chi-squared statistics are selected\n",
    "# chi2 is a functiomported above\n",
    "chi2_features = SelectKBest(chi2, k = 10_000)\n",
    "X_train_chi = chi2_features.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IO9ykCgdrWSn"
   },
   "source": [
    "`X-train` is our **term-document** matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUihdpHIgA7e",
    "outputId": "3a69016d-c65e-4994-e021-3451880e8b00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2960, 13829), (2960, 10000))"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_train_chi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0wp6RbS-rRo"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88mG_1C0-rRo"
   },
   "source": [
    "Now, we are going to train a classifier as usual. We\n",
    "have already split the data and labels into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf-3XQDN-rRo"
   },
   "source": [
    "We use an **SVM classifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "uj7vvTS--rRo",
    "outputId": "c3be5d3a-7801-4311-d8da-45c6a267b36e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf_chi = LinearSVC()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "clf_chi.fit(X_train_chi, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMWe6ZCDrWSo"
   },
   "source": [
    "And we're done.  How'd we do?  Now we  test on the test set.  Before we can do that we need to\n",
    "vectorize the test set.  But don't just copy what we did with the training data:\n",
    "\n",
    "```\n",
    "X_test = tf.fit_transform(T_test)\n",
    "```\n",
    "\n",
    "That would retrain the vectorizer from scratch.  Any words that occurred in the training texts\n",
    "but not in the test texts would be forgotten!  Plus training the vectorizer\n",
    "is part of the classifier training pipeline.  If we let the vectorizer see\n",
    "the test data during its training phase, we'd be compromising the whole\n",
    "idea of splitting training and test data.  So what we want to do\n",
    "with the test data is just apply the transform part of vectorizing:\n",
    "\n",
    "```\n",
    "X_test = tf.transform(T_test)\n",
    "```\n",
    "\n",
    "That is, build a representation of the test data using only the vocabulary you learned\n",
    "about in training.  Ignore any new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rewbP2vT-rRp",
    "outputId": "fa1e71c6-3712-443d-a90f-49faa0d4f013"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8358662613981763, 0.8297872340425532)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = tf.transform(T_test)\n",
    "X_test_chi = chi2_features.transform(X_test)\n",
    "clf.score(X_test, y_test),clf_chi.score(X_test_chi, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, not a reliable result.  But at least trimming down the model didn't seem to hurt it.\n",
    "\n",
    "Let's clean this all up a bit by putting everything in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8409321175278622, 0.5478927203065134, 0.7857142857142857)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "\n",
    "#from sklearn import decomposition as dec\n",
    "#from pipeline import make_pipeline\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#poly = preprocessing.PolynomialFeatures(degree=20, include_bias=True)\n",
    "#scaler = preprocessing.StandardScaler()\n",
    "#lin_reg2 = linear_model.LinearRegression()\n",
    "\n",
    "X,y = df['Comment'].values,df['Insult'].values\n",
    "\n",
    "def make_tf_feat_selection_clf_pipeline (selection_function=None, k=5_000,\n",
    "                                         selector = SelectKBest,sublinear_tf=False):\n",
    "    tf = text.TfidfVectorizer(sublinear_tf=sublinear_tf)\n",
    "    #chi2_features = SelectKBest(selector, k)\n",
    "    if selection_function is not None:\n",
    "        k_best_features = selector(selection_function, k=k)\n",
    "    else:\n",
    "        k_best_features = selector(k=k)\n",
    "    svm_clf = LinearSVC()\n",
    "    return pipeline.Pipeline([('vect', tf), ('feat_selector', k_best_features), ('svm', svm_clf)])\n",
    "\n",
    "#pipeline_reg = make_tf_feat_selection_clf_pipeline (chi2,k=5_000)\n",
    "#pipeline_reg = make_tf_feat_selection_clf_pipeline (mutual_info_classif,k=5_000)\n",
    "pipeline_reg = make_tf_feat_selection_clf_pipeline (selection_function=f_classif, k=5_000,sublinear_tf=True)\n",
    "\n",
    "# Train\n",
    "T_train,T_test, y_train,y_test = train_test_split(X,y)\n",
    "pipeline_reg.fit(T_train, y_train)\n",
    "predicted = pipeline_reg.predict(T_test)\n",
    "\n",
    "accuracy_score(predicted, y_test),\\\n",
    "precision_score(predicted, y_test),\\\n",
    "recall_score(predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f_classif'"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_classif.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_classif\n",
      "===================\n",
      "k= 3_000 a=0.840 p=0.554 r=0.770\n",
      "k= 5_000 a=0.834 p=0.541 r=0.762\n",
      "k=10_000 a=0.832 p=0.586 r=0.731\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "stats = np.zeros((num_runs,3))\n",
    "#selection_function = chi2\n",
    "selection_function = f_classif\n",
    "sublinear_tf=True\n",
    "print(selection_function.__name__,end=\"\\n===================\\n\")\n",
    "for k in (3_000,5_000, 10_000):\n",
    "    for test_run in range(num_runs):\n",
    "        pipeline_reg = make_tf_feat_selection_clf_pipeline (selection_function = selection_function,k=k,\n",
    "                                                           sublinear_tf=sublinear_tf)\n",
    "        T_train,T_test, y_train,y_test = train_test_split(X,y)\n",
    "         \n",
    "        # Train\n",
    "        pipeline_reg.fit(T_train, y_train)\n",
    "        # Test\n",
    "        predicted = pipeline_reg.predict(T_test)\n",
    "        stats[test_run] = accuracy_score(predicted, y_test),\\\n",
    "                            precision_score(predicted, y_test),\\\n",
    "                             recall_score(predicted, y_test)\n",
    "\n",
    "    stats_mn = stats.mean(axis=0)\n",
    "    a,p,r = stats_mn\n",
    "\n",
    "    print(f\"{k=:>6_d} {a=:.3f} {p=:.3f} {r=:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2\n",
      "===================\n",
      "k= 3_000 a=0.837 p=0.555 r=0.774\n",
      "k= 5_000 a=0.836 p=0.568 r=0.759\n",
      "k=10_000 a=0.837 p=0.592 r=0.744\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "stats = np.zeros((num_runs,3))\n",
    "selection_function = chi2\n",
    "#selection_function = f_classif\n",
    "sublinear_tf=True\n",
    "\n",
    "print(selection_function.__name__,end=\"\\n===================\\n\")\n",
    "for k in (3_000,5_000, 10_000):\n",
    "    for test_run in range(num_runs):\n",
    "        pipeline_reg = make_tf_feat_selection_clf_pipeline (selection_function = selection_function,k=k)\n",
    "        T_train,T_test, y_train,y_test = train_test_split(X,y)\n",
    "\n",
    "        # Train\n",
    "        pipeline_reg.fit(T_train, y_train)\n",
    "        # Test\n",
    "        predicted = pipeline_reg.predict(T_test)\n",
    "        stats[test_run] = accuracy_score(predicted, y_test),\\\n",
    "                            precision_score(predicted, y_test),\\\n",
    "                             recall_score(predicted, y_test)\n",
    "\n",
    "    stats_mn = stats.mean(axis=0)\n",
    "    a,p,r = stats_mn\n",
    "\n",
    "    print(f\"{k=:>6_d} {a=:.3f} {p=:.3f} {r=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information can work like the others but used in the context of select KBest\n",
    "it requires discrete-valued features, so we need a slightly different pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb= SelectKBest(mutual_info_classif)\n",
    "\n",
    "# You can try these others with this pipeline as well.\n",
    "#kb= SelectKBest(f_classif)\n",
    "#kb= SelectKBest(chi2)\n",
    "\n",
    "T_train,T_test, y_train,y_test = train_test_split(X,y)\n",
    "         \n",
    "svm_clf = LinearSVC()\n",
    "\n",
    "cv = CountVectorizer()\n",
    "tf = text.TfidfTransformer(sublinear_tf=True)\n",
    "\n",
    "pipeline_reg=pipeline.Pipeline([('vect', cv), ('feat_selector', kb),\n",
    "                                ('tfidf',tf), ('svm', svm_clf)])\n",
    "# Train\n",
    "pipeline_reg.fit(T_train, y_train)\n",
    "# Test\n",
    "predicted = pipeline_reg.predict(T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 3_000 a=0.806 p=0.789 r=0.350\n"
     ]
    }
   ],
   "source": [
    "a,p,r = accuracy_score(y_test,predicted,),\\\n",
    "        precision_score(y_test, predicted),\\\n",
    "            recall_score(y_test, predicted)\n",
    "\n",
    "# MI k= 3_000 a=0.806 p=0.789 r=0.350\n",
    "print(f\"{k=:>6_d} {a=:.3f} {p=:.3f} {r=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to investigate getting around this limitation so let's try coding this up a\n",
    "little differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "\n",
    "We will use Mutual Information two ways\n",
    "\n",
    "1.  Pass in a sequence of strings add select a vocab using word counts (operating on a Count Vectorized representation of the docs like we did above)\n",
    "2.  Operate on a TFIDF TDM to do feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "class MutualInfo:\n",
    "    \n",
    "    def  __init__(self,k,use_counts=True):\n",
    "        self.k =k\n",
    "        self.use_counts= use_counts\n",
    "        \n",
    "    def fit_transform(self,X,y):\n",
    "        \"\"\"\n",
    "        If self.use_counts create  cv, a count vectorized version of X,\n",
    "        and assign feature ranks based mutual info of cv[:,feat_i] and y\n",
    "        Use the feature ranks to choose a vocab of size self.k.\n",
    "        \n",
    "        Otherwise assign feature ranks based mutual info of X[:,feat_i] and y\n",
    "        Use the feature ranks to choose a vocab of size self.k.\n",
    "        \"\"\"\n",
    "        #X, y = make_classification(\n",
    "        # n_samples=100, n_features=10, n_informative=2, n_clusters_per_class=1, \n",
    "        #shuffle=False, random_state=42)\n",
    "        if hasattr(X,'toarray'):\n",
    "            X = X.toarray()\n",
    "            #print(type(X))\n",
    "        if self.use_counts:\n",
    "            self.cv = CountVectorizer()\n",
    "            XC = self.cv.fit_transform(X)#.toarray()\n",
    "            ranks = mutual_info_classif(XC, y)\n",
    "            self.idxs = ranks.argsort()[-self.k:][::-1]\n",
    "            self.vocabulary_ = [wd for (wd,idx) in self.cv.vocabulary_.items() if idx in self.idxs]\n",
    "            return self.vocabulary_\n",
    "        else:\n",
    "            ranks = mutual_info_classif(X, y)\n",
    "            self.idxs = ranks.argsort()[-self.k:]\n",
    "            return self.transform(X)\n",
    "\n",
    "    \n",
    "    def transform (self, X):\n",
    "        if X.ndim == 2:\n",
    "            return X[:,self.idxs]\n",
    "        else:\n",
    "            return X[self.idxs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy test of `MutualInfo` class with use_counts = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "[-1.17278867  0.63356167  0.35137231  0.18646621  0.95400176  0.65139125\n",
      " -0.31526924  0.75896922 -0.77282521 -0.23681861]\n",
      "[ 0.35137231 -1.17278867]\n",
      "(100, 2)\n",
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "X0, y0 = make_classification(\n",
    "        n_samples=100, n_features=10, n_informative=2, n_clusters_per_class=1, \n",
    "        shuffle=False, random_state=42)\n",
    "print(X0.shape)\n",
    "mi = MutualInfo(k=2,use_counts=False)\n",
    "newX = mi.fit_transform(X0,y0)\n",
    "print(X0[0])\n",
    "print(newX[0])\n",
    "print(newX.shape)\n",
    "svm_clf_mi = LinearSVC()\n",
    "svm_clf_mi.fit(newX,y0)\n",
    "svm_clf = LinearSVC()\n",
    "svm_clf.fit(X0,y0)\n",
    "\n",
    "# No loss of info with just two feats (because we set it up that way)\n",
    "print(svm_clf.score(X0,y0),svm_clf_mi.score(newX,y0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def split_fit_and_eval_mutual_info (X,y,k,use_counts=True):\n",
    "    # Split train test\n",
    "    # X is a seq of strings, not a pd.Series inst.\n",
    "    # y is a seq of lbls\n",
    "    T_train,T_test, y_train,y_test = train_test_split(X,y)\n",
    "\n",
    "    mi = MutualInfo(k=k,use_counts=use_counts)\n",
    "    ###  Find Mutual Info Vocab in use counts case\n",
    "    if use_counts:\n",
    "        print(\"Training new vocab to classification task with MI\")\n",
    "        vocab = mi.fit_transform(T_train,y_train)\n",
    "        print(f\"{len(vocab)=} {k=}\")\n",
    "    else:\n",
    "        vocab=None\n",
    "\n",
    "    # Instantiate Vectorizer with new vocab\n",
    "    tf = text.TfidfVectorizer(vocabulary=vocab)\n",
    "    XM = tf.fit_transform(T_train)\n",
    "    XT = tf.transform(T_test)\n",
    "    \n",
    "    if not use_counts:\n",
    "        XM = mi.fit_transform(XM,y_train)\n",
    "        X_test = mi.transform(XT)\n",
    "    else:\n",
    "        X_test = XT\n",
    "\n",
    "    # Train clf with TD Matrix fitted to new vocab\n",
    "    svm_clf = LinearSVC()\n",
    "    svm_clf.fit(XM,y_train)\n",
    "\n",
    "    predictions = svm_clf.predict(X_test)\n",
    "    a,p, r = accuracy_score(y_test,predictions),\\\n",
    "                                precision_score(y_test, predictions),\\\n",
    "                                 recall_score(y_test, predictions)\n",
    "    return a,p,r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete feature selection.  Do feature selection based on word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new vocab to classification task with MI\n",
      "len(vocab)=3000 k=3000\n",
      "k= 3_000 a=0.828 p=0.749 r=0.510\n"
     ]
    }
   ],
   "source": [
    "X,y = df[\"Comment\"].values,df[\"Insult\"].values\n",
    "k,use_counts = 3_000,True\n",
    "\n",
    "a,p,r = split_fit_and_eval_mutual_info (X,y,k,use_counts=use_counts)\n",
    "#a 830 p=702 r=.579; a=0.823 p=0.725 r=0.560\n",
    "print(f\"{k=:>6_d} {a=:.3f} {p=:.3f} {r=:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 3_000 a=0.831 p=0.766 r=0.508\n"
     ]
    }
   ],
   "source": [
    "T_train,T_test, y_train,y_test = train_test_split(X,y)\n",
    "mi = MutualInfo(k=k,use_counts=True)\n",
    "vocab = mi.fit_transform(T_train,y_train)\n",
    "# This fn will implement feature selection by accpeting a pre-selected vocab/.\\\n",
    "a,p,r =  split_fit_and_eval_feature_selection (X,y,vocab=vocab)\n",
    "#a 830 p=702 r=.579\n",
    "print(f\"{k=:>6_d} {a=:.3f} {p=:.3f} {r=:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Continuous value feature selection.  Do feature selection based on TFIDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 3_000 a=0.826 p=0.699 r=0.528\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k,use_counts = 3_000,False\n",
    "\n",
    "a,p,r = split_fit_and_eval_mutual_info (X,y,k,use_counts=use_counts)\n",
    "#a=0.810 p=0.726 r=0.504; \n",
    "print(f\"{k=:>6_d} {a=:.3f} {p=:.3f} {r=:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 3_000 a=0.793 p=0.692 r=0.441\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Or if no vocab is supplied, it will implement feature selection via \n",
    "# mutual information on a matrix of continuous  TFIDF values \n",
    "a,p,r =  split_fit_and_eval_feature_selection (X,y,vocab= None)\n",
    "#a 830 p=702 r=.579\n",
    "print(f\"{k=:>6_d} {a=:.3f} {p=:.3f} {r=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provisionally not using counts i sthe winner, except on precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Refining the results\n",
    "\n",
    "Feature selection is a hyper-parameter. So we do it outside of the usual train test loop.\n",
    "\n",
    "That means our results are themselves tied to a single train test split, so we \n",
    "do feature selection multiple times.  After each feature selection, we run the usual\n",
    "train test loop (num_runs times)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple runs and use_counts = True\n",
    "\n",
    "  1.  Do one train test split to select a vocab with MI\n",
    "  2.  Use that vocab to vectorize, train, and test multiple train/test splits.\n",
    "  \n",
    "For multiple runs and use_counts = False\n",
    "  \n",
    "  1.  Do one train test split to create Vectorizer\n",
    "  2.  Do MI to select a vocab with that Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fit_and_eval_feature_selection (X,y,vocab=None):\n",
    "    \"\"\"\n",
    "    Either use a preselected vocab\n",
    "    or select features using mutual information.    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Split train test\n",
    "    # X is a seq of strings, not a pd.Series inst.\n",
    "    # y is a seq of lbls\n",
    "    T_train,T_test, y_train,y_test = train_test_split(X,y)\n",
    "\n",
    "    # Instantiate Vectorizer with new vocab if not None\n",
    "    # if vocab is None use all vocab\n",
    "    vectorizer = text.TfidfVectorizer(vocabulary=vocab)\n",
    "    X_train = vectorizer.fit_transform(T_train)\n",
    "    X_test = vectorizer.transform(T_test)\n",
    "    \n",
    "    if vocab is None:\n",
    "        mi = MutualInfo(k=k,use_counts=False)\n",
    "        X_train  = mi.fit_transform(X_train,y_train)\n",
    "        X_test = mi.transform(X_test)\n",
    "        \n",
    "\n",
    "    # Train clf with Term-Doc Matrix pruned to precomputed vocab or \n",
    "    # pruned by feature selection\n",
    "    svm_clf = LinearSVC()\n",
    "    svm_clf.fit(X_train,y_train)\n",
    "\n",
    "    predictions = svm_clf.predict(X_test)\n",
    "    a,p, r = accuracy_score(y_test,predictions),\\\n",
    "                                precision_score(y_test, predictions),\\\n",
    "                                 recall_score(y_test, predictions)\n",
    "    return a,p,r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Mutual Information to build a discrete model (based on word counts rather than some continuous significance measure),  Since our features are words.\n",
    "this amounts to having a vocabulary pre selected by Mutual Information. (use_counts=True)\n",
    "\n",
    "Each preselected vocabulary is then eavluated with 10 separate split and fit rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: 0\n",
      " expt: 0\n",
      " expt: 1\n",
      " expt: 2\n",
      " expt: 3\n",
      " expt: 4\n",
      " expt: 5\n",
      " expt: 6\n",
      " expt: 7\n",
      " expt: 8\n",
      " expt: 9\n",
      "split: 1\n",
      " expt: 0\n",
      " expt: 1\n",
      " expt: 2\n",
      " expt: 3\n",
      " expt: 4\n",
      " expt: 5\n",
      " expt: 6\n",
      " expt: 7\n",
      " expt: 8\n",
      " expt: 9\n",
      "split: 2\n",
      " expt: 0\n",
      " expt: 1\n",
      " expt: 2\n",
      " expt: 3\n",
      " expt: 4\n",
      " expt: 5\n",
      " expt: 6\n",
      " expt: 7\n",
      " expt: 8\n",
      " expt: 9\n",
      "split: 3\n",
      " expt: 0\n",
      " expt: 1\n",
      " expt: 2\n",
      " expt: 3\n",
      " expt: 4\n",
      " expt: 5\n",
      " expt: 6\n",
      " expt: 7\n",
      " expt: 8\n",
      " expt: 9\n",
      "split: 4\n",
      " expt: 0\n",
      " expt: 1\n",
      " expt: 2\n",
      " expt: 3\n",
      " expt: 4\n",
      " expt: 5\n",
      " expt: 6\n",
      " expt: 7\n",
      " expt: 8\n",
      " expt: 9\n",
      "split: 5\n",
      " expt: 0\n",
      " expt: 1\n",
      " expt: 2\n",
      " expt: 3\n",
      " expt: 4\n",
      " expt: 5\n",
      " expt: 6\n",
      " expt: 7\n",
      " expt: 8\n",
      " expt: 9\n",
      "split: 6\n",
      " expt: 0\n",
      " expt: 1\n",
      " expt: 2\n",
      " expt: 3\n",
      " expt: 4\n",
      " expt: 5\n",
      " expt: 6\n",
      " expt: 7\n",
      " expt: 8\n",
      " expt: 9\n",
      "split: 7\n",
      " expt: 0\n",
      " expt: 1\n",
      " expt: 2\n",
      " expt: 3\n",
      " expt: 4\n",
      " expt: 5\n",
      " expt: 6\n",
      " expt: 7\n",
      " expt: 8\n",
      " expt: 9\n",
      "split: 8\n",
      " expt: 0\n",
      " expt: 1\n",
      " expt: 2\n",
      " expt: 3\n",
      " expt: 4\n",
      " expt: 5\n",
      " expt: 6\n",
      " expt: 7\n",
      " expt: 8\n",
      " expt: 9\n",
      "split: 9\n",
      " expt: 0\n",
      " expt: 1\n",
      " expt: 2\n",
      " expt: 3\n",
      " expt: 4\n",
      " expt: 5\n",
      " expt: 6\n",
      " expt: 7\n",
      " expt: 8\n",
      " expt: 9\n",
      "k= 3_000 a=0.828 p=0.758 r=0.519\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "total_exps = num_runs**2\n",
    "scores = np.zeros((3,))\n",
    "\n",
    "for i in range(num_runs):\n",
    "    print(f\"\\nsplit: {i}\")\n",
    "    T_train,T_test, y_train,y_test = train_test_split(X,y)\n",
    "    mi = MutualInfo(k=k,use_counts=True)\n",
    "    vocab = mi.fit_transform(T_train,y_train)\n",
    "    \n",
    "    for j in range(num_runs):\n",
    "        print(f\"  expt: {j}\")\n",
    "        scores += split_fit_and_eval_feature_selection (X,y,vocab=vocab)\n",
    "    print()\n",
    "    \n",
    "a,p,r =  scores/total_exps\n",
    "print(f\"{k=:>6_d} {a=:.3f} {p=:.3f} {r=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the continuous version of Mutual Info.  This means doing the MI feature selection on term-document matrices containing TFIDF scores. This requires averaging between the scores of the 3 nearest neighbors to estimate the MI values.\n",
    "\n",
    "The continuous MI feature selection is expensive.  And rather than attempt to follow the paradigm above,\n",
    "(where we create 10 reduced feature models and evaluate each reduced model with 10 train test splits (= 100 expereiments), we simply  try 10 feature selections, each estimated from a different train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "split set: 0\n",
      "  expt: 0\n",
      "  expt: 1\n",
      "  expt: 2\n",
      "  expt: 3\n",
      "  expt: 4\n",
      "  expt: 5\n",
      "  expt: 6\n",
      "  expt: 7\n",
      "  expt: 8\n",
      "  expt: 9\n",
      "\n",
      "k= 3_000 a=0.800 p=0.686 r=0.464\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "total_exps = num_runs\n",
    "scores = np.zeros((3,))\n",
    "\n",
    "for i in range(1):\n",
    "    print(f\"\\nsplit set: {i}\")\n",
    "\n",
    "    #T_train,T_test, y_train,y_test = train_test_split(X,y)\n",
    "    #tf = text.TfidfVectorizer()\n",
    "    #X_train = tf.fit_transform(T_train)\n",
    "    #mi = MutualInfo(k=k,use_counts=False)\n",
    "    #X_train_T = mi.fit_transform(X_train,y_train)\n",
    "\n",
    "    for j in range(num_runs):\n",
    "        print(f\"  expt: {j}\")\n",
    "        scores += split_fit_and_eval_feature_selection (X,y,vocab=None)\n",
    "    print()\n",
    "\n",
    "a,p,r =  scores/total_exps\n",
    "\n",
    "print(f\"{k=:>6_d} {a=:.3f} {p=:.3f} {r=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a resounding defeat. The continuous models perform worse on all measures than the discrete\n",
    "models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using the transformer\n",
    "\n",
    "Get some of the redundancies out by simplifying the Mutual Information instance (MIi) interface.\n",
    "\n",
    "It **always** accepts a Term Doc Matrix and alqys outputs  a reduced form.\n",
    "\n",
    "To do feature selection on the base of counts, we pass in a count vectorizer \n",
    "TDM and pass back a truncated feat vectorizer.  We then pass that to a TFIDFTransformer.\n",
    "(which accepts a CountVectorized TDM).\n",
    "\n",
    "Model A\n",
    "----------\n",
    "\n",
    "T_Train, T_Test, y_Train, y_Test\n",
    "\n",
    "vec_pipe = [CountVectorizer =>  Mutual Info => TFIDFTransformer => LinearSVC]\n",
    "\n",
    "X_Train = vec_pipe.fit_transform(T_Train)\n",
    "predicted = vec_pipe.transform(T_test)\n",
    "\n",
    "\n",
    "Model B\n",
    "-----------\n",
    "\n",
    "```python\n",
    "T_Train, T_Test, y_Train, y_Test\n",
    "```\n",
    "\n",
    "vec_pipe = [TFIDFVectorizer =>  Mutual Info => LinearSVC]\n",
    "\n",
    "\n",
    "```python\n",
    "X_Train = vec_pipe.fit_transform(T_Train)\n",
    "predicted = vec_pipe.transform(T_test)\n",
    "```\n",
    "\n",
    "Note we switch from the TFIDFTransformer to the TFIDF Vectorizer,  The former acceots a count\n",
    "vectorized TD as input.  The latter wants a sequence of document strin gs.  Both\n",
    "out a TDM with TFIDF values.  \n",
    "\n",
    "CountVectorizer => TFIDFTransformer\n",
    "\n",
    "is equivalent  to\n",
    "\n",
    "TFIDFVectorizer\n",
    "\n",
    "The motivation for resorting to the  Transformer is so that we could interpose\n",
    "Mutual Information feature selection between Count Vectorizing and converting the\n",
    "counts to TFIDF values.  This allows MI selection to work on probabilities based on counts,\n",
    "more or less its original intent, and avoids averaging based on nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "class MutualInfo:\n",
    "    \n",
    "    def  __init__(self,k,discrete_features=True):\n",
    "        self.k =k\n",
    "        self.discrete_features = discrete_features\n",
    "        \n",
    "    def fit_transform(self,X,y):\n",
    "        \"\"\"\n",
    "        If self.use_counts create  cv, a count vectorized version of X,\n",
    "        and assign feature ranks based mutual info of cv[:,feat_i] and y\n",
    "        Use the feature ranks to choose a vocab of size self.k.\n",
    "        \n",
    "        Otherwise assign feature ranks based mutual info of X[:,feat_i] and y\n",
    "        Use the feature ranks to choose a vocab of size self.k.\n",
    "        \"\"\"\n",
    "        \n",
    "        if hasattr(X,'toarray'):\n",
    "            X = X.toarray()\n",
    "    \n",
    "        ranks = mutual_info_classif(X, y,discrete_features=self.discrete_features)\n",
    "        self.idxs = ranks.argsort()[-self.k:]\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform (self, X):\n",
    "        if X.ndim == 2:\n",
    "            return X[:,self.idxs]\n",
    "        else:\n",
    "            return X[self.idxs]\n",
    "\n",
    "\n",
    "sublinear_tf,k=True,3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 3_000 a=0.817 p=0.765 r=0.533\n"
     ]
    }
   ],
   "source": [
    "### Model A\n",
    "\n",
    "def make_model_A ():\n",
    "    cv = CountVectorizer()\n",
    "    mi = MutualInfo(k=k,discrete_features=True)\n",
    "    tf = text.TfidfTransformer(sublinear_tf=sublinear_tf)\n",
    "    svm_clf = LinearSVC()\n",
    "\n",
    "    return pipeline.Pipeline([('vect', cv), \n",
    "                              ('feat_selector', mi), \n",
    "                              ('tfidf_vect',tf), \n",
    "                              ('svm', svm_clf)])\n",
    "\n",
    "# Data \n",
    "X,y = df[\"Comment\"].values,df[\"Insult\"].values\n",
    "T_train,T_test, y_train,y_test = train_test_split(X,y)\n",
    "\n",
    "#Train\n",
    "vec_pipe = make_model_A ()\n",
    "vec_pipe.fit(T_train, y_train)\n",
    "#Test\n",
    "predicted = vec_pipe.predict(T_test)\n",
    "\n",
    "a,p, r = accuracy_score(y_test,predicted),\\\n",
    "           precision_score(y_test, predicted),\\\n",
    "              recall_score(y_test, predicted)\n",
    "\n",
    "print(f\"{k=:>6_d} {a=:.3f} {p=:.3f} {r=:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 3_000 a=0.785 p=0.678 r=0.482\n"
     ]
    }
   ],
   "source": [
    "# Model B\n",
    "\n",
    "def make_model_B():\n",
    "    mi = MutualInfo(k=k, discrete_features=False)\n",
    "    tf = text.TfidfVectorizer(sublinear_tf=sublinear_tf)\n",
    "    svm_clf = LinearSVC()\n",
    "\n",
    "    return pipeline.Pipeline([('tfidf_vect',tf),  ('feat_selector', mi), ('svm', svm_clf)])\n",
    "\n",
    "# Data \n",
    "X,y = df[\"Comment\"].values,df[\"Insult\"].values\n",
    "T_train,T_test, y_train,y_test = train_test_split(X,y)\n",
    "\n",
    "#Train\n",
    "vec_pipe =  make_model_B()\n",
    "vec_pipe.fit(T_train, y_train)\n",
    "#Test\n",
    "predicted = vec_pipe.predict(T_test)\n",
    "\n",
    "a,p, r = accuracy_score(y_test, predicted),\\\n",
    "           precision_score(y_test, predicted),\\\n",
    "              recall_score(y_test, predicted)\n",
    "\n",
    "print(f\"{k=:>6_d} {a=:.3f} {p=:.3f} {r=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search + Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of dimensionality reduction\n",
    "\n",
    "Model\n",
    "\n",
    "[TFDIF -> TruncatedSVD -> SVM]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Trucated SVD for reduction\n",
      "Beginning reduction fit print 2024-04-19 15:04:00.292262 \n",
      "Reduction fit completed 2024-04-19 15:04:01.904266 \n",
      "k=   500 a=0.839 p=0.698 r=0.615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF, PCA, TruncatedSVD\n",
    "# Avoiding SparsePCA which seems ill-behaved\n",
    "from datetime import datetime\n",
    "\n",
    "## Data \n",
    "X,y = df[\"Comment\"].values,df[\"Insult\"].values\n",
    "T_train,T_test, y_train,y_test = train_test_split(X,y)\n",
    "\n",
    "## Reduction params\n",
    "k,sparse_pca=500,False\n",
    "#red = TruncatedSVD(n_components=k)\n",
    "#red = NMF(n_components=k)\n",
    "\n",
    "if sparse_pca:\n",
    "    print(\"Using sparse PCA for reduction\")\n",
    "    red = SparsePCA(n_components=k)\n",
    "else:\n",
    "    print(\"Using Trucated SVD for reduction\")\n",
    "    red = TruncatedSVD(n_components=k)\n",
    "# Data \n",
    "\n",
    "#TSVD k= 500 a=0.823 p=0.716 r=0.580\n",
    "\n",
    "######  Text -> TFIDF\n",
    "vectorizer = text.TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(T_train)\n",
    "X_test = vectorizer.transform(T_test)\n",
    "\n",
    "if sparse_pca:\n",
    "    X_train = X_train.toarray()\n",
    "    X_test = X_test.toarray()\n",
    "\n",
    "#########\n",
    "\n",
    "###### TFIDF -> reduced\n",
    "print(f\"Beginning reduction fit print {datetime.now()} \")\n",
    "X_train_red = red.fit_transform(X_train)\n",
    "print(f\"Reduction fit completed {datetime.now()} \")\n",
    "X_test_red = red.transform(X_test)\n",
    "############\n",
    "\n",
    "### Reduced =>  Class\n",
    "\n",
    "svm_clf = LinearSVC()\n",
    "svm_clf.fit(X_train_red,y_train)\n",
    "predicted = svm_clf.predict(X_test_red)\n",
    "\n",
    "### Eval\n",
    "a,p, r = accuracy_score(y_test, predicted),\\\n",
    "           precision_score(y_test, predicted),\\\n",
    "              recall_score(y_test, predicted)\n",
    "\n",
    "print(f\"{k=:>6_d} {a=:.3f} {p=:.3f} {r=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Putting it all together:  The Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two sets of different techniques for reducing the size of document representations\n",
    "and hopefully improving classifier performance: dimensionality redction and feature selection.\n",
    "But which reduction technique shoudl we use, and how man features should we keep?\n",
    "\n",
    "Answering questions like these is what the scikit learn grid search package was designed for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", text.TfidfVectorizer()),\n",
    "        # the reduce_dim stage is populated by the param_grid\n",
    "        (\"reduce_dim\", \"passthrough\"),\n",
    "        (\"classify\", LinearSVC()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "N_FEATURES_OPTIONS = [5_000, 3_000, 500]\n",
    "C_OPTIONS = [1, 10, 100]\n",
    "#DIM_REDUCERS = [TruncatedSVD(), NMF(max_iter=1_000)]\n",
    "DIM_REDUCERS = [TruncatedSVD()]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"reduce_dim\": DIM_REDUCERS,\n",
    "        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n",
    "        \"classify__C\": C_OPTIONS,\n",
    "    },\n",
    "    {\n",
    "        \"reduce_dim\": [SelectKBest(chi2),SelectKBest(f_classif)],\n",
    "        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n",
    "        \"classify__C\": C_OPTIONS,\n",
    "    },\n",
    "]\n",
    "\n",
    "# This is the default\n",
    "nfolds=5\n",
    "\n",
    "grid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid, scoring=\"f1\",cv=nfolds)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reducer_labels = [\"SVD\", \"KBest(chi2)\",\"KBest(f_classif)\"]\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_[\"mean_test_score\"])\n",
    "# scores are in the order of param_grid iteration, which is alphabetical\n",
    "mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n",
    "# select score for best C\n",
    "mean_scores = mean_scores.max(axis=0)\n",
    "# create a dataframe to ease plotting\n",
    "mean_scores = pd.DataFrame(\n",
    "    mean_scores.T, index=N_FEATURES_OPTIONS, columns=reducer_labels\n",
    ")\n",
    "\n",
    "ax = mean_scores.plot.bar()\n",
    "ax.set_title(\"Comparing feature reduction techniques\")\n",
    "ax.set_xlabel(\"Reduced number of features\")\n",
    "ax.set_ylabel(\"F-Score\")\n",
    "ax.set_ylim((0, 1))\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion questions\n",
    "\n",
    "1.  How many experiments did the grid search do?  Note this depends both on the param grid and the number of \"folds\" in the cross validation strategy.\n",
    "2. How many distinct grid points are represented in the plot?  \n",
    "3. Where did the others go?\n",
    "4. What feature has been left out of plot? \n",
    "5. What is the best combination of features?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Insults_with_Naive_Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "94px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
