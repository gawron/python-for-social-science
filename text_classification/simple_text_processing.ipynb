{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056e49b5",
   "metadata": {},
   "source": [
    "## Text data on the web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8416f",
   "metadata": {},
   "source": [
    "## Intro: The basic idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed90bbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://gutenberg.org/cache/epub/31100/pg31100.txt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://gutenberg.org/cache/epub/31100/pg31100.txt\"\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6236651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "# This happens to be Frankenstein, the most downloaded of all Gutenberg books on the day this\n",
    "# NB was created.\n",
    "idx=\"31100\"\n",
    "url = f\"https://gutenberg.org/cache/epub/{idx}/pg{idx}.txt\"\n",
    "\n",
    "with urllib.request.urlopen(url) as stream:\n",
    "    byte_str = stream.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0f0a05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f04f6ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xef\\xbb\\xbfThe Project Gutenberg eBook of The Complete Project Gutenberg Works of Jane Austen\\r\\n    \\r\\nThis eb'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_str[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f2fa1fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(byte_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a04558d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_str[0]b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "656caf33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(14*16)+15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b57a5a2",
   "metadata": {},
   "source": [
    "It's not a string because it hasn't been decoded from \"UTF-8\" into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "70af58b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=byte_str.decode(\"UTF8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78653df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xef\\xbb\\xbfThe Project Gutenberg eBook of The Complete Project Gutenberg Works of Jane Austen\\r\\n    \\r\\nThis eb'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_str[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba33c9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0709c624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of Frankenstein; Or, The Modern Prometheus\\r\\n    \\r\\nThis ebook is for the'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e237e36",
   "metadata": {},
   "source": [
    "Make a function implementing the idea (retrieval of books from Gutenberg.org by book index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cef31c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def get_book (ind):\n",
    "    \"\"\"\n",
    "    We replace \"\\r\\n\" (Mac rep for newlines) with Windows rep (\"\\n\")\n",
    "    to facilate regexp matching across newline barriers, but this\n",
    "    is only a pathc on a bigger problem.\n",
    "    \"\"\"\n",
    "    url = f\"https://gutenberg.org/cache/epub/{ind}/pg{ind}.txt\"\n",
    "    with urllib.request.urlopen(url) as stream:\n",
    "        byte_str = stream.read()\n",
    "        return byte_str.decode(\"UTF8\").replace(\"\\r\\n\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c80b8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get book 84 (= Frankenstein) from Gutenberg\n",
    "text2 = get_book(84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dcda7688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of Frankenstein; Or, The Modern Prometheus\\n    \\nThis ebook is for the u'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "728cf55e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text == text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87c141",
   "metadata": {},
   "source": [
    "##  Get data pointers from a trusted source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac067c73",
   "metadata": {},
   "source": [
    "The [Gutenberg.org page](https://gutenberg.org/browse/scores/top) contains a list of the 100\n",
    "most downloaded books, including *Frankenstein*, which we just downloaded in our introductory section,\n",
    "and *Pride and Prejudice*, which has shown up in a lot of our examples.\n",
    "\n",
    "Let's get the list and compute some statistics from that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1e1d942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n <meta charset=\"UTF-8\"/>\\n\\n<title>Top 100 | Project Gutenberg</title>\\n <link rel=\"stylesheet\" href=\"/gutenberg/style.css?v=1.1\">\\n <link rel=\"stylesheet\" href=\"/gutenberg/collapsible.css?1.1\">\\n <link rel=\"stylesheet\" href=\"/gutenberg/new_nav.css?v=1.321231\">\\n<link rel=\"stylesheet\" href=\"/gutenberg/pg-desktop-one.css\">\\n <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n <meta name=\"keywords\" content=\"books, ebo'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_doc[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfb5d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url2 = \"https://gawron.sdsu.edu/\"\n",
    "#book_list = \"https://gutenberg.org/browse/scores/top\"\n",
    "with urllib.request.urlopen(url2) as stream:\n",
    "    html_doc = stream.read()\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4c634dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Jean Mark Gawron</title>\n",
      "****\n",
      "<div>\n",
      "<p align=\"left\">\n",
      "<h1>Jean Mark Gawron</h1>\n",
      "</p>\n",
      "</div>\n",
      "****\n",
      "<p align=\"left\">\n",
      "<h1>Jean Mark Gawron</h1>\n",
      "</p>\n",
      "****\n",
      "<h1>Jean Mark Gawron</h1>\n",
      "****\n",
      "<div>\n",
      "<p>\n",
      "<table border=\"0\" cellpadding=\"10\" cellspacing=\"0\" width=\"100%\">\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "<td colspan=\"2\">\n",
      " \n",
      "</td>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "\n",
      "<tr>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<table bgcolor=\"lightgray\" width=\"210\">\n",
      "<tr>\n",
      "<td>\n",
      "<a href=\"#classes\">Classes</a> <br/><br/>\n",
      "<a href=\"bulba.sdsu.edu/compling-program/research/gawron/\">Research</a><br/> <br/>\n",
      "<a href=\"http://linguistics.sdsu.edu/certificates/index.html\">Computational Linguistics Certificates</a><br/><br/>\n",
      "<a href=\"key.asc\">PGP Public key</a><br/><br/>\n",
      "<a href=\"key_with_photo.asc\">PGP key + Photo </a> <br/><br/>\n",
      "<a href=\"GPG_explanation.html\">Explaining PGP</a>\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</td>\n",
      "<td valign=\"top\">\n",
      "<img src=\"gawron2.jpg\" width=\"400\"/><br/>\n",
      "</td>\n",
      "<td>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Department of Linguistics and Asian/Middle Eastern Languages<br/>\n",
      "San Diego State University <br/>\n",
      "5500 Campanile Dr. <br/>\n",
      "San Diego, CA 92182-7727<br/>\n",
      "<br/>\n",
      "</td>\n",
      "<td colspan=\"2\">\n",
      "<p>\n",
      "\"I have come to believe that the whole world is an enigma, a harmless enigma\n",
      "that is made terrible by our own mad attempt to interpret it as though it had\n",
      "an underlying truth.\"\n",
      "-- Umberto Eco<br/>\n",
      "</p>\n",
      "<p>\n",
      "\"And now with some pleasure I find that it's seven; and must cook dinner. Haddock and sausage meat. I think it is true that one gains a certain hold on sausage and haddock by writing them down.\" -- The last entry in Virginia Woolf's diary<br/>\n",
      "</p></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"4\">\n",
      "<ul>\n",
      "<h3>Computational Linguistics  and Text Analytics Certificates</h3>\n",
      "<p>\n",
      "<p>\n",
      "<table width=\"80%\">\n",
      "<tr>\n",
      "<td>Text Analytics</td>\n",
      "<td>\n",
      "The Computational Linguistics program at SDSU is\n",
      "offering a new minor and a new certificate in Text Analytics for\n",
      "which undergrads, grads, and community members may qualify.\n",
      "<a href=\"http://gawron.sdsu.edu/text_analytics/\">Details</a>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Comp Ling Cert</td>\n",
      "<td>\n",
      "The Computational Linguistics program at SDSU is\n",
      "offering a new certificate in Computational Linguistics for\n",
      "which undergrads, grads, and community members may qualify.\n",
      "<a href=\"CompLingCert.rtf\">Details</a>\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</p></p></ul></td>\n",
      "</tr>\n",
      "<tr>\n",
      "</tr>\n",
      "\n",
      "</table>\n",
      "\n",
      "<div>\n",
      "<ul>\n",
      "<table bgcolor=\"lightgray\" border=\"\" fgcolor=\"#ffffff\" width=\"85%\">\n",
      "<tr>\n",
      "<td bgcolor=\"white\">\n",
      "  \n",
      "</td>\n",
      "<td bgcolor=\"white\">\n",
      "<h2><a name=\"Classes\"> Courses</a> </h2>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "Spring 2022\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 572\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"python_for_ss/\">\n",
      "   Python for Social Science</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Coping with biggish data\n",
      "</td>\n",
      "</tr>\n",
      "</table></td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<tr>\n",
      "<td>\n",
      "Fall 2020\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 522\n",
      "</td>\n",
      "<td valign=\"top\">  \n",
      "See Blackboard\n",
      "</td>\n",
      "<td>   </td><td valign=\"top\">\n",
      "Structure: Why put up with it?\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 572\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"python_for_ss/\">\n",
      "   Python for Social Science</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Coping with biggish data\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "Spring 2020\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 525\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"semantics/\">\n",
      "   Intro to semantics</a> </td>\n",
      "<td>\n",
      "</td>\n",
      "<td valign=\"top\">\n",
      "Meaning: What is it?\n",
      "</td>\n",
      "</tr>\n",
      "</table></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 581\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"compling/\">\n",
      "   Comp Ling Intro</a>\n",
      "</td><td>\n",
      "</td>\n",
      "<td valign=\"top\">\n",
      "Computing with words &amp; linguistic structure\n",
      "</td>\n",
      "</tr>\n",
      "\n",
      "\n",
      "\n",
      "<tr>\n",
      "<td>\n",
      "Fall 2019\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 572\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"python_for_ss/\">\n",
      "   Python for Social Science</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Coping with biggish data\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "Spring 2019\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 525\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"semantics/\">\n",
      "   Intro to semantics</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Meaning: What is it?\n",
      "</td>\n",
      "</tr>\n",
      "</table></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 581\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"compling/\">\n",
      "   Intro to computational linguistics</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Computing with words &amp; linguistic structure\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 626\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"pragmatics/\">\n",
      "   Pragmatics</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "How what you say differs from what you mean\n",
      "</td>\n",
      "</tr>\n",
      "\n",
      "\n",
      "\n",
      "<tr>\n",
      "<td>\n",
      "Fall 2018\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 572\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"python_for_ss/\">\n",
      "   Python for Social Science</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Coping with biggish data\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 522\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"syntax/\">\n",
      "   Intro to syntax</a>\n",
      "</td>\n",
      "<td>   </td><td valign=\"top\">\n",
      "Structure: Why put up with it?\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "Spring 2018\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 525\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"semantics/\">\n",
      "   Intro to semantics</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Meaning: What is it?\n",
      "</td>\n",
      "</tr>\n",
      "</table></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 581\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"compling/\">\n",
      "   Intro to computational linguistics</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Computing with words &amp; linguistic structure\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 626\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"pragmatics/\">\n",
      "   Pragmatics</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "How what you say differs from what you mean\n",
      "</td>\n",
      "</tr>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "</ul></div></p>\n",
      "<div>\n",
      "<p>\n",
      "<h2>Software</h2>\n",
      "<li> <a href=\"word_sim_package.tar.gz\">Word similarity tools.</a>\n",
      "<li> <a href=\"crawler_package-0.1.tar.gz\">Basic crawler module: A host of Python components implementing a basic crawler for data collection.</a>\n",
      "<li> <a href=\"archiver.zip\">Archiver module: Presupposes crawler.  Creates archive of compressed text files,  WithTwitter data, can be used to store various augmented information in an Excel file, including page rank, and disabbreviated web links with title info.</a>\n",
      "<p>\n",
      "<h2>Papers and Talks</h2>\n",
      "<ul>\n",
      "<li> <a href=\"Gawron_2015.pdf\">Improving Sparse Word Similarity Models with Asymmetric Measures.</a>\n",
      "<li> <a href=\"Gawron_Stephens_2014.pdf\">Sparseness  and Normalization in Word Similarity Systems.</a>\n",
      "<li> <a href=\"submitted_spatial_aspect.pdf\">The Lexical Semantics of Extent Verbs</a>\n",
      "This version of the work descending from\n",
      "the \"Generalized Paths\" Salt paper is under submission and will be stable\n",
      "a while.\n",
      "<li> <a href=\"frames_hand.pdf\">Argument Structure as soft constraints</a> (talk\n",
      "given at Xerox Parc March 9, 2008)\n",
      "<li> <a href=\"http://repositories.cdlib.org/ucsdling/sdlp3/1\">\n",
      "Interpreting Words</a> (current draft entitled: <i>Circumstances\n",
      "and Perspective: The Logic of Argument Structure</i>)\n",
      "<li> Paths and the Language of Change (<a href=\"paths_change.pdf\">pdf</a>, <a href=\"new_scales_paper.pdf\">Old version</a>,\n",
      "<a href=\"scales_hand-2pg.pdf\">ps handout</a>,\n",
      "<a href=\"scales_hand-2pg.ps\">ps handout</a>)\n",
      "<li> \n",
      "Generalized Paths\n",
      "(<a href=\"salt_hand.pdf\">Salt XV handout</a>,\n",
      "<a href=\"salt_paper.pdf\">Salt XV paper</a>,\n",
      ")\n",
      "<li> \n",
      "Motion, Scalar Paths, and Lexical Aspect\n",
      "(<a href=\"new_motion_to_state_handout.pdf\">pdf version</a>)\n",
      "<li> \n",
      "Frames and Lexical Semantics\n",
      "(<a href=\"frame_perspective.ps\">ps version</a>)\n",
      "<li> \n",
      "Two Kinds of Determiner in DP\n",
      "(<a href=\"lsa_hand.ps\">\n",
      "    LSA handout [postscript]</a>,\n",
      " <a href=\"lsa_talk.ps\">\n",
      "  LSA  talk [postscript]</a>)\n",
      "<ul>\n",
      "<li> Handout contains a superset of the LSA talk handout\n",
      "<li> Talk contains read version of the LSA talk, some\n",
      "clarification of material in the handout\n",
      "</li></li></ul>\n",
      "<li> \n",
      "Abduction and Mismatch in Machine Translation \n",
      "(<a href=\"mt_paper.pdf\">pdf version</a>,\n",
      "<a href=\"http://www.ai.sri.com/mt_paper.ps\"> ps version</a>)\n",
      "<li> Universal Concessive Conditionals (<a href=\"nuccpap.ps\">\n",
      "ps</a>, <a href=\"nuccpap.pdf\">\n",
      "pdf</a>)\n",
      "</li></li></li></li></li></li></li></li></li></li></li></li></ul>\n",
      "<h2>Projects</h2>\n",
      "<ul>\n",
      "<li> <a href=\"http://mappingideas.sdsu.edu\">Mapping ideas</a>\n",
      "<li> <a href=\"http://bulba.sdsu.edu/framenetmt\">Machine Translation\n",
      "with FrameNet</a>\n",
      "<li> <a href=\"http://www.ai.sri.com/natural-language/projects/arpa-sls/nat-lang.html\">Gemini</a>\n",
      "<li> <a href=\"http://www.ai.sri.com/aic/natural-language/natural-language.html#spoken-language\">Spoken Language Systems</a>\n",
      "<li> <a href=\"http://jazz.alexanderstreet.com.libproxy.sdsu.edu/View/547208/\">\n",
      "Bill Evans</a>\n",
      "</li></li></li></li></li></ul>\n",
      "<h2>Works</h2>\n",
      "<p>\n",
      "<p>\n",
      "<table border=\"\">\n",
      "<tr>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "<td>\n",
      "<img src=\"algorithm.png\" width=\"190\"/>\n",
      "</td>\n",
      "<td>\n",
      "<img src=\"Traume.jpg\" width=\"190\"/>\n",
      "</td>\n",
      "<td>\n",
      "<img src=\"Apology.jpg\" width=\"190\"/>\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "<h2>Oddz Endz</h2>\n",
      "<ul>\n",
      "<li> <a href=\"http://www.utexas.edu/courses/myth/12.Icarus.htm\">\n",
      "Cynthia Shelmerdine's Icarus page </a> (especially Muriel Rukeyser's\n",
      "contribution)\n",
      "<li> <a href=\"http://www.johnsayles.com/movies.html\">John Sayles</a>\n",
      "<li> <a href=\"http://web.mac.com/therealjanesmiley/iWeb/Site/index.html\">\n",
      "Jane Smiley</a> (Not that fond of <i>Private Lives</i>, but\n",
      "<i>Horse Heaven</i>, <i>A Thousand Acres</i>, <i>Moo</i>,\n",
      "<i>Ten Days in the Hills</i>, <i>Duplicate Keys</i> and <i>Good Faith</i>\n",
      "are not only all stunning, but stunning in their variety)\n",
      "</li></li></li></ul>\n",
      "<hr/>\n",
      "<h3>Dictionaries</h3>\n",
      "<ul>\n",
      "<li> <a href=\"http://www.yourdictionary.com\">Bucknell dictionaries,</a>\n",
      "<a href=\"http://www.emich.edu/~linguist/dictionaries.html\">EMich dictionaries</a>\n",
      "</li></ul>\n",
      "<hr/>\n",
      "<address> Jean Mark Gawron \n",
      "</address> SDSU, 5900 Campanile Drive, San Diego, CA 92182-7717\n",
      "<hr noshade=\"\" size=\"3\" width=\"80%\"/>\n",
      "<!-- hhmts start -->\n",
      "Last modified: Tue Aug 22 15:39:48 2000\n",
      "<!-- hhmts end -->\n",
      "\n",
      "</p></p></p></li></li></li></p></div></div>\n",
      "****\n",
      "<p>\n",
      "<table border=\"0\" cellpadding=\"10\" cellspacing=\"0\" width=\"100%\">\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "<td colspan=\"2\">\n",
      " \n",
      "</td>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "\n",
      "<tr>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<table bgcolor=\"lightgray\" width=\"210\">\n",
      "<tr>\n",
      "<td>\n",
      "<a href=\"#classes\">Classes</a> <br/><br/>\n",
      "<a href=\"bulba.sdsu.edu/compling-program/research/gawron/\">Research</a><br/> <br/>\n",
      "<a href=\"http://linguistics.sdsu.edu/certificates/index.html\">Computational Linguistics Certificates</a><br/><br/>\n",
      "<a href=\"key.asc\">PGP Public key</a><br/><br/>\n",
      "<a href=\"key_with_photo.asc\">PGP key + Photo </a> <br/><br/>\n",
      "<a href=\"GPG_explanation.html\">Explaining PGP</a>\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</td>\n",
      "<td valign=\"top\">\n",
      "<img src=\"gawron2.jpg\" width=\"400\"/><br/>\n",
      "</td>\n",
      "<td>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Department of Linguistics and Asian/Middle Eastern Languages<br/>\n",
      "San Diego State University <br/>\n",
      "5500 Campanile Dr. <br/>\n",
      "San Diego, CA 92182-7727<br/>\n",
      "<br/>\n",
      "</td>\n",
      "<td colspan=\"2\">\n",
      "<p>\n",
      "\"I have come to believe that the whole world is an enigma, a harmless enigma\n",
      "that is made terrible by our own mad attempt to interpret it as though it had\n",
      "an underlying truth.\"\n",
      "-- Umberto Eco<br/>\n",
      "</p>\n",
      "<p>\n",
      "\"And now with some pleasure I find that it's seven; and must cook dinner. Haddock and sausage meat. I think it is true that one gains a certain hold on sausage and haddock by writing them down.\" -- The last entry in Virginia Woolf's diary<br/>\n",
      "</p></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"4\">\n",
      "<ul>\n",
      "<h3>Computational Linguistics  and Text Analytics Certificates</h3>\n",
      "<p>\n",
      "<p>\n",
      "<table width=\"80%\">\n",
      "<tr>\n",
      "<td>Text Analytics</td>\n",
      "<td>\n",
      "The Computational Linguistics program at SDSU is\n",
      "offering a new minor and a new certificate in Text Analytics for\n",
      "which undergrads, grads, and community members may qualify.\n",
      "<a href=\"http://gawron.sdsu.edu/text_analytics/\">Details</a>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Comp Ling Cert</td>\n",
      "<td>\n",
      "The Computational Linguistics program at SDSU is\n",
      "offering a new certificate in Computational Linguistics for\n",
      "which undergrads, grads, and community members may qualify.\n",
      "<a href=\"CompLingCert.rtf\">Details</a>\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</p></p></ul></td>\n",
      "</tr>\n",
      "<tr>\n",
      "</tr>\n",
      "\n",
      "</table>\n",
      "\n",
      "<div>\n",
      "<ul>\n",
      "<table bgcolor=\"lightgray\" border=\"\" fgcolor=\"#ffffff\" width=\"85%\">\n",
      "<tr>\n",
      "<td bgcolor=\"white\">\n",
      "  \n",
      "</td>\n",
      "<td bgcolor=\"white\">\n",
      "<h2><a name=\"Classes\"> Courses</a> </h2>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "Spring 2022\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 572\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"python_for_ss/\">\n",
      "   Python for Social Science</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Coping with biggish data\n",
      "</td>\n",
      "</tr>\n",
      "</table></td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<tr>\n",
      "<td>\n",
      "Fall 2020\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 522\n",
      "</td>\n",
      "<td valign=\"top\">  \n",
      "See Blackboard\n",
      "</td>\n",
      "<td>   </td><td valign=\"top\">\n",
      "Structure: Why put up with it?\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 572\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"python_for_ss/\">\n",
      "   Python for Social Science</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Coping with biggish data\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "Spring 2020\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 525\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"semantics/\">\n",
      "   Intro to semantics</a> </td>\n",
      "<td>\n",
      "</td>\n",
      "<td valign=\"top\">\n",
      "Meaning: What is it?\n",
      "</td>\n",
      "</tr>\n",
      "</table></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 581\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"compling/\">\n",
      "   Comp Ling Intro</a>\n",
      "</td><td>\n",
      "</td>\n",
      "<td valign=\"top\">\n",
      "Computing with words &amp; linguistic structure\n",
      "</td>\n",
      "</tr>\n",
      "\n",
      "\n",
      "\n",
      "<tr>\n",
      "<td>\n",
      "Fall 2019\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 572\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"python_for_ss/\">\n",
      "   Python for Social Science</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Coping with biggish data\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "Spring 2019\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 525\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"semantics/\">\n",
      "   Intro to semantics</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Meaning: What is it?\n",
      "</td>\n",
      "</tr>\n",
      "</table></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 581\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"compling/\">\n",
      "   Intro to computational linguistics</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Computing with words &amp; linguistic structure\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 626\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"pragmatics/\">\n",
      "   Pragmatics</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "How what you say differs from what you mean\n",
      "</td>\n",
      "</tr>\n",
      "\n",
      "\n",
      "\n",
      "<tr>\n",
      "<td>\n",
      "Fall 2018\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 572\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"python_for_ss/\">\n",
      "   Python for Social Science</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Coping with biggish data\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 522\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"syntax/\">\n",
      "   Intro to syntax</a>\n",
      "</td>\n",
      "<td>   </td><td valign=\"top\">\n",
      "Structure: Why put up with it?\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "Spring 2018\n",
      "</td>\n",
      "<td>\n",
      "<table>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 525\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"semantics/\">\n",
      "   Intro to semantics</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Meaning: What is it?\n",
      "</td>\n",
      "</tr>\n",
      "</table></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 581\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"compling/\">\n",
      "   Intro to computational linguistics</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "Computing with words &amp; linguistic structure\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Ling 626\n",
      "</td>\n",
      "<td valign=\"top\"> <a href=\"pragmatics/\">\n",
      "   Pragmatics</a>\n",
      "</td>\n",
      "<td>   </td>\n",
      "<td valign=\"top\">\n",
      "How what you say differs from what you mean\n",
      "</td>\n",
      "</tr>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "</ul></div></p>\n",
      "****\n",
      "<table border=\"0\" cellpadding=\"10\" cellspacing=\"0\" width=\"100%\">\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "<td colspan=\"2\">\n",
      " \n",
      "</td>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "\n",
      "<tr>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<table bgcolor=\"lightgray\" width=\"210\">\n",
      "<tr>\n",
      "<td>\n",
      "<a href=\"#classes\">Classes</a> <br/><br/>\n",
      "<a href=\"bulba.sdsu.edu/compling-program/research/gawron/\">Research</a><br/> <br/>\n",
      "<a href=\"http://linguistics.sdsu.edu/certificates/index.html\">Computational Linguistics Certificates</a><br/><br/>\n",
      "<a href=\"key.asc\">PGP Public key</a><br/><br/>\n",
      "<a href=\"key_with_photo.asc\">PGP key + Photo </a> <br/><br/>\n",
      "<a href=\"GPG_explanation.html\">Explaining PGP</a>\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</td>\n",
      "<td valign=\"top\">\n",
      "<img src=\"gawron2.jpg\" width=\"400\"/><br/>\n",
      "</td>\n",
      "<td>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td valign=\"top\">\n",
      "Department of Linguistics and Asian/Middle Eastern Languages<br/>\n",
      "San Diego State University <br/>\n",
      "5500 Campanile Dr. <br/>\n",
      "San Diego, CA 92182-7727<br/>\n",
      "<br/>\n",
      "</td>\n",
      "<td colspan=\"2\">\n",
      "<p>\n",
      "\"I have come to believe that the whole world is an enigma, a harmless enigma\n",
      "that is made terrible by our own mad attempt to interpret it as though it had\n",
      "an underlying truth.\"\n",
      "-- Umberto Eco<br/>\n",
      "</p>\n",
      "<p>\n",
      "\"And now with some pleasure I find that it's seven; and must cook dinner. Haddock and sausage meat. I think it is true that one gains a certain hold on sausage and haddock by writing them down.\" -- The last entry in Virginia Woolf's diary<br/>\n",
      "</p></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"4\">\n",
      "<ul>\n",
      "<h3>Computational Linguistics  and Text Analytics Certificates</h3>\n",
      "<p>\n",
      "<p>\n",
      "<table width=\"80%\">\n",
      "<tr>\n",
      "<td>Text Analytics</td>\n",
      "<td>\n",
      "The Computational Linguistics program at SDSU is\n",
      "offering a new minor and a new certificate in Text Analytics for\n",
      "which undergrads, grads, and community members may qualify.\n",
      "<a href=\"http://gawron.sdsu.edu/text_analytics/\">Details</a>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Comp Ling Cert</td>\n",
      "<td>\n",
      "The Computational Linguistics program at SDSU is\n",
      "offering a new certificate in Computational Linguistics for\n",
      "which undergrads, grads, and community members may qualify.\n",
      "<a href=\"CompLingCert.rtf\">Details</a>\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</p></p></ul></td>\n",
      "</tr>\n",
      "<tr>\n",
      "</tr>\n",
      "\n",
      "</table>\n",
      "****\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "****\n",
      "<td colspan=\"2\">\n",
      " \n",
      "</td>\n",
      "****\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "****\n",
      "<tr>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "</tr>\n",
      "****\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "****\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "****\n",
      "<td>\n",
      " \n",
      "</td>\n",
      "****\n"
     ]
    }
   ],
   "source": [
    "# soup is a document tree (sort of)\n",
    "L = soup.findChildren()[0].findChildren()\n",
    "for child in L[:15]:\n",
    "    if child.name == \"head\":\n",
    "        continue\n",
    "    print(child)\n",
    "    print(\"****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d96f1",
   "metadata": {},
   "source": [
    "A properly closed link looks like this:\n",
    "\n",
    "```\n",
    "<a href=\"/ebooks/4300\">Ulysses by James Joyce (434)</a>\n",
    "````\n",
    "\n",
    "Find all of the links.  Extract the book idx (4300, in this case) and the title from the link.\n",
    "\n",
    "Use the structure in the parsed html (the `soup` instance).,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b943b3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 book indices found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2680',\n",
       " '64317',\n",
       " '10940',\n",
       " '1727',\n",
       " '33283',\n",
       " '1661',\n",
       " '16',\n",
       " '41580',\n",
       " '6761',\n",
       " '1232']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#  All links are inside the tag <a ... >\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# We're restricting our harvest to links whose hrefs start with this string (links to book pages)\n",
    "path_re = \"/ebooks/(\\d+)\"\n",
    "reg_exp = re.compile(path_re)\n",
    "\n",
    "## Containers for collected data\n",
    "idxs = []\n",
    "# Let's grab the book titles too (cause we're humans. and like names instead of numbers)\n",
    "titles = dict()\n",
    "\n",
    "#  Do the collecting\n",
    "for link in links:\n",
    "    # Get the ref string from inside the link instance\n",
    "    ref = link.get(\"href\")\n",
    "    match = reg_exp.findall(ref)\n",
    "    if match:\n",
    "        # findall returns a list.  If there's a match, there will be only one idx\n",
    "        idx = match[0]\n",
    "        idxs.append(idx)\n",
    "        titles[idx]  = link.get_text()\n",
    "\n",
    "# There is more than one top 100 list on the page. They have duplicates.  Remove them\n",
    "idxs = list(set(idxs))\n",
    "print(f\"{len(idxs)} book indices found.\")\n",
    "idxs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d5e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "str0 = r\"hey\\dthere\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5515d859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'84': 'Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley (85269)',\n",
       " '1342': 'Pride and Prejudice by Jane Austen (68228)',\n",
       " '2701': 'Moby Dick; Or, The Whale by Herman Melville (63500)',\n",
       " '12233': 'Stonewall Jackson and the American Civil War by G. F. R.  Henderson (2177)',\n",
       " '1513': 'Romeo and Juliet by William Shakespeare (58361)',\n",
       " '145': 'Middlemarch by George Eliot (46099)',\n",
       " '37106': 'Little Women; Or, Meg, Jo, Beth, and Amy by Louisa May Alcott (42596)',\n",
       " '100': 'The Complete Works of William Shakespeare by William Shakespeare (43956)',\n",
       " '55231': 'A history of the Peninsular War, Vol. 3, Sep. 1809-Dec. 1810 : by Charles Oman (1470)',\n",
       " '2641': 'A Room with a View by E. M.  Forster (44091)',\n",
       " '16389': 'The Enchanted April by Elizabeth Von Arnim (39653)',\n",
       " '2542': \"A Doll's House : a play by Henrik Ibsen (29019)\",\n",
       " '67979': 'The Blue Castle: a novel by L. M.  Montgomery (39425)',\n",
       " '64317': 'The Great Gatsby by F. Scott  Fitzgerald (29613)',\n",
       " '844': 'The Importance of Being Earnest: A Trivial Comedy for Serious People by Oscar Wilde (28134)',\n",
       " '11': \"Alice's Adventures in Wonderland by Lewis Carroll (32619)\",\n",
       " '6761': 'The Adventures of Ferdinand Count Fathom — Complete by T.  Smollett (36538)',\n",
       " '2160': 'The Expedition of Humphry Clinker by T.  Smollett (35695)',\n",
       " '394': 'Cranford by Elizabeth Cleghorn Gaskell (36169)',\n",
       " '4085': 'The Adventures of Roderick Random by T.  Smollett (35341)',\n",
       " '6593': 'History of Tom Jones, a Foundling by Henry Fielding (34837)',\n",
       " '5197': 'My Life — Volume 1 by Richard Wagner (34159)',\n",
       " '1259': 'Twenty years after by Alexandre Dumas and Auguste Maquet (34490)',\n",
       " '174': 'The Picture of Dorian Gray by Oscar Wilde (27832)',\n",
       " '2130': 'Utopia by Saint Thomas More (1007)',\n",
       " '5200': 'Metamorphosis by Franz Kafka (25562)',\n",
       " '2554': 'Crime and Punishment by Fyodor Dostoyevsky (24764)',\n",
       " '98': 'A Tale of Two Cities by Charles Dickens (22478)',\n",
       " '12915': 'The White Devil by John Webster (855)',\n",
       " '1952': 'The Yellow Wallpaper by Charlotte Perkins Gilman (23961)',\n",
       " '1080': 'A Modest Proposal by Jonathan Swift (19605)',\n",
       " '345': 'Dracula by Bram Stoker (20251)',\n",
       " '43750': 'Ancient Man in Britain by Donald A.  Mackenzie (765)',\n",
       " '47629': 'Ang \"Filibusterismo\" (Karugtóng ng Noli Me Tangere) by José Rizal (15030)',\n",
       " '3207': 'Leviathan by Thomas Hobbes (10135)',\n",
       " '86': \"A Connecticut Yankee in King Arthur's Court by Mark Twain (723)\",\n",
       " '73358': 'Classified object by John Victor Peterson (693)',\n",
       " '43045': \"George Eliot's Life, as Related in Her Letters and Journals. Vol. 3 (of 3) by George Eliot (686)\",\n",
       " '1400': 'Great Expectations by Charles Dickens (17455)',\n",
       " '25344': 'The Scarlet Letter by Nathaniel Hawthorne (16315)',\n",
       " '34279': 'Handbook of Medical Entomology by O. A.  Johannsen and William A.  Riley (626)',\n",
       " '20228': 'Noli Me Tangere by José Rizal (13622)',\n",
       " '43': 'The Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson (16467)',\n",
       " '219': 'Heart of Darkness by Joseph Conrad (15126)',\n",
       " '43044': \"George Eliot's Life, as Related in Her Letters and Journals. Vol. 2 (of 3) by George Eliot (596)\",\n",
       " '1260': 'Jane Eyre: An Autobiography by Charlotte Brontë (16234)',\n",
       " '28054': 'The Brothers Karamazov by Fyodor Dostoyevsky (14696)',\n",
       " '76': 'Adventures of Huckleberry Finn by Mark Twain (15569)',\n",
       " '43043': \"George Eliot's Life, as Related in Her Letters and Journals. Vol. 1 (of 3) by George Eliot (564)\",\n",
       " '10940': 'Manners, Customs, and Dress During the Middle Ages and During the Renaissance Period by P. L. Jacob (526)',\n",
       " '1661': 'The Adventures of Sherlock Holmes by Arthur Conan Doyle (15196)',\n",
       " '408': 'The Souls of Black Folk by W. E. B.  Du Bois (12798)',\n",
       " '73354': 'A world to die for by Sam Carson (521)',\n",
       " '13923': 'The Whitehouse Cookbook (1887) by F. L.  Gillette and Hugo Ziemann (505)',\n",
       " '73352': 'The heel of Achilles by E. M. Delafield (501)',\n",
       " '73351': 'Mr. Loneliness by Henry Slesar (489)',\n",
       " '6130': 'The Iliad by Homer (13413)',\n",
       " '768': 'Wuthering Heights by Emily Brontë (12086)',\n",
       " '25709': 'A Practical View of the Prevailing Religious System of Professed Christians, in the Middle and Highe (469)',\n",
       " '73356': 'Too close to the forest by Al Reynolds and Bryce Walton (468)',\n",
       " '2814': 'Dubliners by James Joyce (10546)',\n",
       " '4300': 'Ulysses by James Joyce (12844)',\n",
       " '73357': \"No star's land by William Douglas Morrison (461)\",\n",
       " '73355': 'The deadly ones by F. L.  Wallace (456)',\n",
       " '42931': 'Plotinos: Complete Works, v. 2 by Plotinus (450)',\n",
       " '2591': \"Grimms' Fairy Tales by Jacob Grimm and Wilhelm Grimm (13038)\",\n",
       " '996': 'Don Quixote by Miguel de Cervantes Saavedra (9397)',\n",
       " '1727': 'The Odyssey by Homer (11745)',\n",
       " '42933': 'Plotinos: Complete Works, v. 4 by Plotinus (429)',\n",
       " '2000': 'Don Quijote by Miguel de Cervantes Saavedra (12827)',\n",
       " '10676': 'The Reign of Greed by José Rizal (11193)',\n",
       " '1232': 'The Prince by Niccolò Machiavelli (12186)',\n",
       " '19598': 'How to Live: Rules for Healthful Living Based on Modern Science by Fisher and Fisk (406)',\n",
       " '67098': 'Winnie-the-Pooh by A. A.  Milne (9057)',\n",
       " '41445': 'Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley (9099)',\n",
       " '40077': 'The Principles of Economics, with Applications to Practical Problems by Frank A.  Fetter (392)',\n",
       " '205': 'Walden, and On The Duty Of Civil Disobedience by Henry David Thoreau (10335)',\n",
       " '23': 'Narrative of the Life of Frederick Douglass, an American Slave by Frederick Douglass (7456)',\n",
       " '1998': 'Thus Spake Zarathustra: A Book for All and None by Friedrich Wilhelm Nietzsche (11465)',\n",
       " '73362': 'Stars and atoms by Sir Arthur Stanley Eddington (374)',\n",
       " '27827': 'The Kama Sutra of Vatsyayana by Vatsyayana (10078)',\n",
       " '30254': 'The Romance of Lust: A classic Victorian erotic novel by Anonymous (9855)',\n",
       " '1184': 'The Count of Monte Cristo by Alexandre Dumas and Auguste Maquet (10283)',\n",
       " '74': 'The Adventures of Tom Sawyer, Complete by Mark Twain (10218)',\n",
       " '120': 'Treasure Island by Robert Louis Stevenson (8749)',\n",
       " '5740': 'Tractatus Logico-Philosophicus by Ludwig Wittgenstein (11438)',\n",
       " '45': 'Anne of Green Gables by L. M.  Montgomery (9537)',\n",
       " '4363': 'Beyond Good and Evil by Friedrich Wilhelm Nietzsche (9710)',\n",
       " '2600': 'War and Peace by graf Leo Tolstoy (12437)',\n",
       " '27752': 'Aphrodisiacs and Anti-aphrodisiacs: Three Essays on the Powers of Reproduction by John Davenport (344)',\n",
       " '16718': 'Mineralogia Polyglotta by Christian Keferstein (341)',\n",
       " '46': 'A Christmas Carol in Prose; Being a Ghost Story of Christmas by Charles Dickens (10953)',\n",
       " '37499': \"Napoleon's Letters to Josephine, 1796-1812 by Emperor of the French Napoleon I (336)\",\n",
       " '7370': 'Second Treatise of Government by John Locke (9044)',\n",
       " '244': 'A Study in Scarlet by Arthur Conan Doyle (8668)',\n",
       " '36': 'The War of the Worlds by H. G.  Wells (8965)',\n",
       " '11030': 'Incidents in the Life of a Slave Girl, Written by Herself by Harriet A.  Jacobs (6697)',\n",
       " '600': 'Notes from the Underground by Fyodor Dostoyevsky (8974)',\n",
       " '2680': 'Meditations by Emperor of Rome Marcus Aurelius (8340)',\n",
       " '1497': 'The Republic by Plato (10076)',\n",
       " '42996': 'Castes and Tribes of Southern India. Vol. 6 of 7 by Edgar Thurston (3395)',\n",
       " '732': 'History of the Decline and Fall of the Roman Empire — Volume 2 by Edward Gibbon (3217)',\n",
       " '15293': 'Influences of Geographic Environment by Ellen Churchill Semple (3206)',\n",
       " '57374': 'Annals and Antiquities of Rajasthan, v. 1 of 3 by James Tod (3056)',\n",
       " '33283': 'Calculus Made Easy by Silvanus P.  Thompson (8578)',\n",
       " '48334': 'Ireland under the Tudors, with a Succinct Account of the Earlier History. Vol. 2 (of 3) by Bagwell (2370)',\n",
       " '15845': 'Florante at Laura by Francisco Balagtas (2310)',\n",
       " '514': 'Little Women by Louisa May Alcott (9181)',\n",
       " '135': 'Les Misérables by Victor Hugo (6689)',\n",
       " '41580': 'Haunted London by Walter Thornbury (2090)',\n",
       " '8800': 'The divine comedy by Dante Alighieri (10039)',\n",
       " '58585': 'The Prophet by Kahlil Gibran (8605)',\n",
       " '67223': 'The Strangest Things in the World: A Book About Extraordinary Manifestations of Nature by Henry (1981)',\n",
       " '10': 'The King James Version of the Bible (8471)',\n",
       " '10007': 'Carmilla by Joseph Sheridan Le Fanu (7823)',\n",
       " '8492': 'The King in Yellow by Robert W.  Chambers (8507)',\n",
       " '2852': 'The Hound of the Baskervilles by Arthur Conan Doyle (8191)',\n",
       " '35899': 'The Philippines a Century Hence by José Rizal (6227)',\n",
       " '16': 'Peter Pan by J. M.  Barrie (8471)',\n",
       " '55': 'The Wonderful Wizard of Oz by L. Frank  Baum (8518)',\n",
       " '22599': 'The Hindu-Arabic Numerals by Louis Charles Karpinski and David Eugene Smith (1846)',\n",
       " '158': 'Emma by Jane Austen (7910)',\n",
       " '16119': 'Doctrina Christiana (7026)',\n",
       " '45304': 'The City of God, Volume I by Bishop of Hippo Saint Augustine (1831)',\n",
       " '730': 'Oliver Twist by Charles Dickens (7401)',\n",
       " '36034': 'White Nights and Other Stories by Fyodor Dostoyevsky (7768)',\n",
       " '35': 'The Time Machine by H. G.  Wells (7085)',\n",
       " '55084': 'The Homing Pigeon by United States. Army. Signal Corps and United States. War Department (21819)',\n",
       " '829': \"Gulliver's Travels into Several Remote Nations of the World by Jonathan Swift (7458)\",\n",
       " '161': 'Sense and Sensibility by Jane Austen (7262)',\n",
       " '3825': 'Pygmalion by Bernard Shaw (7001)',\n",
       " '3296': 'The Confessions of St. Augustine by Bishop of Hippo Saint Augustine (6906)',\n",
       " '5827': 'The Problems of Philosophy by Bertrand Russell (6724)',\n",
       " '24869': 'The Rámáyan of Válmíki, translated into English verse by Valmiki (6619)',\n",
       " '105': 'Persuasion by Jane Austen (6616)',\n",
       " '42324': 'Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley (6063)',\n",
       " '4217': 'A Portrait of the Artist as a Young Man by James Joyce (6031)',\n",
       " '16328': 'Beowulf: An Anglo-Saxon Epic Poem (6007)'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785d129",
   "metadata": {},
   "source": [
    "For you possibly puzzled Tolstoy fans, *graf* is Russian for Count.  Although the link\n",
    "takes you to the Constance Garnett translation, the metadata lists the author as \"graf Leo Tolstoy\".\n",
    "I don't know why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce877b",
   "metadata": {},
   "source": [
    "##  Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74be8d",
   "metadata": {},
   "source": [
    "We have the **indexes** for the books we want.  Now download the data using `get_book` (defined\n",
    "in the first section).  Reset `num_samples` to fit your time and space requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64367565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bd150f18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting 20 books\n",
      "0 read!\n",
      "1 read!\n",
      "2 read!\n",
      "3 read!\n",
      "Err 4\n",
      "5 read!\n",
      "6 read!\n",
      "7 read!\n",
      "8 read!\n",
      "9 read!\n",
      "10 read!\n",
      "11 read!\n",
      "12 read!\n",
      "13 read!\n",
      "14 read!\n",
      "15 read!\n",
      "16 read!\n",
      "17 read!\n",
      "18 read!\n",
      "19 read!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "books = []\n",
    "errs = []\n",
    "\n",
    "# Implement delay between downloads to be NICE to host website\n",
    "delay = 5\n",
    "num_samples = 20\n",
    "\n",
    "print(f\"Getting {num_samples} books\")\n",
    "for (i,idx) in enumerate(idxs[:num_samples]):\n",
    "    try:  #  There do seem to be missing books\n",
    "       books.append(get_book(idx))\n",
    "       print(f\"{i} read!\")\n",
    "       time.sleep(delay)\n",
    "    except urllib.request.HTTPError:\n",
    "        print(f\"Err {i}\")\n",
    "        errs.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd420919",
   "metadata": {},
   "source": [
    "The books that have been moved or removed (when downloading all 115 books):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d7c62073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculus Made Easy by Silvanus P.  Thompson (8578)\n"
     ]
    }
   ],
   "source": [
    "for idx in errs:\n",
    "    print(titles[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e3881c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculus Made Easy by Silvanus P.  Thompson (10359)\n",
      "Moby Word Lists by Grady Ward (355)\n",
      "Tractatus Logico-Philosophicus by Ludwig Wittgenstein (12068)\n"
     ]
    }
   ],
   "source": [
    "for idx in errs:\n",
    "    print(titles[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0c039",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Remove Gutenberg.org identifying front matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e0d6bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_str = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
    "def get_tag_str_line_no (line_list, strict=False):\n",
    "    for (i,l) in enumerate(line_list):\n",
    "        if l.startswith(tag_str):\n",
    "            return i\n",
    "    if strict:\n",
    "        raise Exception(\"No luck!\")\n",
    "    else:\n",
    "        return -1\n",
    "        \n",
    "def clean_book (book_str,strict=False):\n",
    "    lines = book_str.splitlines()\n",
    "    return '\\n'.join(lines[get_tag_str_line_no(lines,strict=strict)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9e47c80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning 19 books\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "cleaned_books = []\n",
    "\n",
    "#  Return with Exception if cleaning fails\n",
    "strict = True\n",
    "print(f\"Cleaning {len(books)} books\")\n",
    "for (i,book_str) in enumerate(books):\n",
    "    try:\n",
    "       cleaned_books.append(clean_book (book_str,strict=strict))\n",
    "    except Exception:\n",
    "        print(f\"Err {i}\")\n",
    "        continue\n",
    "\n",
    "print(len(cleaned_books))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c7afb",
   "metadata": {},
   "source": [
    "Save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "72bf45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = cleaned_books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff075c",
   "metadata": {},
   "source": [
    "## English letter frequencies\n",
    "\n",
    "We illustrate some simple statistics tracking with text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1948d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ltr_ctr = Counter()\n",
    "\n",
    "for book in books:\n",
    "    ltr_ctr.update(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cef7679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 2985656),\n",
       " ('e', 1701383),\n",
       " ('t', 1207977),\n",
       " ('o', 1065592),\n",
       " ('a', 1039458),\n",
       " ('n', 918774),\n",
       " ('i', 868089),\n",
       " ('h', 867218),\n",
       " ('s', 850086),\n",
       " ('r', 821677)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltr_ctr.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda7d42",
   "metadata": {},
   "source": [
    "Removing white space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac0b61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_ctr2 = Counter()\n",
    "\n",
    "for book in books:\n",
    "    ltr_ctr2.update(''.join(book.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "790eecc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e', 1602260),\n",
       " ('t', 1149828),\n",
       " ('a', 1007349),\n",
       " ('o', 961439),\n",
       " ('h', 914048),\n",
       " ('n', 875366),\n",
       " ('s', 791139),\n",
       " ('i', 785031),\n",
       " ('r', 726181),\n",
       " ('d', 578171)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltr_ctr2.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbe7be",
   "metadata": {},
   "source": [
    "## English letter digraph frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "536bf765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'),\n",
       " ('b', 'r'),\n",
       " ('r', 'a'),\n",
       " ('a', 'c'),\n",
       " ('c', 'a'),\n",
       " ('a', 'd'),\n",
       " ('d', 'a'),\n",
       " ('a', 'b'),\n",
       " ('b', 'r'),\n",
       " ('r', 'a')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(\"abracadabra\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "534300a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "ltr_digraph_ctr = Counter()\n",
    "\n",
    "for book in books:\n",
    "    ltr_digraph_ctr.update(bigrams(''.join(book.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5ca0bb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('t', 'h'), 422250),\n",
       " (('h', 'e'), 367676),\n",
       " (('e', 'r'), 238425),\n",
       " (('i', 'n'), 222905),\n",
       " (('a', 'n'), 207068),\n",
       " (('r', 'e'), 187583),\n",
       " (('n', 'd'), 166000),\n",
       " (('e', 's'), 155890),\n",
       " (('e', 'n'), 155167),\n",
       " (('h', 'a'), 153920)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltr_digraph_ctr.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40851d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "752cc34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "reg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbcf03a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return the coefficient of determination of the prediction.\n",
      "\n",
      "        The coefficient of determination :math:`R^2` is defined as\n",
      "        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "        The best possible score is 1.0 and it can be negative (because the\n",
      "        model can be arbitrarily worse). A constant model that always predicts\n",
      "        the expected value of `y`, disregarding the input features, would get\n",
      "        a :math:`R^2` score of 0.0.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Test samples. For some estimators this may be a precomputed\n",
      "            kernel matrix or a list of generic objects instead with shape\n",
      "            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "            is the number of samples used in the fitting for the estimator.\n",
      "\n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            True values for `X`.\n",
      "\n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "        with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "        This influences the ``score`` method of all the multioutput\n",
      "        regressors (except for\n",
      "        :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(reg.score.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d77752",
   "metadata": {},
   "source": [
    "Criticism:  This technique creates spurious letter bigrams:\n",
    "\n",
    "```\n",
    "of the\n",
    "```\n",
    "\n",
    "becomes\n",
    "\n",
    "```\n",
    "ofthe\n",
    "```\n",
    "\n",
    "creating the unlikely digraph \"ft\".  And sure enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ade170b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46843"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltr_digraph_ctr[\"f\",\"t\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e21ba9",
   "metadata": {},
   "source": [
    "To avoid this update digraph counts word by word (which is slower):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9108f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "a\n",
    "\n",
    "ltr_digraph_ctr2 = Counter()\n",
    "\n",
    "for book in books:\n",
    "    for word in word_tokenize(book):\n",
    "        ltr_digraph_ctr2.update(bigrams(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9e706e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('t', 'h'), 434605),\n",
       " (('h', 'e'), 398254),\n",
       " (('a', 'n'), 220483),\n",
       " (('i', 'n'), 209097),\n",
       " (('e', 'r'), 194978),\n",
       " (('n', 'd'), 194009),\n",
       " (('r', 'e'), 169484),\n",
       " (('h', 'a'), 142927),\n",
       " (('o', 'u'), 133577),\n",
       " (('a', 't'), 126597)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltr_digraph_ctr2.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bc48d329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('e', 'n'), 135528)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltr_digraph_ctr.most_common(10)[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2a43fa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46843, 10031)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltr_digraph_ctr[\"f\",\"t\"],ltr_digraph_ctr2[\"f\",\"t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fb2d71c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133737, 116265)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltr_digraph_ctr[\"e\",\"d\"],ltr_digraph_ctr2[\"e\",\"d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f3348801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153818, 142927)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltr_digraph_ctr[\"h\",\"a\"],ltr_digraph_ctr2[\"h\",\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f0ecd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'books' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w9/bx4mylnd27g_kqqgn5hrn2x40000gr/T/ipykernel_37936/298546270.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'books' is not defined"
     ]
    }
   ],
   "source": [
    "books.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151997c",
   "metadata": {},
   "source": [
    "##  English word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7106156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [(',', 6539), ('and', 3159), ('.', 2972), ('the', 2637), ('of', 2485), ('to', 2001), ('that', 1902), ('is', 1441), ('in', 1137), ('it', 1133)]\n",
      "====================\n",
      "1 [(',', 9652), ('.', 5445), ('the', 5013), ('and', 4699), ('of', 3693), ('to', 3192), ('a', 2499), ('that', 2483), ('in', 1962), ('I', 1854)]\n",
      "====================\n",
      "2 [(',', 24648), ('the', 18273), ('of', 13483), ('.', 11159), ('and', 10071), ('to', 7412), ('a', 5496), ('in', 5413), ('that', 3830), ('was', 3107)]\n",
      "====================\n",
      "3 [(',', 34023), ('the', 24992), ('of', 17090), ('and', 15346), ('.', 14779), ('to', 10951), ('a', 7492), ('in', 7275), ('that', 5126), ('was', 4168)]\n",
      "====================\n",
      "4 [(',', 41819), ('the', 30414), ('of', 19823), ('.', 19307), ('and', 18212), ('to', 13678), ('a', 10070), ('in', 9015), ('I', 6939), ('that', 6775)]\n",
      "====================\n",
      "5 6 7 8 9 10 11 12 13 14 15 16 17 18 "
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "wd_ctr =  Counter()\n",
    "\n",
    "for (i,book)  in enumerate(books):\n",
    "    print(f\"{i}\", end=\" \")\n",
    "    wd_ctr.update(word_tokenize(book))\n",
    "    if i < 5:\n",
    "        print(wd_ctr.most_common(10))\n",
    "        print(\"=\"*20)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "784ed990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 292750), ('.', 173957), ('the', 161817), ('of', 100781), ('and', 88099), ('to', 77120), ('a', 57897), ('in', 51476), ('I', 45022), ('’', 40088)]\n"
     ]
    }
   ],
   "source": [
    "print(wd_ctr.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c59dd163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of Frankenstein; Or, The Modern Prometheus\\n    \\nThis ebook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with alm'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58b0ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tkns = word_tokenize(text[:10_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4691c354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1963"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tkns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd7ea998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wonders',\n",
       " 'and',\n",
       " 'in',\n",
       " 'beauty',\n",
       " 'every',\n",
       " 'region',\n",
       " 'hitherto',\n",
       " 'discovered',\n",
       " 'on',\n",
       " 'the',\n",
       " 'habitable',\n",
       " 'globe',\n",
       " '.',\n",
       " 'Its',\n",
       " 'productions',\n",
       " 'and',\n",
       " 'features',\n",
       " 'may',\n",
       " 'be',\n",
       " 'without',\n",
       " 'example',\n",
       " ',',\n",
       " 'as',\n",
       " 'the',\n",
       " 'phenomena',\n",
       " 'of',\n",
       " 'the',\n",
       " 'heavenly',\n",
       " 'bodies',\n",
       " 'undoubtedly',\n",
       " 'are',\n",
       " 'in',\n",
       " 'those',\n",
       " 'undiscovered',\n",
       " 'solitudes',\n",
       " '.',\n",
       " 'What',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'expected',\n",
       " 'in',\n",
       " 'a',\n",
       " 'country',\n",
       " 'of',\n",
       " 'eternal',\n",
       " 'light',\n",
       " '?',\n",
       " 'I',\n",
       " 'may',\n",
       " 'there',\n",
       " 'discover',\n",
       " 'the',\n",
       " 'wondrous',\n",
       " 'power',\n",
       " 'which',\n",
       " 'attracts',\n",
       " 'the',\n",
       " 'needle',\n",
       " 'and',\n",
       " 'may',\n",
       " 'regulate',\n",
       " 'a',\n",
       " 'thousand',\n",
       " 'celestial',\n",
       " 'observations',\n",
       " 'that',\n",
       " 'require',\n",
       " 'only',\n",
       " 'this',\n",
       " 'voyage',\n",
       " 'to',\n",
       " 'render',\n",
       " 'their',\n",
       " 'seeming',\n",
       " 'eccentricities',\n",
       " 'consistent',\n",
       " 'for',\n",
       " 'ever',\n",
       " '.',\n",
       " 'I',\n",
       " 'shall',\n",
       " 'satiate',\n",
       " 'my',\n",
       " 'ardent',\n",
       " 'curiosity',\n",
       " 'with',\n",
       " 'the',\n",
       " 'sight',\n",
       " 'of',\n",
       " 'a',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'never',\n",
       " 'before',\n",
       " 'visited',\n",
       " ',',\n",
       " 'and']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkns[500:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3bf27",
   "metadata": {},
   "source": [
    "Compare the original figures.  Explain the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f4d76",
   "metadata": {},
   "source": [
    "## Creating a corpus of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c81361",
   "metadata": {},
   "source": [
    "If you're running in google colab do this first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Create the following folder in the root directory\n",
    "!mkdir -p \"/content/drive/My Drive/nltk\"\n",
    "\n",
    "nltk_corpus_dir = \"/content/drive/My Drive/nltk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c33ef",
   "metadata": {},
   "source": [
    "Otherwise pick a corpus directory on your own file system and create a subdirectory for your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ad50088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import nltk.data\n",
    "\n",
    "nltk_corpus_dir = '~/nltk_data/corpora/gutenberg2'\n",
    "# Note if you changed the path above you have to retype it here (because variables dont work right in !-commands)\n",
    "!mkdir -p ~/nltk_data/corpora/gutenberg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "791780c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_corpus_dir = os.path.expanduser(nltk_corpus_dir)\n",
    "\n",
    "if nltk_corpus_dir not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_corpus_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704f0f8",
   "metadata": {},
   "source": [
    "Define the code to put your data in `nltk_corpus_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbfa099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Many gutenberg title tags include a digit sequence in parens.  Not needed.\n",
    "# Making the pattern as specific as possible so as to not affect titles with genuine parentheses\n",
    "reg_exp23 = \"(\\(\\d+\\))\"\n",
    "reg_exp23_c = re.compile(reg_exp23)\n",
    "\n",
    "\n",
    "def make_file_name (title):\n",
    "    try:\n",
    "        (start,end) = reg_exp23_c.search(title).span()\n",
    "        ttn = title[:start] + title[end:]\n",
    "    except:\n",
    "        ttn = title\n",
    "    return '_'.join(ttn.split()) + \".txt\"\n",
    "\n",
    "def make_corpus(corpus_dir,idxs,books,titles,verbose=False):\n",
    "    for (i,book) in enumerate(books):\n",
    "        idx = idxs[i]\n",
    "        fn = make_file_name(titles[idx])\n",
    "        with open(os.path.join(corpus_dir,fn),'w') as ofh:\n",
    "            ofh.write(book)\n",
    "        if verbose:\n",
    "            print(f\"{fn} written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1022f9f",
   "metadata": {},
   "source": [
    "Put your data in `nltk_corpus_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1b76c502",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moby_Word_Lists_by_Grady_Ward.txt written!\n",
      "The_Confessions_of_St._Augustine_by_Bishop_of_Hippo_Saint_Augustine.txt written!\n",
      "Treasure_Island_by_Robert_Louis_Stevenson.txt written!\n",
      "The_King_James_Version_of_the_Bible.txt written!\n",
      "The_Yellow_Wallpaper_by_Charlotte_Perkins_Gilman.txt written!\n",
      "The_giant_horse_of_Oz_by_Ruth_Plumly_Thompson.txt written!\n",
      "The_Importance_of_Being_Earnest:_A_Trivial_Comedy_for_Serious_People_by_Oscar_Wilde.txt written!\n",
      "Jane_Eyre:_An_Autobiography_by_Charlotte_Brontë.txt written!\n",
      "Lady_Chatterley's_lover_by_D._H._Lawrence.txt written!\n",
      "Middlemarch_by_George_Eliot.txt written!\n",
      "The_divine_comedy_by_Dante_Alighieri.txt written!\n",
      "The_War_of_the_Worlds_by_H._G._Wells.txt written!\n",
      "The_Rámáyan_of_Válmíki,_translated_into_English_verse_by_Valmiki.txt written!\n",
      "Thus_Spake_Zarathustra:_A_Book_for_All_and_None_by_Friedrich_Wilhelm_Nietzsche.txt written!\n",
      "Peter_Pan_by_J._M._Barrie.txt written!\n",
      "Pogo_Planet_by_Donald_A._Wollheim.txt written!\n",
      "Walden,_and_On_The_Duty_Of_Civil_Disobedience_by_Henry_David_Thoreau.txt written!\n",
      "Dubliners_by_James_Joyce.txt written!\n",
      "Don_Quixote_by_Miguel_de_Cervantes_Saavedra.txt written!\n"
     ]
    }
   ],
   "source": [
    "make_corpus(nltk_corpus_dir,idxs,books,titles,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75826145",
   "metadata": {},
   "source": [
    "Import your spanking new corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0819c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "newcorpus = PlaintextCorpusReader(nltk_corpus_dir, '.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce05e4f",
   "metadata": {},
   "source": [
    "Sanity check.  There may be more books in the corpus if you do this on multiple days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0e31848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A_Modest_Proposal_by_Jonathan_Swift.txt',\n",
       " 'A_Room_with_a_View_by_E._M._Forster.txt',\n",
       " 'A_Study_in_Scarlet_by_Arthur_Conan_Doyle.txt',\n",
       " 'Ang_\"Filibusterismo\"_(Karugtóng_ng_Noli_Me_Tangere)_by_José_Rizal.txt',\n",
       " 'Carmilla_by_Joseph_Sheridan_Le_Fanu.txt',\n",
       " 'Don_Quixote_by_Miguel_de_Cervantes_Saavedra.txt',\n",
       " 'Dubliners_by_James_Joyce.txt',\n",
       " \"Gulliver's_Travels_into_Several_Remote_Nations_of_the_World_by_Jonathan_Swift.txt\",\n",
       " 'Jane_Eyre:_An_Autobiography_by_Charlotte_Brontë.txt',\n",
       " \"Lady_Chatterley's_lover_by_D._H._Lawrence.txt\",\n",
       " 'Middlemarch_by_George_Eliot.txt',\n",
       " 'Moby_Word_Lists_by_Grady_Ward.txt',\n",
       " 'Peter_Pan_by_J._M._Barrie.txt',\n",
       " 'Pogo_Planet_by_Donald_A._Wollheim.txt',\n",
       " 'Pygmalion_by_Bernard_Shaw.txt',\n",
       " 'Second_Treatise_of_Government_by_John_Locke.txt',\n",
       " 'Sense_and_Sensibility_by_Jane_Austen.txt',\n",
       " 'The_Adventures_of_Tom_Sawyer,_Complete_by_Mark_Twain.txt',\n",
       " 'The_Confessions_of_St._Augustine_by_Bishop_of_Hippo_Saint_Augustine.txt',\n",
       " 'The_Count_of_Monte_Cristo_by_Alexandre_Dumas_and_Auguste_Maquet.txt',\n",
       " 'The_Enchanted_April_by_Elizabeth_Von_Arnim.txt',\n",
       " 'The_Importance_of_Being_Earnest:_A_Trivial_Comedy_for_Serious_People_by_Oscar_Wilde.txt',\n",
       " 'The_King_James_Version_of_the_Bible.txt',\n",
       " 'The_Life_and_Adventures_of_Robinson_Crusoe_by_Daniel_Defoe.txt',\n",
       " 'The_Problems_of_Philosophy_by_Bertrand_Russell.txt',\n",
       " 'The_Republic_by_Plato.txt',\n",
       " 'The_Rámáyan_of_Válmíki,_translated_into_English_verse_by_Valmiki.txt',\n",
       " 'The_Strange_Case_of_Dr._Jekyll_and_Mr._Hyde_by_Robert_Louis_Stevenson.txt',\n",
       " 'The_War_of_the_Worlds_by_H._G._Wells.txt',\n",
       " 'The_Yellow_Wallpaper_by_Charlotte_Perkins_Gilman.txt',\n",
       " 'The_divine_comedy_by_Dante_Alighieri.txt',\n",
       " 'The_giant_horse_of_Oz_by_Ruth_Plumly_Thompson.txt',\n",
       " 'Thus_Spake_Zarathustra:_A_Book_for_All_and_None_by_Friedrich_Wilhelm_Nietzsche.txt',\n",
       " 'Treasure_Island_by_Robert_Louis_Stevenson.txt',\n",
       " 'Walden,_and_On_The_Duty_Of_Civil_Disobedience_by_Henry_David_Thoreau.txt',\n",
       " 'White_Nights_and_Other_Stories_by_Fyodor_Dostoyevsky.txt']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newcorpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d4c4a3",
   "metadata": {},
   "source": [
    "Get the raw string for the third book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "10d9a832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\nA STUDY IN SCARLET\\n\\nBy A. Conan Doyle\\n\\n\\n\\n\\nCONTENTS\\n\\n A STUDY IN SCARLET.\\n\\n PART I.\\n CHAPTER I. MR. SHERLOCK HOLMES.\\n CHAPTER II. THE SCIENCE OF DEDUCTION.\\n CHAPTER III. THE LAURISTON GARDENS MYSTE'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_book = newcorpus.fileids()[2]\n",
    "first_book_str = newcorpus.raw(first_book)\n",
    "first_book_str[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b65b9",
   "metadata": {},
   "source": [
    "Sentence tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f411e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(first_book_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c2c3ffb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "A STUDY IN SCARLET\n",
      "\n",
      "By A. Conan Doyle\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      " A STUDY IN SCARLET.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eac4860b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The campaign brought honours and promotion to many, but for me it had\n",
      "nothing but misfortune and disaster.\n",
      "\n",
      "******************\n",
      "\n",
      "I was removed from my brigade and\n",
      "attached to the Berkshires, with whom I served at the fatal battle of\n",
      "Maiwand.\n",
      "\n",
      "******************\n",
      "\n",
      "There I was struck on the shoulder by a Jezail bullet, which\n",
      "shattered the bone and grazed the subclavian artery.\n",
      "\n",
      "******************\n",
      "\n",
      "I should have\n",
      "fallen into the hands of the murderous Ghazis had it not been for the\n",
      "devotion and courage shown by Murray, my orderly, who threw me across a\n",
      "pack-horse, and succeeded in bringing me safely to the British lines.\n",
      "\n",
      "******************\n",
      "\n",
      "Worn with pain, and weak from the prolonged hardships which I had\n",
      "undergone, I was removed, with a great train of wounded sufferers, to\n",
      "the base hospital at Peshawar.\n"
     ]
    }
   ],
   "source": [
    "print(*sentences[35:40],sep=\"\\n\\n******************\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645b7e0",
   "metadata": {},
   "source": [
    "### Using regular expressions to search for patterns in your corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412677ed",
   "metadata": {},
   "source": [
    "For help using regular expressions for pattern searches, see [Andrew Kuchling's Regular Expressions Tutorial.](https://docs.python.org/3/howto/regex.html)\n",
    "\n",
    "For a complete description of the Python regular expression language,\n",
    "see [the very accessible Python regular expression documentation.](https://docs.python.org/3/library/re.html)\n",
    "\n",
    "If you prefer using [a web interface](https://regex101.com/r/eY4wC6/2) to testing the regular expressions\n",
    "directly in Python as in the next cell, try the link.\n",
    "\n",
    "Also see the regular expressions notebook for this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d51ed11",
   "metadata": {},
   "source": [
    "Let's use regular expressions to find all sentences beginning with \"Let's\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0232714b",
   "metadata": {},
   "source": [
    "Here's a quick look at the search pattern and some test  search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "41b25544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <re.Match object; span=(0, 5), match=\"Let's\">\n",
      "2 <re.Match object; span=(0, 5), match=\"let's\">\n",
      "5 <re.Match object; span=(0, 5), match=\"Let's\">\n",
      "5 <re.Match object; span=(0, 5), match=\"Let's\">\n",
      "3 None\n",
      "4 None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Let's at Start of string (assuming sentence tokenized input)\n",
    "# Nonalphanumeric characters follow\n",
    "reg_exp29 = r\"^Let's\\b\"\n",
    "# Ignore case, match acrss line boundaries, let . include line boundaries\n",
    "reg_exp29_c = re.compile(reg_exp29,re.I|re.M|re.S)\n",
    "\n",
    "#########  Examples   ######################################\n",
    "print(1, reg_exp29_c.search(\"Let's go fly a kite.\"))\n",
    "# OK not to capitalize\n",
    "print(2, reg_exp29_c.search(\"let's go fly a kite.\"))\n",
    "# Ok  Let's can be followed by any non alphumeric\n",
    "print(5, reg_exp29_c.search(\"Let's.\"))\n",
    "# Ok even if Let's at the end of string\n",
    "print(5, reg_exp29_c.search(\"Let's\"))\n",
    "\n",
    "# Negative result.  Let's does not start sentence.\n",
    "print(3, reg_exp29_c.search(\"You said Let's go fly a kite.\"))\n",
    "# Negative result.  Let's word internal: not followed by non alphanumeric\n",
    "print(4, reg_exp29_c.search(\"Let'sgo fly a kite.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2a6e9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Search for \"Let's\" sentence initially\n",
    "# With Single quote\n",
    "reg_exp29a = r\"^Let's\\b\"\n",
    "# With Aprostrophe ( a distinct unicode character)\n",
    "reg_exp29b = r\"^Let’s\\b\"\n",
    "reg_exp29_c = re.compile(reg_exp29b,re.I|re.M|re.S)\n",
    "found = []\n",
    "\n",
    "for sent in sent_tokenize(first_book_str):\n",
    "    res = reg_exp29_c.search(sent)\n",
    "    if res is not None:\n",
    "        found.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "00b75877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e0b5b8",
   "metadata": {},
   "source": [
    "Negative result\n",
    "\n",
    "Let's try a different pattern.\n",
    "\n",
    "Search for \"not a\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2f654309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "reg_exp31 = r\"not\\s+a\\b\"\n",
    "reg_exp31_c = re.compile(reg_exp31,re.I|re.M|re.S)\n",
    "found = []\n",
    "\n",
    "for sent in sent_tokenize(first_book_str):\n",
    "    res = reg_exp31_c.search(sent)\n",
    "    if res is not None:\n",
    "        found.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a1fa7463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78ff76",
   "metadata": {},
   "source": [
    "Search for  forms of *be* followed by \"therefore\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "83662291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#reg_exp31 = r\"\\bfall(en)?\\s+into\\b\"\n",
    "reg_exp37 = r\"\\b((was)|(were)|(are)|(been)|(be))\\s+therefore\\b\"\n",
    "reg_exp37_c = re.compile(reg_exp37,re.I|re.M|re.S)\n",
    "found = []\n",
    "\n",
    "for (i,sent) in enumerate(sent_tokenize(first_book_str)):\n",
    "    res = reg_exp37_c.search(sent)\n",
    "    if res is not None:\n",
    "        found.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "45522ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659ab6b",
   "metadata": {},
   "source": [
    "### Search through the whole saved NLTK corpus:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a3e1f",
   "metadata": {},
   "source": [
    "Using the code below assumes you have created an NLTK corpus and saved it on disk.\n",
    "If you have a sequence of strings in memory (for example `books` as defined earlier in this NB),\n",
    "consult the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8473b6a",
   "metadata": {},
   "source": [
    "Define search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "854a9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "use_punkt=True\n",
    "\n",
    "if use_punkt:\n",
    "    sent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle').tokenize\n",
    "else:\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# flags is a customizable parameter, though this is a useful default.\n",
    "# You may however not want to ignore case, and call this with flags=re.M|re.S\n",
    "def search_nltk_corpus (corpus, pattern,flags=re.I|re.M|re.S,fileids=None):\n",
    "    pattern_c = re.compile(pattern,flags)\n",
    "    found = []\n",
    "    if fileids is None:\n",
    "        fileids = newcorpus.fileids()\n",
    "    for fileid in fileids:\n",
    "        book_str = newcorpus.raw(fileid)\n",
    "        for (sent_idx,sent) in enumerate(sent_tokenize(book_str)):\n",
    "            res = pattern_c.search(sent)\n",
    "            if res is not None:\n",
    "                found.append((fileid, sent_idx, sent))\n",
    "    return found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a34626",
   "metadata": {},
   "source": [
    "Load the created corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9e99e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "newcorpus = PlaintextCorpusReader(nltk_corpus_dir, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b545c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(newcorpus)\n",
    "rdr = newcorpus.sents(newcorpus.fileids()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3846e625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'Modest', 'Proposal'], ['For', 'preventing', 'the', 'children', 'of', 'poor', 'people', 'in', 'Ireland', ',', 'from', 'being', 'a', 'burden', 'on', 'their', 'parents', 'or', 'country', ',', 'and', 'for', 'making', 'them', 'beneficial', 'to', 'the', 'publick', '.'], ...]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f929f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A_Modest_Proposal_by_Jonathan_Swift.txt',\n",
       " 'A_Room_with_a_View_by_E._M._Forster.txt',\n",
       " 'A_Study_in_Scarlet_by_Arthur_Conan_Doyle.txt',\n",
       " 'Ang_\"Filibusterismo\"_(Karugtóng_ng_Noli_Me_Tangere)_by_José_Rizal.txt',\n",
       " 'Carmilla_by_Joseph_Sheridan_Le_Fanu.txt',\n",
       " 'Don_Quixote_by_Miguel_de_Cervantes_Saavedra.txt',\n",
       " 'Dubliners_by_James_Joyce.txt',\n",
       " \"Gulliver's_Travels_into_Several_Remote_Nations_of_the_World_by_Jonathan_Swift.txt\",\n",
       " 'Jane_Eyre:_An_Autobiography_by_Charlotte_Brontë.txt',\n",
       " \"Lady_Chatterley's_lover_by_D._H._Lawrence.txt\",\n",
       " 'Middlemarch_by_George_Eliot.txt',\n",
       " 'Moby_Word_Lists_by_Grady_Ward.txt',\n",
       " 'Peter_Pan_by_J._M._Barrie.txt',\n",
       " 'Pogo_Planet_by_Donald_A._Wollheim.txt',\n",
       " 'Pygmalion_by_Bernard_Shaw.txt',\n",
       " 'Second_Treatise_of_Government_by_John_Locke.txt',\n",
       " 'Sense_and_Sensibility_by_Jane_Austen.txt',\n",
       " 'The_Adventures_of_Tom_Sawyer,_Complete_by_Mark_Twain.txt',\n",
       " 'The_Confessions_of_St._Augustine_by_Bishop_of_Hippo_Saint_Augustine.txt',\n",
       " 'The_Count_of_Monte_Cristo_by_Alexandre_Dumas_and_Auguste_Maquet.txt',\n",
       " 'The_Enchanted_April_by_Elizabeth_Von_Arnim.txt',\n",
       " 'The_Importance_of_Being_Earnest:_A_Trivial_Comedy_for_Serious_People_by_Oscar_Wilde.txt',\n",
       " 'The_King_James_Version_of_the_Bible.txt',\n",
       " 'The_Life_and_Adventures_of_Robinson_Crusoe_by_Daniel_Defoe.txt',\n",
       " 'The_Problems_of_Philosophy_by_Bertrand_Russell.txt',\n",
       " 'The_Republic_by_Plato.txt',\n",
       " 'The_Rámáyan_of_Válmíki,_translated_into_English_verse_by_Valmiki.txt',\n",
       " 'The_Strange_Case_of_Dr._Jekyll_and_Mr._Hyde_by_Robert_Louis_Stevenson.txt',\n",
       " 'The_War_of_the_Worlds_by_H._G._Wells.txt',\n",
       " 'The_Yellow_Wallpaper_by_Charlotte_Perkins_Gilman.txt',\n",
       " 'The_divine_comedy_by_Dante_Alighieri.txt',\n",
       " 'The_giant_horse_of_Oz_by_Ruth_Plumly_Thompson.txt',\n",
       " 'Thus_Spake_Zarathustra:_A_Book_for_All_and_None_by_Friedrich_Wilhelm_Nietzsche.txt',\n",
       " 'Treasure_Island_by_Robert_Louis_Stevenson.txt',\n",
       " 'Walden,_and_On_The_Duty_Of_Civil_Disobedience_by_Henry_David_Thoreau.txt',\n",
       " 'White_Nights_and_Other_Stories_by_Fyodor_Dostoyevsky.txt']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newcorpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd6f63b",
   "metadata": {},
   "source": [
    "Search for forms of be followed by \"therefore\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5c95f19f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reg_exp37 = r\"\\b((was)|(were)|(are)|(been)|(be))\\s+therefore\\b\"\n",
    "found_37 = search_nltk_corpus (newcorpus, reg_exp37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426cfcf9",
   "metadata": {},
   "source": [
    "The search yielded many examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ab9cc533",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(found_37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984ed9be",
   "metadata": {},
   "source": [
    "#### The case of apostrophe (allo-characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088fa19",
   "metadata": {},
   "source": [
    "This search failed before, but we tried only one book.  Now let's try the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "045fc039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyboard apostrophe (=single quote)\n",
    "reg_exp29a = r\"^Let's\\b\"\n",
    "# Unicode apostrophe\n",
    "reg_exp29b = r\"^Let’s\\b\"\n",
    "\n",
    "# Unicode first\n",
    "found_29b = search_nltk_corpus (newcorpus, reg_exp29b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1a81b77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(found_29b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "24ab9145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A_Room_with_a_View_by_E._M._Forster.txt', 3879, 'Let’s tell her.'),\n",
       " ('A_Room_with_a_View_by_E._M._Forster.txt',\n",
       "  3937,\n",
       "  'Let’s turn\\nin here.”\\n\\n“Here” was the British Museum.'),\n",
       " ('A_Room_with_a_View_by_E._M._Forster.txt', 3942, 'Let’s go to Mudie’s.'),\n",
       " ('A_Room_with_a_View_by_E._M._Forster.txt', 4015, 'Let’s\\nall go.'),\n",
       " ('Pygmalion_by_Bernard_Shaw.txt',\n",
       "  481,\n",
       "  'Let’s see how fast you can make her hop it.'),\n",
       " ('Pygmalion_by_Bernard_Shaw.txt', 1345, 'Let’s give him ten.'),\n",
       " ('The_Adventures_of_Tom_Sawyer,_Complete_by_Mark_Twain.txt',\n",
       "  1726,\n",
       "  'Let’s us go, too, Tom.”\\n\\n“I won’t!'),\n",
       " ('The_Adventures_of_Tom_Sawyer,_Complete_by_Mark_Twain.txt',\n",
       "  2508,\n",
       "  'Let’s hide the tools in the bushes.”\\n\\nThe boys were there that night, about the appointed time.'),\n",
       " ('The_Adventures_of_Tom_Sawyer,_Complete_by_Mark_Twain.txt',\n",
       "  2593,\n",
       "  'Let’s run!”\\n\\n“Keep still!'),\n",
       " ('The_Adventures_of_Tom_Sawyer,_Complete_by_Mark_Twain.txt',\n",
       "  3167,\n",
       "  'Let’s try some other way, so as not to go\\nthrough there.”\\n\\n“Well.'),\n",
       " ('The_Adventures_of_Tom_Sawyer,_Complete_by_Mark_Twain.txt',\n",
       "  3484,\n",
       "  'Let’s snake it\\nout.'),\n",
       " ('The_Confessions_of_St._Augustine_by_Bishop_of_Hippo_Saint_Augustine.txt',\n",
       "  1021,\n",
       "  'Let’s have a go of the rum.”\\n\\n“Dick,” said Silver, “I trust you.'),\n",
       " ('The_Confessions_of_St._Augustine_by_Bishop_of_Hippo_Saint_Augustine.txt',\n",
       "  1155,\n",
       "  'Let’s allow the men an afternoon ashore.'),\n",
       " ('The_Enchanted_April_by_Elizabeth_Von_Arnim.txt',\n",
       "  3937,\n",
       "  'Let’s go indoors to the fire and\\nMrs. Fisher.'),\n",
       " ('Thus_Spake_Zarathustra:_A_Book_for_All_and_None_by_Friedrich_Wilhelm_Nietzsche.txt',\n",
       "  2181,\n",
       "  'Let’s.')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_29b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8015af3",
   "metadata": {},
   "source": [
    "Then apostrophe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7fc2ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apostrophe\n",
    "reg_exp29a = r\"^Let's\\b\"\n",
    "found_29a = search_nltk_corpus (newcorpus, reg_exp29a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17429d",
   "metadata": {},
   "source": [
    "In fact the two searches yield disjoint results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a9e7d2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(found_29a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "25bd5367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(found_29b) & set(found_29a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88277a78",
   "metadata": {},
   "source": [
    "So two different unicode characters, ASCII apostrophe and unicode apostrophe, can express the English apostrophe in our data. Therefore, to do the search, we need to look for ASCII apostrophe OR unicode apostrophe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "88c01fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_exp29a_or_b = r\"^Let('|’)s\\b\"\n",
    "found_29a_or_b = search_nltk_corpus (newcorpus, reg_exp29a_or_b)\n",
    "len(found_29a_or_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b7057a",
   "metadata": {},
   "source": [
    "History: The search that got me some examples of unicode apostrophe printing out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d12c75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp29 = r\"^Let\\b\"\n",
    "found_29 = search_nltk_corpus (newcorpus, reg_exp29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "86cde607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(found_29)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab256f",
   "metadata": {},
   "source": [
    "#### A more complicated regular expression\n",
    "\n",
    "Parsing the next reg exp (search for \"fall into\", \"fell into\", or \"falling into\" or \"fallen into\"):\n",
    "\n",
    "$$\n",
    "\\begin{array}[t]{cccccccc}\n",
    "\\text{\\\\b}& \\text{f} & \\text{(a | e)} & \\text{ll} & \\text{((en)|(ing))?} & \\text{\\\\s+} & \\text{into}& \\text{\\\\b}\\\\\n",
    "(0) & (1) & (2) & (3) & (4) & (5) & (6) & (7) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "0.  No characters that can appear inside a word can precede the character \"f\".\n",
    "1.  Character \"f\" here.\n",
    "2.  Either the character \"a\" or the character \"e\" here.\n",
    "3.  Characters \"ll\" (as in \"llama\") here.\n",
    "4.  Optionally: Characters \"en\" or characters \"ing\" here.\n",
    "5.  Arbitrary number of white space characters here, but at least one. White space characters include line breaks\n",
    "6.  Characters \"into\" here\n",
    "7.  No characters that can appear inside a word can follow \"into\" so the word \"into\" can appear here and be followed by a space or a comma,  but not the characters \"xication\", as in \"intoxication\".\n",
    "\n",
    "As result, the regular expression matches the bracketed part of all of the following:\n",
    "\n",
    "```\n",
    "They may [fall into] trouble.\n",
    "She is [falling into] bad habits.\n",
    "The boy [fell into] a deep hole.\n",
    "You have [fallen into] a trap\n",
    "through [fall into] winter\n",
    "```\n",
    "as well as\n",
    "\n",
    "```\n",
    "I afraid I will [fall\n",
    "into] bad habits.\n",
    "```\n",
    " where \"fall\" and \"into\" appear on separate lines.  It also matches\n",
    " \n",
    " ```\n",
    " [fellen into]\n",
    " [felling into]\n",
    " ```\n",
    " \n",
    " It matches neither of the following (failed match in parens)\n",
    " \n",
    " ```\n",
    " she felt none of that summer sadness or (fall into)xication\n",
    " This sudden turn of events threw one of those men few tragedies be(fall into) a deep depression.\n",
    " ```\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1c61d7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Allow fall into, fell into, fallen into, falling into\n",
    "reg_exp31 = r\"\\bf(a|e)ll((en)|(ing))?\\s+into\\b\"\n",
    "found_31 = search_nltk_corpus (newcorpus, reg_exp31)\n",
    "len(found_31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bf104b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A_Room_with_a_View_by_E._M._Forster.txt',\n",
       "  2657,\n",
       "  'The bank broke away, and he fell into the pool before he had weighed\\nthe question properly.'),\n",
       " ('A_Room_with_a_View_by_E._M._Forster.txt',\n",
       "  2996,\n",
       "  'I fell into\\nall those violets, and he was silly and surprised.'),\n",
       " ('A_Room_with_a_View_by_E._M._Forster.txt',\n",
       "  3256,\n",
       "  '“Also\\nthat men fall into two classes—those who forget views and those who\\nremember them, even in small rooms.”\\n\\n“Mr.'),\n",
       " ('A_Study_in_Scarlet_by_Arthur_Conan_Doyle.txt',\n",
       "  38,\n",
       "  'I should have\\nfallen into the hands of the murderous Ghazis had it not been for the\\ndevotion and courage shown by Murray, my orderly, who threw me across a\\npack-horse, and succeeded in bringing me safely to the British lines.'),\n",
       " ('Carmilla_by_Joseph_Sheridan_Le_Fanu.txt',\n",
       "  989,\n",
       "  'Very late, she said, she had got to the\\nhousekeeper’s bedroom in despair of finding us, and had then fallen\\ninto a deep sleep which, long as it was, had hardly sufficed to recruit\\nher strength after the fatigues of the ball.'),\n",
       " ('Dubliners_by_James_Joyce.txt',\n",
       "  968,\n",
       "  'On the contrary, by granting life to those papers, let the\\ncruelty of Marcela live for ever, to serve as a warning in ages to come\\nto all men to shun and avoid falling into like danger; or I and all of\\nus who have come here know already the story of this your love-stricken\\nand heart-broken friend, and we know, too, your friendship, and the\\ncause of his death, and the directions he gave at the close of his\\nlife; from which sad story may be gathered how great was the cruelty of\\nMarcela, the love of Chrysostom, and the loyalty of your friendship,\\ntogether with the end awaiting those who pursue rashly the path that\\ninsane passion opens to their eyes.'),\n",
       " ('Dubliners_by_James_Joyce.txt',\n",
       "  1642,\n",
       "  'Cardenio was then in his\\nright mind, free from any attack of that madness which so frequently\\ncarried him away, and seeing them dressed in a fashion so unusual among\\nthe frequenters of those wilds, could not help showing some surprise,\\nespecially when he heard them speak of his case as if it were a\\nwell-known matter (for the curate’s words gave him to understand as\\nmuch) so he replied to them thus:\\n\\n“I see plainly, sirs, whoever you may be, that Heaven, whose care it is\\nto succour the good, and even the wicked very often, here, in this\\nremote spot, cut off from human intercourse, sends me, though I deserve\\nit not, those who seek to draw me away from this to some better\\nretreat, showing me by many and forcible arguments how unreasonably I\\nact in leading the life I do; but as they know, that if I escape from\\nthis evil I shall fall into another still greater, perhaps they will\\nset me down as a weak-minded man, or, what is worse, one devoid of\\nreason; nor would it be any wonder, for I myself can perceive that the\\neffect of the recollection of my misfortunes is so great and works so\\npowerfully to my ruin, that in spite of myself I become at times like a\\nstone, without feeling or consciousness; and I come to feel the truth\\nof it when they tell me and show me proofs of the things I have done\\nwhen the terrible fit overmasters me; and all I can do is bewail my lot\\nin vain, and idly curse my destiny, and plead for my madness by telling\\nhow it was caused, to any that care to hear it; for no reasonable\\nbeings on learning the cause will wonder at the effects; and if they\\ncannot help me at least they will not blame me, and the repugnance they\\nfeel at my wild ways will turn into pity for my woes.'),\n",
       " ('Dubliners_by_James_Joyce.txt',\n",
       "  2121,\n",
       "  'Had not Camilla, however, been informed beforehand by\\nLothario that this love for Chloris was a pretence, and that he himself\\nhad told Anselmo of it in order to be able sometimes to give utterance\\nto the praises of Camilla herself, no doubt she would have fallen into\\nthe despairing toils of jealousy; but being forewarned she received the\\nstartling news without uneasiness.'),\n",
       " ('Dubliners_by_James_Joyce.txt',\n",
       "  3380,\n",
       "  'Consider, too, that your daughter Mari-Sancha will not die\\nof grief if we marry her; for I have my suspicions that she is as eager\\nto get a husband as you to get a government; and, after all, a daughter\\nlooks better ill married than well whored.”\\n\\n“By my faith,” replied Sancho, “if God brings me to get any sort of a\\ngovernment, I intend, wife, to make such a high match for Mari-Sancha\\nthat there will be no approaching her without calling her ‘my lady.”\\n\\n“Nay, Sancho,” returned Teresa; “marry her to her equal, that is the\\nsafest plan; for if you put her out of wooden clogs into high-heeled\\nshoes, out of her grey flannel petticoat into hoops and silk gowns, out\\nof the plain ‘Marica’ and ‘thou,’ into ‘Doña So-and-so’ and ‘my lady,’\\nthe girl won’t know where she is, and at every turn she will fall into\\na thousand blunders that will show the thread of her coarse homespun\\nstuff.”\\n\\n“Tut, you fool,” said Sancho; “it will be only to practise it for two\\nor three years; and then dignity and decorum will fit her as easily as\\na glove; and if not, what matter?'),\n",
       " ('Dubliners_by_James_Joyce.txt',\n",
       "  3412,\n",
       "  'There are men\\nof low rank who strain themselves to bursting to pass for gentlemen,\\nand high gentlemen who, one would fancy, were dying to pass for men of\\nlow rank; the former raise themselves by their ambition or by their\\nvirtues, the latter debase themselves by their lack of spirit or by\\ntheir vices; and one has need of experience and discernment to\\ndistinguish these two kinds of gentlemen, so much alike in name and so\\ndifferent in conduct.”\\n\\n“God bless me!” said the niece, “that you should know so much,\\nuncle—enough, if need be, to get up into a pulpit and go preach in the\\nstreets—and yet that you should fall into a delusion so great and a\\nfolly so manifest as to try to make yourself out vigorous when you are\\nold, strong when you are sickly, able to put straight what is crooked\\nwhen you yourself are bent by age, and, above all, a caballero when you\\nare not one; for though gentlefolk may be so, poor men are nothing of\\nthe kind!”\\n\\n“There is a great deal of truth in what you say, niece,” returned Don\\nQuixote, “and I could tell you somewhat about birth that would astonish\\nyou; but, not to mix up things human and divine, I refrain.')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_31[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa01776e",
   "metadata": {},
   "source": [
    "### Search through a list of doc strings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d9e9e",
   "metadata": {},
   "source": [
    "This code will get you through the next two sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a9973a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import urllib\n",
    "\n",
    "def get_book (ind):\n",
    "    url = f\"https://gutenberg.org/cache/epub/{ind}/pg{ind}.txt\"\n",
    "    with urllib.request.urlopen(url) as stream:\n",
    "        byte_str = stream.read()\n",
    "        return byte_str.decode(\"UTF8\").replace(\"\\r\\n\",\"\\n\")\n",
    "\n",
    "def search_doc(book_str, pattern_c,found=None,book_id='',count_hits=False,flags=re.I|re.M|re.S):\n",
    "    global findall_sents,matches,match_obj,sent0,sentx\n",
    "    if found is None:\n",
    "        found = []\n",
    "    count,findall_sents,matches = 0,[],[]\n",
    "    for (sent_idx,sent) in enumerate(sent_tokenize(book_str)):\n",
    "        sentx=sent\n",
    "        try:\n",
    "            res = pattern_c.search(sent)\n",
    "        except AttributeError:\n",
    "            pattern_c = re.compile(pattern_c,flags)\n",
    "            res = pattern_c.search(sent)\n",
    "        if res is not None and count_hits:\n",
    "            findalls=pattern_c.findall(sent)\n",
    "            this_count = len(findalls)\n",
    "            count += this_count\n",
    "            findall_sents.extend(findalls)\n",
    "            sent0 = sent\n",
    "            for i in range(this_count):\n",
    "                match_obj = pattern_c.search(sent0)\n",
    "                if match_obj is None:\n",
    "                    break\n",
    "                z = match_obj.start() - 5\n",
    "                start = z if z > 0 else 0\n",
    "                extracted = sent0[start:match_obj.end()+5]\n",
    "                if extracted:\n",
    "                    matches.append(extracted)\n",
    "                else:\n",
    "                    pass ## Debugging statementts now outdated\n",
    "                    #print(extracted, sent0)\n",
    "                    #print(sent,end=\"\\n*****\\n\")\n",
    "                    #sentx=sent\n",
    "                    #raise Exception\n",
    "                sent0 = sent0[match_obj.end()+1:]\n",
    "            found.append((book_id, sent_idx, sent))\n",
    "        elif res is not None:\n",
    "            found.append((book_id, sent_idx, sent))\n",
    "    if count_hits:\n",
    "        return (count,found)\n",
    "    else:\n",
    "        return found\n",
    "        \n",
    "    \n",
    "def search_doc_strings (book_strings, pattern):\n",
    "    pattern_c = re.compile(pattern, re.I|re.M|re.S)\n",
    "    found = []\n",
    "    for (book_id,book_str) in enumerate(book_strings):\n",
    "        search_doc(book_str, pattern_c,found=found,book_id=book_id)\n",
    "    return found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d56e7",
   "metadata": {},
   "source": [
    "To search through a list of strings, do the following.\n",
    "\n",
    "If you have been following along, `books` should be a list of doc strings defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a7d88c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp37 = r\"\\b((was)|(were)|(are)|(been)|(be))\\s+therefore\\b\"\n",
    "found_37 = search_doc_strings(books, reg_exp37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1b9edd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(found_37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa1671",
   "metadata": {},
   "source": [
    "###  Searching an arbitrary book on Gutenberg.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4902d",
   "metadata": {},
   "source": [
    "When looking up the index of a book, make sure it is not the index of the audio book.  Many books appear on\n",
    "Gutenberg in both print and audio form, and they have different indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "105b3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frankenstein_idx = 84\n",
    "reg_exp37 = r\"\\b((was)|(were)|(are)|(been)|(be))\\s+therefore\\b\"\n",
    "text2= get_book(84)\n",
    "found = search_doc(text2, reg_exp37, book_id='Frankenstein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "56f0d784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Frankenstein',\n",
       "  202,\n",
       "  'You have been tutored and\\nrefined by books and retirement from the world, and you are therefore\\nsomewhat fastidious; but this only renders you the more fit to\\nappreciate the extraordinary merits of this wonderful man.'),\n",
       " ('Frankenstein',\n",
       "  384,\n",
       "  'My\\ndeparture was therefore fixed at an early date, but before the day\\nresolved upon could arrive, the first misfortune of my life\\noccurred—an omen, as it were, of my future misery.')]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a630b",
   "metadata": {},
   "source": [
    "How often when Sherlock Holmes is referred to, is his first name used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a28deaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search A study in Scarlet; idx is 244\n",
    "reg_exp43 = r\"\\bSherlock\\b\"\n",
    "text3 = get_book(244)\n",
    "found_43 = search_doc(text3, reg_exp43, book_id='A Study in Scarlet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8b449368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(found_43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b5c8d",
   "metadata": {},
   "source": [
    "How often is his last name used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1f1e270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search A study in Scarlet; idx is 244\n",
    "reg_exp47 = r\"\\bHolmes\\b\"\n",
    "# Uncomment if needed\n",
    "#text3 = get_book(244)\n",
    "found_47 = search_doc(text3, reg_exp47, book_id='A Study in Scarlet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "97f1f416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(found_47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d7335504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A Study in Scarlet', 5, 'CHAPTER I. MR. SHERLOCK HOLMES.'),\n",
       " ('A Study in Scarlet',\n",
       "  32,\n",
       "  '(_Being a reprint from the Reminiscences of_ JOHN H. WATSON, M.D.,\\n_Late of the Army Medical Department._)\\n\\n\\n\\n\\nCHAPTER I.\\nMR. SHERLOCK HOLMES.'),\n",
       " ('A Study in Scarlet',\n",
       "  63,\n",
       "  '“You\\ndon’t know Sherlock Holmes yet,” he said; “perhaps you would not care\\nfor him as a constant companion.”\\n\\n“Why, what is there against him?”\\n\\n“Oh, I didn’t say there was anything against him.'),\n",
       " ('A Study in Scarlet',\n",
       "  82,\n",
       "  '“Holmes is a little too scientific for my tastes—it approaches\\nto cold-bloodedness.'),\n",
       " ('A Study in Scarlet',\n",
       "  100,\n",
       "  'Watson, Mr. Sherlock Holmes,” said Stamford, introducing us.'),\n",
       " ('A Study in Scarlet',\n",
       "  130,\n",
       "  'Now we have the\\nSherlock Holmes’ test, and there will no longer be any difficulty.”\\n\\nHis eyes fairly glittered as he spoke, and he put his hand over his\\nheart and bowed as if to some applauding crowd conjured up by his\\nimagination.'),\n",
       " ('A Study in Scarlet',\n",
       "  137,\n",
       "  'Call it the ‘Police\\nNews of the Past.’”\\n\\n“Very interesting reading it might be made, too,” remarked Sherlock\\nHolmes, sticking a small piece of plaster over the prick on his finger.'),\n",
       " ('A Study in Scarlet',\n",
       "  140,\n",
       "  '“My friend here wants to take diggings, and as you were\\ncomplaining that you could get no one to go halves with you, I thought\\nthat I had better bring you together.”\\n\\nSherlock Holmes seemed delighted at the idea of sharing his rooms with\\nme.'),\n",
       " ('A Study in Scarlet',\n",
       "  174,\n",
       "  'That\\nvery evening I moved my things round from the hotel, and on the\\nfollowing morning Sherlock Holmes followed me with several boxes and\\nportmanteaus.'),\n",
       " ('A Study in Scarlet',\n",
       "  177,\n",
       "  'Holmes was certainly not a difficult man to live with.')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_47[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d16e89",
   "metadata": {},
   "source": [
    "Note the previous two counts include overlap, because there are cases\n",
    "in which \"Sherlock Holmes\" is used to refer to Sherlock Holmes. A more difficult regular expression is \n",
    "to look for cases in which uses of \"Sherlock\" are not followed by \"Holmes\".\n",
    "This involves **negative lookahead**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "958363b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sherlock not followed by \" Holmes\"\n",
    "reg_exp53 = r\"\\bSherlock(?!\\s+Holmes\\b)\"\n",
    "found_53 = search_doc(text3, reg_exp53, book_id='A Study in Scarlet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "95a27b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff30ec5",
   "metadata": {},
   "source": [
    "Confirming what every Sherlock Holmes fan knows:  Watson never calls Sherlock Holmes Sherlock.\n",
    "It's always \"Holmes, how the devil do you know that?\" or \"It can't be him, Holmes!\"  In fact,\n",
    "Watson rarely does even that.  If we search for occurrences of \"Holmes\" inside quotation\n",
    "marks, a left quotation mark followed by any number of non-right quotation mark characters\n",
    "followed by \"Holmes\" when it is not preceded by \"Sherlock\" (involving a negative lookbehind),\n",
    "we get only one genuine hit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "756a297a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A Study in Scarlet',\n",
       "  564,\n",
       "  '“There is nothing like first hand evidence,” he remarked; “as a matter\\nof fact, my mind is entirely made up upon the case, but still we may as\\nwell learn all that is to be learned.”\\n\\n“You amaze me, Holmes,” said I.')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_exp57 = r\"“[^”]+(?<!Sherlock)\\s+Holmes\"\n",
    "search_doc(text3, reg_exp57, book_id='A Study in Scarlet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095bf572",
   "metadata": {},
   "source": [
    "###  Looking at the occurrences of a taboo word in Huck Finn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "51da42af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx=76\n",
    "# Optional \"s\"  Match the plural as well\n",
    "reg_exp_43 = r\"niggers?\"\n",
    "\n",
    "#The has a few false negatives because of some instances of \"_<PAT>_\"  disallowed by `\\b`\n",
    "#because `_` can occur in a word.\n",
    "#reg_exp_43a = r\"\\bniggers?\\b\"\n",
    "#count_43a,found_43a = search_doc(text_43, reg_exp_43a, book_id='Huckleberry Finn',count_hits=True)\n",
    "\n",
    "#text_43 = get_book(idx)\n",
    "count_43,found_43 = search_doc(text_43, reg_exp_43, book_id='Huckleberry Finn',count_hits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "ce42f7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d5a69",
   "metadata": {},
   "source": [
    "Our `count_43` just about agrees with the N-word count of 219, cited in [the NYT book review of Percival Everett's new book *James*, ]( https://www.nytimes.com/2024/03/11/books/review/percival-everett-james.html)\n",
    "which is *Huckleberry Finn* told from Jim's point of view.  We can't explain the discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a3d09",
   "metadata": {},
   "source": [
    "Sanity check:  the set of words matched with the reg exp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e9ed1b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Nigger', 'Niggers', 'nigger', 'niggers'}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(findall_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd47b41",
   "metadata": {},
   "source": [
    "So the n-word occurs quite often.  Nearly twice as many times as the word *raft*, which is  very important\n",
    "in this story about a trip down the Mississippi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "da6c5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp_59 = r\"\\braft\\b\"\n",
    "count_59,found_59 = search_doc(text_43, reg_exp_59, book_id='Huckleberry Finn',count_hits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "c8f1dba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22bc17",
   "metadata": {},
   "source": [
    "#### Collecting the result of multiple searches\n",
    "\n",
    "You can append the results of a search to previous results, if you want.  Just be sure to give\n",
    "book_id a useful value, so you know what book yielded what results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "2531e5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "#Search E.M. Forster: A Room with a View = 2641\n",
    "reg_exp47 = r\"\\bfashionable\\s+world\\b\"\n",
    "text4 = get_book(2641)\n",
    "print(len(found))\n",
    "search_doc(text4, reg_exp47, book_id='A Room with a View', found=found)\n",
    "print(len(found))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2a184",
   "metadata": {},
   "source": [
    "### Twitter texts  emoji collecting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784236f",
   "metadata": {},
   "source": [
    "Finding emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c6f2c",
   "metadata": {},
   "source": [
    "Initial pass.  Check data feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "200f5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001f602 😂 FACE WITH TEARS OF JOY 24225 found!\n",
      "0001f44d 👍 THUMBS UP SIGN 2459 found!\n",
      "0001f525 🔥 FIRE 2331 found!\n"
     ]
    }
   ],
   "source": [
    "from re import findall\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "\n",
    "#😂 code: 128514 face with tears of joy\n",
    "#👍 code: 128077 thumbs up\n",
    "#🔥 code: 128293 fire\n",
    "codes = [128514,128077,128293]\n",
    "######  OR ##################\n",
    "#inc=60\n",
    "#inc=1036\n",
    "#codes = range(0x1F600, 0x1F600+inc)\n",
    "\n",
    "\n",
    "emoji_cts_initial = defaultdict(list)\n",
    "\n",
    "with open ('chat_corp/chat_corpus-master/twitter_en.txt') as fh:\n",
    "    # A list of lines\n",
    "    text_str = fh.read()\n",
    " \n",
    "# There are long uninterrupted seqs of printable emoji starting at 0x1F600\n",
    "# and 0001f900\n",
    "for code in codes:\n",
    "    found = findall(chr(code),text_str)\n",
    "    try:\n",
    "        print(f\"{code:08x} {chr(code)} {unicodedata.name(chr(code))} {len(found)} found!\")\n",
    "    except ValueError:\n",
    "        print(f\"****Couldn't print {code}!*****\")\n",
    "    emoji_cts_initial[code] = len(found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5898d9",
   "metadata": {},
   "source": [
    "#### The main task\n",
    "\n",
    "1. For each of the three chosen emoji types:\n",
    "\n",
    "   a. Find the tweets in the emoji type occurs at the end of tweet.  Save to a file\n",
    "   b. Find the tweets in the emoji type occurs at the start of tweet.  Save to a file\n",
    "   c. Find the tweets in the emoji type occurs anywhere in the tweet.  Save to a file\n",
    "\n",
    "2.  For all the hits in 1, preserve the line number and get the other half of the conversational pair\n",
    "    that line belongs to.  For tweets with index i, i is odd, get tweet i - 1 (00-based indexing).\n",
    "    For tweets with index i, i is even, get tweet i + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37e44095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from re import findall\n",
    "import unicodedata\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "with open ('chat_corp/chat_corpus-master/twitter_en.txt') as fh:\n",
    "    # A list of lines\n",
    "    text = fh.readlines()\n",
    "    \n",
    "emoji_results = defaultdict(list)\n",
    "emoji_results_start = defaultdict(list)\n",
    "emoji_results_end = defaultdict(list)\n",
    "emoji_counts = Counter()\n",
    "double_hits =  defaultdict(set)\n",
    "# The three choisen emoji\n",
    "codes = [128514,128077,128293]\n",
    "\n",
    "for (i,line) in enumerate(text):\n",
    "    line=line.strip()\n",
    "    for code in codes:\n",
    "        if i in double_hits[code]:\n",
    "            continue\n",
    "        emoji = chr(code)\n",
    "        if re.search(emoji,line):\n",
    "            if i % 2 == 1:\n",
    "                other_line = text[i-1].strip()\n",
    "                pair = (i-1, other_line, line)\n",
    "            else:\n",
    "                other_line = text[i+1].strip()\n",
    "                pair = (i, line, other_line)\n",
    "                if re.search(emoji,other_line):\n",
    "                    double_hits[code].add(i+1)\n",
    "                    emoji_counts[code] += other_line.count(emoji)\n",
    "            emoji_results[code].append(pair)\n",
    "            if re.match(chr(code),line):\n",
    "                emoji_results_start[code].append(pair)\n",
    "            if line.endswith(chr(code)):\n",
    "                emoji_results_end[code].append(pair)\n",
    "            emoji_counts[code] += line.count(emoji)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4c2a2",
   "metadata": {},
   "source": [
    "Checking that (even,odd) constitutes a conversational pair, because of 0-based indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "03370867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 you think so ? a lot girls been fucking with it just not the right ones lmfao, but i'm thinking its time for the new cut 😂\n",
      "\n",
      "39 idk how females fuck with this 😂\n",
      "\n",
      "==============================\n",
      "\n",
      "40 dad!! ’s dad is our new favorite person.\n",
      "\n",
      "41 u gotta love his dad😍\n",
      "\n",
      "==============================\n",
      "\n",
      "42 i bet these guys that are playing d for philly feel so fresh!\n",
      "\n",
      "43 after those takeaways! bet they're competing for who gets the next one!!\n",
      "\n",
      "==============================\n",
      "\n",
      "44 4-0. i don't remember you ever winning. &amp; i took this one. you didn't \"let\" nothing. 😂\n",
      "\n",
      "45 😭😭😭 i thought you was gonna let me slide\n",
      "\n",
      "==============================\n",
      "\n",
      "46 oh no she's back 😒\n",
      "\n",
      "47 you talking about the brand ambassador of watsapp?\n",
      "\n",
      "==============================\n",
      "\n",
      "48 after a long hiatus, i've joined a gym. thus ends my rather wonderful minimum viable body phase. 🙏🏽\n",
      "\n",
      "49 bay club or equinox?\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start=38\n",
    "for (i,l) in enumerate(text[start:50],start):\n",
    "    print(i, l)\n",
    "    if i%2 == 1:\n",
    "        print(\"=\"*30, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89bfab",
   "metadata": {},
   "source": [
    "Checking the first 3 hit pairs for FACE WITH TEARS OF JOY in the dictionary `emoji_results`.  One member of the conversational pair should have FACE WITH TEARS OF JOY somewhere in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb1ebed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24225\n",
      "12166\n",
      "(38, \"you think so ? a lot girls been fucking with it just not the right ones lmfao, but i'm thinking its time for the new cut 😂\", 'idk how females fuck with this 😂')\n",
      "(44, '4-0. i don\\'t remember you ever winning. &amp; i took this one. you didn\\'t \"let\" nothing. 😂', '😭😭😭 i thought you was gonna let me slide')\n",
      "(52, \"you think so ? a lot girls been fucking with it just not the right ones lmfao, but i'm thinking its time for the new cut 😂\", 'gotta get that brad pitt from fury')\n"
     ]
    }
   ],
   "source": [
    "code = 128514\n",
    "face = emoji_results[code]\n",
    "\n",
    "print(emoji_counts[code]) # Correct number of hits based on initial pass counts\n",
    "print(len(face)) # These two numbers are different because a single conversational pair\n",
    "                 # may contain mutltiple hits on the same emoji.  See 44 below\n",
    "for tw in face[:3]:\n",
    "    print(tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd96e78",
   "metadata": {},
   "source": [
    "Checking dictionary `emoji_results_start`: One member of the conversational pair should have FACE WITH TEARS OF JOY at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a97b36de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1068\n",
      "(1916, 'give him some water 😭', \"😂😂😂😂 we should've gave him some. he would've been out of the race\")\n",
      "(2174, '😂😂😂😂 i9milha boogie', 'iphone 7 w nba 2k17 oula full o*****m 😂')\n",
      "(2348, 'dad: u want me to pay for this?! me: ya...sorry dad: well thats okay guys i got a wallet that says shit ton of money', '😂i love it!!!')\n"
     ]
    }
   ],
   "source": [
    "code = 128514\n",
    "face_start = emoji_results_start[code]\n",
    "\n",
    "print(len(face_start)) # These two numbers are different because a single conversational pair\n",
    "                 # may contain mutltiple hits on the same emoji.  See 44 below\n",
    "for tw in face_start[:3]:\n",
    "    print(tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2975d6f",
   "metadata": {},
   "source": [
    "Checking dictionary `emoji_results_end`: One member of the conversational pair should have FACE WITH TEARS OF JOY at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "924879c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8048\n",
      "(754300, \"i know someone who'd cry..\", 'lol not gonna lie i probably would cry 😂')\n",
      "(754490, \"omg beau brought his friend who is from san diego, but is actually from london but he's mexican and italian i'm so confused 😂😂\", \"and it doesn't help that he's drunk 💀💀\")\n",
      "(754496, 'thou have broughten the fermented oat elixir😂😂', 'i poureth some for the comrades')\n"
     ]
    }
   ],
   "source": [
    "code = 128514\n",
    "face_end = emoji_results_end[code]\n",
    "\n",
    "print(len(face_end)) # These two numbers are different because a single conversational pair\n",
    "                 # may contain mutltiple hits on the same emoji.  See 44 below\n",
    "for tw in face_end[-3:]:\n",
    "    print(tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e9eed3",
   "metadata": {},
   "source": [
    "#### Saving to files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8abbd8",
   "metadata": {},
   "source": [
    "The basic idea.  Create DataFrames. Use `df.to_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "49987b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "(index, initial, response) = zip(*emoji_results[code])\n",
    "df = pd.DataFrame(dict(Utterance=initial,Response=response),index=index, columns=[\"Utterance\",\"Response\"])\n",
    "#emoji_results[code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b5f406e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>no panty up at tonight 🇵🇷👊🏾</td>\n",
       "      <td>that song is 🔥🔥🔥🔥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>these joints are so 🔥</td>\n",
       "      <td>so flee, remind me of these laser af 1 but way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>happy birthday gabby ❣🔥🔥🔥</td>\n",
       "      <td>thank you !!!!💙💙</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>yo it's true</td>\n",
       "      <td>your article voiced all the feelings i had whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4814</th>\n",
       "      <td>burn it 🔥🔥🔥🔥🔥</td>\n",
       "      <td>nah bruh he's my exterminator. 🕷🕷🕷🕷</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Utterance  \\\n",
       "486   no panty up at tonight 🇵🇷👊🏾   \n",
       "3230        these joints are so 🔥   \n",
       "3452    happy birthday gabby ❣🔥🔥🔥   \n",
       "4268                 yo it's true   \n",
       "4814                burn it 🔥🔥🔥🔥🔥   \n",
       "\n",
       "                                               Response  \n",
       "486                                   that song is 🔥🔥🔥🔥  \n",
       "3230  so flee, remind me of these laser af 1 but way...  \n",
       "3452                                   thank you !!!!💙💙  \n",
       "4268  your article voiced all the feelings i had whe...  \n",
       "4814                nah bruh he's my exterminator. 🕷🕷🕷🕷  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "de164d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_stems = [\"anywhere\", \"start\", \"end\"]\n",
    "emoji_dicts = dict(anywhere=emoji_results, start=emoji_results_start, end=emoji_results_end)\n",
    "file_dict = dict()\n",
    "\n",
    "def get_filename (code,emoji_loc):\n",
    "    emoji_name_str = '_'.join(unicodedata.name(chr(code)).lower().split())\n",
    "    return '_'.join([emoji_loc,emoji_name_str]) + '.csv'\n",
    "\n",
    "for emoji_loc in fn_stems:\n",
    "    emoji_dict = emoji_dicts[emoji_loc]\n",
    "    for code in emoji_dict:\n",
    "        (index, initial, response) = zip(*emoji_dict[code])\n",
    "        df = pd.DataFrame(dict(Utterance=initial,Response=response),\n",
    "                          index=index, \n",
    "                          columns=[\"Utterance\",\"Response\"])\n",
    "        fn = get_filename (code,emoji_loc)\n",
    "        file_dict[fn] = df\n",
    "        df.to_csv(fn,header=True,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "657fad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anywhere_face_with_tears_of_joy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w9/bx4mylnd27g_kqqgn5hrn2x40000gr/T/ipykernel_78514/2565861400.py:4: FutureWarning: column_space is deprecated and will be removed in a future version. Use df.to_string(col_space=...) instead.\n",
      "  pd.set_option('display.column_space', 1000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>you think so ? a lot girls been fucking with i...</td>\n",
       "      <td>idk how females fuck with this 😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4-0. i don't remember you ever winning. &amp;amp; ...</td>\n",
       "      <td>😭😭😭 i thought you was gonna let me slide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>you think so ? a lot girls been fucking with i...</td>\n",
       "      <td>gotta get that brad pitt from fury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>guess youre gon have to bite me hahaha</td>\n",
       "      <td>&amp;lt;-- she volunteered so... yeah. haha. go fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>no you deadass said i'm getting a crouton for ...</td>\n",
       "      <td>this is funny af 😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754170</th>\n",
       "      <td>😂😂 ima hit the r2 button and hit her with that...</td>\n",
       "      <td>lmaoooo nooo not the coon cock lmaoooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754246</th>\n",
       "      <td>he felt so bad 😂💀</td>\n",
       "      <td>lonzo ball too good!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754300</th>\n",
       "      <td>i know someone who'd cry..</td>\n",
       "      <td>lol not gonna lie i probably would cry 😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754490</th>\n",
       "      <td>omg beau brought his friend who is from san di...</td>\n",
       "      <td>and it doesn't help that he's drunk 💀💀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754496</th>\n",
       "      <td>thou have broughten the fermented oat elixir😂😂</td>\n",
       "      <td>i poureth some for the comrades</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12166 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Utterance                                           Response\n",
       "38      you think so ? a lot girls been fucking with i...                   idk how females fuck with this 😂\n",
       "44      4-0. i don't remember you ever winning. &amp; ...           😭😭😭 i thought you was gonna let me slide\n",
       "52      you think so ? a lot girls been fucking with i...                 gotta get that brad pitt from fury\n",
       "184                guess youre gon have to bite me hahaha  &lt;-- she volunteered so... yeah. haha. go fo...\n",
       "216     no you deadass said i'm getting a crouton for ...                                 this is funny af 😂\n",
       "...                                                   ...                                                ...\n",
       "754170  😂😂 ima hit the r2 button and hit her with that...             lmaoooo nooo not the coon cock lmaoooo\n",
       "754246                                  he felt so bad 😂💀                               lonzo ball too good!\n",
       "754300                         i know someone who'd cry..           lol not gonna lie i probably would cry 😂\n",
       "754490  omg beau brought his friend who is from san di...             and it doesn't help that he's drunk 💀💀\n",
       "754496     thou have broughten the fermented oat elixir😂😂                    i poureth some for the comrades\n",
       "\n",
       "[12166 rows x 2 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fns = list(file_dict.keys())\n",
    "fn = fns[0]\n",
    "print(fn)\n",
    "pd.set_option('display.column_space', 1000)\n",
    "df = file_dict[fn]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b488d",
   "metadata": {},
   "source": [
    "To look at complete tweets (change column space so the tweets don't get truncated(:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d1bf388e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Utterance\n",
      "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     you think so ? a lot girls been fucking with it just not the right ones lmfao, but i'm thinking its time for the new cut 😂\n",
      "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     4-0. i don't remember you ever winning. &amp; i took this one. you didn't \"let\" nothing. 😂\n",
      "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     you think so ? a lot girls been fucking with it just not the right ones lmfao, but i'm thinking its time for the new cut 😂\n",
      "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        guess youre gon have to bite me hahaha\n",
      "216                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                no you deadass said i'm getting a crouton for my room and i said tf a crouton? and you said you don't know what a crouton is?!\n",
      "262                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  the choke wasn't hard enough\n",
      "300                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    to dinner at the applebees\n",
      "310                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          i lowkey wanted to kill you but everytime we stopped to eat i got happy again 😂 especially when we stopped at magnum\n",
      "442                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   heck no! i have a free one coming my brutha\n",
      "456                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              wiz khalifa gave a fan a joint and his reaction is priceless 😂🙌🏽\n",
      "620                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               hillary clinton says \"let's make it happen\" when told \"hillary for prison!\" 😂😁😂\n",
      "672                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            oh look at you...😁😁😁 ok...sounds good. but the toss needs to be good or they'll need to take sips.\n",
      "742                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    exactly, not even financially stable but using your whole day to search for a relationship\n",
      "844                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       you forgot both seth and roman stand tall and the shield fists come together again lmao\n",
      "860                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       the most annoying thing about c++ might be how grossly it violates my expectation that header files don’t contain code.\n",
      "900                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ladies and gents, i present to you dsv's other twitter handle . she is in rare form.\n",
      "968                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 fuck you netflix i've been down for you since you delivered dvds in envelopes\n",
      "1158                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         hahaha 😂😂 i honestly don't blame those bankers honestly that's what wells fargo gets\n",
      "1232                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              i'd be that guy with the beer 😂\n",
      "1374                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          i have no words for that video....holy fucking shit\n"
     ]
    }
   ],
   "source": [
    "# First 20 only utterance column.\n",
    "df_str = df.iloc[:20][[\"Utterance\"]].to_string(col_space=1000)\n",
    "print(df_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73655c2a",
   "metadata": {},
   "source": [
    "Look at at one row one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "acff1fd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😂😂 ima hit the r2 button and hit her with that coom cock 😂'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[754170][\"Utterance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520470e",
   "metadata": {},
   "source": [
    "Same tweet response column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "12dbd042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lmaoooo nooo not the coon cock lmaoooo'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[754170][\"Response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d245f",
   "metadata": {},
   "source": [
    "#### Retrieving from file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a0fd4c",
   "metadata": {},
   "source": [
    "Every row in a tweet file has conversational pairs (2nd tweet is response to first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "85ba4560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start_face_with_tears_of_joy.csv'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "# For the file with face with tears of joy emojis\n",
    "# all at the start of one the tweets in the conversational pair.\n",
    "code,emoji_loc= 128514,\"start\"\n",
    "\n",
    "fn =  get_filename (code,emoji_loc)\n",
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "11f4d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fn,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e4494771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>give him some water 😭</td>\n",
       "      <td>😂😂😂😂 we should've gave him some. he would've b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>😂😂😂😂 i9milha boogie</td>\n",
       "      <td>iphone 7 w nba 2k17 oula full o*****m 😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>dad: u want me to pay for this?! me: ya...sorr...</td>\n",
       "      <td>😂i love it!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>not another soul!</td>\n",
       "      <td>😂😂😂😂 u always do that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>happy birthday dude!!!</td>\n",
       "      <td>😂😂😂thanks jflo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750824</th>\n",
       "      <td>😂😂😂😂 whoooo lmao</td>\n",
       "      <td>omg the one who they were like bff's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751786</th>\n",
       "      <td>😂😂 you are going to be up super early than. lol</td>\n",
       "      <td>ha i'll probably pack on sunday/monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752276</th>\n",
       "      <td>yo lemme get you in touch with some of my peop...</td>\n",
       "      <td>😂😂😂😂😂😂😂😂😂😂😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752948</th>\n",
       "      <td>😂😂😂 haha what u tell them</td>\n",
       "      <td>lmao i didn't even have time to tell them noth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754170</th>\n",
       "      <td>😂😂 ima hit the r2 button and hit her with that...</td>\n",
       "      <td>lmaoooo nooo not the coon cock lmaoooo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1068 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Utterance                                           Response\n",
       "1916                                give him some water 😭  😂😂😂😂 we should've gave him some. he would've b...\n",
       "2174                                  😂😂😂😂 i9milha boogie            iphone 7 w nba 2k17 oula full o*****m 😂\n",
       "2348    dad: u want me to pay for this?! me: ya...sorr...                                      😂i love it!!!\n",
       "2478                                    not another soul!                              😂😂😂😂 u always do that\n",
       "2498                               happy birthday dude!!!                                     😂😂😂thanks jflo\n",
       "...                                                   ...                                                ...\n",
       "750824                                   😂😂😂😂 whoooo lmao               omg the one who they were like bff's\n",
       "751786    😂😂 you are going to be up super early than. lol             ha i'll probably pack on sunday/monday\n",
       "752276  yo lemme get you in touch with some of my peop...                                        😂😂😂😂😂😂😂😂😂😂😂\n",
       "752948                          😂😂😂 haha what u tell them  lmao i didn't even have time to tell them noth...\n",
       "754170  😂😂 ima hit the r2 button and hit her with that...             lmaoooo nooo not the coon cock lmaoooo\n",
       "\n",
       "[1068 rows x 2 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899899e1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
