{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification:  Insults with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "MejilF82-rRZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV as gs\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.naive_bayes as nb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qildTjvw-rRb"
   },
   "source": [
    "## Loading and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28jZSTW_-rRb"
   },
   "source": [
    "Let's open the CSV file with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "MtGQlB1q-rRc"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "site = 'https://raw.githubusercontent.com/gawron/python-for-social-science/master/'\\\n",
    "'text_classification/'\n",
    "#site = 'https://gawron.sdsu.edu/python_for_ss/course_core/book_draft/_static/'\n",
    "df = pd.read_csv(os.path.join(site,\"troll.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3ncLUYx-rRc"
   },
   "source": [
    "Each row is a comment  taken from a blog or online forum. There are three columns: whether the comment is insulting (1) or not (0), the data, and the unicode-encoded contents of the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pFeNaW-m-rRd",
    "outputId": "0d5e5e36-697f-4fd8-ff00-ef8d164a3c35"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>\"you are both morons and that is never happening\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>\"You're all upset, defending this hipster band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult                                            Comment\n",
       "3942       1  \"you are both morons and that is never happening\"\n",
       "3943       0  \"Many toolbars include spell check, like Yahoo...\n",
       "3944       0  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...\n",
       "3945       0  \"How about Felix? He is sure turning into one ...\n",
       "3946       0  \"You're all upset, defending this hipster band..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Insult', 'Comment']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sEjygA0-rRe"
   },
   "source": [
    "Write a pandas command to create a DataFrame containing just the insults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "b8QDhTRJ-rRe"
   },
   "outputs": [],
   "source": [
    "# Solution replaces df on the RHS\n",
    "insult_df = df[df['Insult'] ==1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "9Mse4iGA-rRf",
    "outputId": "2d251430-ed4b-4b94-b242-9575a9147132",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"shut the fuck up. you and the rest of your fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502173553Z</td>\n",
       "      <td>\"Either you are fake or extremely stupid...may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620160512Z</td>\n",
       "      <td>\"That you are an idiot who understands neither...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>20120611090207Z</td>\n",
       "      <td>\"FOR SOME REASON U SOUND RETARDED. LOL. DAMN. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320162532Z</td>\n",
       "      <td>\"You with the 'racist' screen name\\n\\nYou are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320075347Z</td>\n",
       "      <td>\"your such a dickhead...\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320203947Z</td>\n",
       "      <td>\"Your a retard go post your head up your #%&amp;*\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>20120515132156Z</td>\n",
       "      <td>\"Allinit123, your\\xa0hypocrisy\\xa0is sickening...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620161958Z</td>\n",
       "      <td>\"I can't believe the stupid people on this sit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>20120609163743Z</td>\n",
       "      <td>\"fucking behave then you prick!\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620075045Z</td>\n",
       "      <td>\"Is that it??  your sum total to any conversat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619074710Z</td>\n",
       "      <td>\"faggot\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>20120529130844Z</td>\n",
       "      <td>\"Aimless Abe you are a delusion weirdo that sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320131007Z</td>\n",
       "      <td>\"Even as a troll you are a pathetic failure.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502173058Z</td>\n",
       "      <td>\"You're a moron, truth is beyond your reach\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619203151Z</td>\n",
       "      <td>\"Your anti-Semitic rants are not welcomed here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619033424Z</td>\n",
       "      <td>\"god, you're tiresome. get a life, you loser.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619194023Z</td>\n",
       "      <td>\"Fact : Georgia passed a strict immigration po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>20120530000452Z</td>\n",
       "      <td>\"Of course you would bottom feeder ...\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619162450Z</td>\n",
       "      <td>\"You are\\xa0 a fukin moron. \\xa0\\xa0 You are j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619171832Z</td>\n",
       "      <td>\"You are a fucking dumb ass!.  Go back to you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502203704Z</td>\n",
       "      <td>\"Lets see your papers arealconservati.\\n\\nTill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Correction Bitch! You don't think it's superb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>20120611215519Z</td>\n",
       "      <td>\"I think the only trickle that effected you wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Insult             Date                                            Comment\n",
       "0        1  20120618192155Z                               \"You fuck your dad.\"\n",
       "7        1              NaN  \"shut the fuck up. you and the rest of your fa...\n",
       "8        1  20120502173553Z  \"Either you are fake or extremely stupid...may...\n",
       "9        1  20120620160512Z  \"That you are an idiot who understands neither...\n",
       "15       1  20120611090207Z  \"FOR SOME REASON U SOUND RETARDED. LOL. DAMN. ...\n",
       "16       1  20120320162532Z  \"You with the 'racist' screen name\\n\\nYou are ...\n",
       "18       1  20120320075347Z                          \"your such a dickhead...\"\n",
       "19       1  20120320203947Z     \"Your a retard go post your head up your #%&*\"\n",
       "34       1  20120515132156Z  \"Allinit123, your\\xa0hypocrisy\\xa0is sickening...\n",
       "37       1  20120620161958Z  \"I can't believe the stupid people on this sit...\n",
       "38       1  20120609163743Z                   \"fucking behave then you prick!\"\n",
       "41       1  20120620075045Z  \"Is that it??  your sum total to any conversat...\n",
       "45       1  20120619074710Z                                           \"faggot\"\n",
       "47       1  20120529130844Z  \"Aimless Abe you are a delusion weirdo that sh...\n",
       "51       1  20120320131007Z      \"Even as a troll you are a pathetic failure.\"\n",
       "55       1  20120502173058Z       \"You're a moron, truth is beyond your reach\"\n",
       "59       1  20120619203151Z  \"Your anti-Semitic rants are not welcomed here...\n",
       "61       1  20120619033424Z     \"god, you're tiresome. get a life, you loser.\"\n",
       "79       1  20120619194023Z  \"Fact : Georgia passed a strict immigration po...\n",
       "80       1  20120530000452Z            \"Of course you would bottom feeder ...\"\n",
       "82       1  20120619162450Z  \"You are\\xa0 a fukin moron. \\xa0\\xa0 You are j...\n",
       "88       1  20120619171832Z  \"You are a fucking dumb ass!.  Go back to you ...\n",
       "93       1  20120502203704Z  \"Lets see your papers arealconservati.\\n\\nTill...\n",
       "95       1              NaN  \"Correction Bitch! You don't think it's superb...\n",
       "96       1  20120611215519Z  \"I think the only trickle that effected you wa..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insult_df[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(insult_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3947"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are documents of a **variety** of lengths, from various kinds of social media.  From pretty long..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "uPevaZIm-rRg",
    "outputId": "9b616c95-b275-4517-f6b9-55cfe5942318",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Fact : Georgia passed a strict immigration policy and most of the Latino farm workers left the area. Vidalia Georgia now has over 3000 agriculture job openings and they have been able to fill about 250 of them in past year. All you White Real Americans who are looking for work that the Latinos stole from you..Where are you ? The jobs are i Vadalia just waiting for you..Or maybe its the fact that you would rather collect unemployment like the rest of the Tea Klaners.. You scream..you complain..and you sit at home in your wife beaters and drink beer..Typical Real White Tea Klan....\"'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Comment'][79]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To very very short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_PkM5-Y-rRh",
    "outputId": "f5fa1949-6819-46c6-84e5-0cd7b48831f7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Insult                   1\n",
       "Date       20120620121441Z\n",
       "Comment           \"Retard\"\n",
       "Size                     8\n",
       "Name: 755, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insult_df.loc[755]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the range.  Some very long documents have to processed and\n",
    "correctly taggeda s insulting.  This is part of the challenge of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "(fig, ax) = plt.subplots(1,1,figsize=(8,8))\n",
    "\n",
    "bar = insult_df[\"Size\"].plot(kind=\"hist\",bins=[5, 10, 20, 30,40, 50,60, \n",
    "                                         70, 80, 90, 100, 120,140, 150], width=4,color=\"dodgerblue\",\n",
    "                          title=\"Length Distribution for Insults\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DeYXbqK-rRW"
   },
   "source": [
    "## Analyzing insults with Naive Bayes: sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jPBwaTr-rRi"
   },
   "source": [
    "We want to use one of the linear classifiers in `sklearn`,\n",
    "but the learners in `sklearn` only work with 2D numerical arrays. \n",
    "We next discuss how to convert text into an array  of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSvV0ToN-rRi"
   },
   "source": [
    "Given a collection of documents of size D and some class labels L,\n",
    "the classical solution can be described in several steps:\n",
    "\n",
    "1. extract a **vocabulary** of size V by scanning all the texts in . Then, we build a DxV **term-document matrix** `M`.\n",
    "2. For each document `d` and each word `w` fill cell `[d,w]` of `M` with a number representing the importance of word `w` in document `d`, in the simplest case, its count.  Thus one row of M represents one document in the collection. Since most documents contain only a small portion of the total vocabulary of the document collection, M is almost always **sparse**; that is, it consists almost entirely of zeros.\n",
    "3. Train a classifer on the term document matrix M and class labels L.\n",
    "\n",
    "The `sklearn` module allows us to\n",
    "complete steps 1 and 2 in a few lines of code using what is called a **vectorizer**.  Step\n",
    "3 is another few lines of code using a scikit learn  **classifier**. Although\n",
    "there are a few things to watch out for, the same classifiers that work for other classification\n",
    "problems generally work for text classification.\n",
    "\n",
    "We first encountered a scikit learn vectorizer in the regression and classification\n",
    "notebook in building an adjective classifier.  There, we were classifying words, not documents.\n",
    "and the features used for classification were letter sequences from 2 to 4 characters long;\n",
    "the feature values were counts of how many times a given letter sequence occurred in the word.\n",
    "The Count Vectorizer learned 78,696  features, so in\n",
    "the training data matrix, each word in the training data was rrepsented as\n",
    "a row (or **vector**) of 78, 696 integers, most of them zero.\n",
    " \n",
    "For example, the nonzero feature counts for the word \"alfalfa\" looked like this\n",
    "\n",
    "\n",
    "```python\n",
    "' a'  : 1\n",
    "' al' : 1\n",
    "' alf': 1\n",
    "'a '  : 1\n",
    "'al'  : 2\n",
    "'alf' : 2\n",
    "'alfa': 2\n",
    "'fal' : 1\n",
    "'fa'  : 2\n",
    "'fa ' : 1\n",
    "'falf': 1\n",
    "'lfa' : 2\n",
    "'lf'  : 2\n",
    "'lfa ': 1\n",
    "'lfal': 1\n",
    "```\n",
    "\n",
    "Based on these feature counts\n",
    "the classifier computed a probability that a word was an adjective.\n",
    "Although a vectorizer that measures the importance of a word\n",
    "by its count is always feasible, it doesn't work as well as you might think\n",
    "when the features are words.  Words counts are not very reliable \n",
    "indicators of the importance of a word because the most frequent\n",
    "words (*the*, *of*, *a*) tell us nothing about what a document is about.\n",
    "\n",
    "To represent the importance of a word in a document,  we will use a **TFIDF score**.\n",
    "Although there are a metric that has had success  in a number Natural Language Processing tasks; scikit_learn\n",
    "makes this `TfidfVectorizer`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rk9HaGLJ-rRk",
    "outputId": "9d85eaf7-398b-4b55-e6c4-16497634cd35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "\n",
      "    Equivalent to :class:`CountVectorizer` followed by\n",
      "    :class:`TfidfTransformer`.\n",
      "\n",
      "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    input : {'filename', 'file', 'content'}, default='content'\n",
      "        - If `'filename'`, the sequence passed as an argument to fit is\n",
      "          expected to be a list of filenames that need reading to fetch\n",
      "          the raw content to analyze.\n",
      "\n",
      "        - If `'file'`, the sequence items must have a 'read' method (file-like\n",
      "          object) that is called to fetch the bytes in memory.\n",
      "\n",
      "        - If `'content'`, the input is expected to be a sequence of items that\n",
      "          can be of type string or byte.\n",
      "\n",
      "    encoding : str, default='utf-8'\n",
      "        If bytes or files are given to analyze, this encoding is used to\n",
      "        decode.\n",
      "\n",
      "    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "        Instruction on what to do if a byte sequence is given to analyze that\n",
      "        contains characters not of the given `encoding`. By default, it is\n",
      "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "        values are 'ignore' and 'replace'.\n",
      "\n",
      "    strip_accents : {'ascii', 'unicode'}, default=None\n",
      "        Remove accents and perform other character normalization\n",
      "        during the preprocessing step.\n",
      "        'ascii' is a fast method that only works on characters that have\n",
      "        an direct ASCII mapping.\n",
      "        'unicode' is a slightly slower method that works on any characters.\n",
      "        None (default) does nothing.\n",
      "\n",
      "        Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "        :func:`unicodedata.normalize`.\n",
      "\n",
      "    lowercase : bool, default=True\n",
      "        Convert all characters to lowercase before tokenizing.\n",
      "\n",
      "    preprocessor : callable, default=None\n",
      "        Override the preprocessing (string transformation) stage while\n",
      "        preserving the tokenizing and n-grams generation steps.\n",
      "        Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "    tokenizer : callable, default=None\n",
      "        Override the string tokenization step while preserving the\n",
      "        preprocessing and n-grams generation steps.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      "        Whether the feature should be made of word or character n-grams.\n",
      "        Option 'char_wb' creates character n-grams only from text inside\n",
      "        word boundaries; n-grams at the edges of words are padded with space.\n",
      "\n",
      "        If a callable is passed it is used to extract the sequence of features\n",
      "        out of the raw, unprocessed input.\n",
      "\n",
      "        .. versionchanged:: 0.21\n",
      "            Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n",
      "            is first read from the file and then passed to the given callable\n",
      "            analyzer.\n",
      "\n",
      "    stop_words : {'english'}, list, default=None\n",
      "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
      "        list is returned. 'english' is currently the only supported string\n",
      "        value.\n",
      "        There are several known issues with 'english' and you should\n",
      "        consider an alternative (see :ref:`stop_words`).\n",
      "\n",
      "        If a list, that list is assumed to contain stop words, all of which\n",
      "        will be removed from the resulting tokens.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "        If None, no stop words will be used. max_df can be set to a value\n",
      "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
      "        words based on intra corpus document frequency of terms.\n",
      "\n",
      "    token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      "        Regular expression denoting what constitutes a \"token\", only used\n",
      "        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      "        or more alphanumeric characters (punctuation is completely ignored\n",
      "        and always treated as a token separator).\n",
      "\n",
      "        If there is a capturing group in token_pattern then the\n",
      "        captured group content, not the entire match, becomes the token.\n",
      "        At most one capturing group is permitted.\n",
      "\n",
      "    ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "        The lower and upper boundary of the range of n-values for different\n",
      "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      "        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
      "        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
      "        only bigrams.\n",
      "        Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "    max_df : float or int, default=1.0\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly higher than the given threshold (corpus-specific\n",
      "        stop words).\n",
      "        If float in range [0.0, 1.0], the parameter represents a proportion of\n",
      "        documents, integer absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    min_df : float or int, default=1\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly lower than the given threshold. This value is also\n",
      "        called cut-off in the literature.\n",
      "        If float in range of [0.0, 1.0], the parameter represents a proportion\n",
      "        of documents, integer absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    max_features : int, default=None\n",
      "        If not None, build a vocabulary that only consider the top\n",
      "        max_features ordered by term frequency across the corpus.\n",
      "\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    vocabulary : Mapping or iterable, default=None\n",
      "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "        indices in the feature matrix, or an iterable over terms. If not\n",
      "        given, a vocabulary is determined from the input documents.\n",
      "\n",
      "    binary : bool, default=False\n",
      "        If True, all non-zero term counts are set to 1. This does not mean\n",
      "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      "        is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
      "\n",
      "    dtype : dtype, default=float64\n",
      "        Type of the matrix returned by fit_transform() or transform().\n",
      "\n",
      "    norm : {'l1', 'l2'}, default='l2'\n",
      "        Each output row will have unit norm, either:\n",
      "\n",
      "        - 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "          similarity between two vectors is their dot product when l2 norm has\n",
      "          been applied.\n",
      "        - 'l1': Sum of absolute values of vector elements is 1.\n",
      "          See :func:`preprocessing.normalize`.\n",
      "\n",
      "    use_idf : bool, default=True\n",
      "        Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
      "\n",
      "    smooth_idf : bool, default=True\n",
      "        Smooth idf weights by adding one to document frequencies, as if an\n",
      "        extra document was seen containing every term in the collection\n",
      "        exactly once. Prevents zero divisions.\n",
      "\n",
      "    sublinear_tf : bool, default=False\n",
      "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    vocabulary_ : dict\n",
      "        A mapping of terms to feature indices.\n",
      "\n",
      "    fixed_vocabulary_ : bool\n",
      "        True if a fixed vocabulary of term to indices mapping\n",
      "        is provided by the user.\n",
      "\n",
      "    idf_ : array of shape (n_features,)\n",
      "        The inverse document frequency (IDF) vector; only defined\n",
      "        if ``use_idf`` is True.\n",
      "\n",
      "    stop_words_ : set\n",
      "        Terms that were ignored because they either:\n",
      "\n",
      "          - occurred in too many documents (`max_df`)\n",
      "          - occurred in too few documents (`min_df`)\n",
      "          - were cut off by feature selection (`max_features`).\n",
      "\n",
      "        This is only available if no vocabulary was given.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      "\n",
      "    TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      "        matrix of counts.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The ``stop_words_`` attribute can get large and increase the model size\n",
      "    when pickling. This attribute is provided only for introspection and can\n",
      "    be safely removed using delattr or set to None before pickling.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    >>> corpus = [\n",
      "    ...     'This is the first document.',\n",
      "    ...     'This document is the second document.',\n",
      "    ...     'And this is the third one.',\n",
      "    ...     'Is this the first document?',\n",
      "    ... ]\n",
      "    >>> vectorizer = TfidfVectorizer()\n",
      "    >>> X = vectorizer.fit_transform(corpus)\n",
      "    >>> vectorizer.get_feature_names_out()\n",
      "    array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
      "           'this'], ...)\n",
      "    >>> print(X.shape)\n",
      "    (4, 9)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(text.TfidfVectorizer.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this idea out on a very small data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "        'This is the first document.',\n",
    "         'This document is the second document.',\n",
    "         'And this is the third one.',\n",
    "         'Is this the first document?',\n",
    "    ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feats = vectorizer.get_feature_names_out()\n",
    "print(len(feats))\n",
    "print(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the vocabulary size is 9, so assuming this is  all the training data,\n",
    "the document vectors of any future documents will be represented as vectors\n",
    "of length 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the output of `.fit_transform(...)` , a 4 x 9  term document matrix,\n",
    "where 4 is the number of documents and 9 is the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document and word the numerical value is a TFIDF score indicating the weigh/importance\n",
    "of that word in that document.  In many cases that importance is 0,\n",
    "indicating that word did not occur in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Explaining TFIDF\n",
    "\n",
    "Here are some statistics from the **British National Corpus**:\n",
    "\n",
    "```\n",
    "BNC\n",
    "\n",
    "Corpus size   51,994,153\n",
    " Vocab size      511,928\n",
    "   Num docs        1,726\n",
    "```\n",
    "\n",
    "And here are some interesting cases where word frequency\n",
    "is close and document frequency  isn't:\n",
    "\n",
    "```\n",
    "social                                             18,419          1,083         \n",
    "want                                               18,284          1,415         \n",
    "\n",
    "allow                                               5,285          1,232* \n",
    "computer                                            5,262            715\n",
    "treatment                                           5,250            906 \n",
    "gives                                               5,258          1,191*\n",
    "easily                                              5,218          1,212*\n",
    "```\n",
    "\n",
    "What we're seeing is that certain words are **burstier** than others.  Once\n",
    "they occur once in a document, they are much more likely to occur\n",
    "again in that same document than you'd expect given their frequency.  Take, for example,\n",
    "*computer*.  Once you see this word, it's likely\n",
    "that the document it occurs in has something to say about\n",
    "some technical or computer-related topic, and\n",
    "so the chances of seeing the word again are high.\n",
    "On the other hand, consider the word *gives*, whose overall\n",
    "frequency is nearly the same as *computer*.  This word\n",
    "doesn't tell you nearly as much about the topic of the document\n",
    "we're looking at, and the chances of seeing it again in the same document\n",
    "are neither higher nor lower than you'd expect for\n",
    "a word of that frequency: *computer* is bursty (it's 5K occurences are distributed\n",
    "over relatively few documents); *gives* is not.\n",
    "\n",
    "The TFIDF statistic takes into account not just the relative frequency of a word\n",
    "in a document (the **Term Frequency**). It also takes into account its burstiness.  Burstiness is measured by **Inverse Document Frequency**.\n",
    "\n",
    "The term frequency of a term in a document is just its **relative frequency** (frequency\n",
    "divided by document size).  That depends on both the word and the document \n",
    "\n",
    "\n",
    "$$\n",
    "(1) \\; \\text{tf}(t,d) = \\frac{f_{t,d}}{\\mid d \\mid}\n",
    "$$\n",
    "\n",
    "\n",
    "The inverse document frquency of a term $t$ in a set of documents D\n",
    "is the inverse of its relative frequency in D:\n",
    "\n",
    "$$\n",
    "(2) \\; \\text{idf}(t, D) = \\frac{\\mid D \\mid}{\\mid\\lbrace d \\mid d \\in \\text{D} \\text{ and } t \\in d  \\rbrace\\mid}\n",
    "$$\n",
    "\n",
    "This metric depends on the  word and document **set**.\n",
    "\n",
    "$$\n",
    "\\begin{array}[t]{ll}\n",
    "\\text{D}   & \\text{the set of  documents in the training data}\\\\\n",
    "\\mid\\text{D}\\mid   & \\text{ the number of docs in D}\\\\\n",
    "t          & \\text{the term or word}\\\\\n",
    "\\mid\\lbrace d \\mid d \\in \\text{D} \\text{ and } t \\in d  \\rbrace\\mid &\n",
    "\\text{the number of documents } t \\text{ occurs in }\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Often, instead of using IDF, what's used is the logarithm of IDF:\n",
    "\n",
    "$$\n",
    "(3) \\; \\text{log-idf}(t,\\text{D})  = \\log (\\text{idf}(t, \\text{D}))\n",
    "$$\n",
    "\n",
    "The expression $\\log \\text{idf}(t, D)$ is $- \\log \\text{prob}_{D}(t)$,\n",
    "which in information theory is the amount of the information\n",
    "gained by knowing $t$ occurs in a document in the corpus.  So TFIDF\n",
    "weights the term frequency by the information value of the term.\n",
    "\n",
    "\n",
    "A very popular version of TFIDF is the product of \n",
    "the log inverse document frequency and the term count.\n",
    "\n",
    "$$\n",
    "(4)\\; \\text{TFIDF}(t,d)  = \\text{tf}(t,d)  \\cdot \\text{log-idf}(t, D)\n",
    "$$\n",
    "\n",
    "Just weight the term frequency of $t$ in $d$ by the information value of $t$.\n",
    "\n",
    "The version of TFIDF used in scikit learn (also popoular) is esentially the product of \n",
    "the log inverse document frequency and the term count.\n",
    "\n",
    "$$\n",
    "(5)\\; \\text{TFIDF}(t,d)  = f_{t,d} \\cdot \\text{log-idf}(t, D)\n",
    "$$\n",
    "\n",
    "The raw term frequency is often used rather than the relative frequency\n",
    "because the document vectors are going to be normalized to\n",
    "unit length, so the document size will  be taken into\n",
    "account, but in a slightly different way.\n",
    "\n",
    "\n",
    "There are some technical details of the scikit learn implementation\n",
    "discussed [here](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "left out of equation (5), but that's the essential idea.\n",
    "\n",
    "\n",
    "Let's finish with an example.  Suppose we have a document in which\n",
    "the word *given* and the word *computer* both occur 3 times.  Let's\n",
    "use equation (5) and the statistics above to compute the 2 TFIDF scores:\n",
    "\n",
    "$$\n",
    "\\begin{array}[t]{ll}\n",
    "(a)\\; \\text{TFIDF}(\\text{computer},d)  &= \n",
    "\\begin{array}[t]{l}\n",
    "3 \\cdot \\text{log-idf}(\\text{computer}, D)\\\\\n",
    "\\log \\frac{1726}{715}\\\\\n",
    "2.64\\\\\n",
    "\\end{array}\\\\\n",
    "(b)\\; \\text{TFIDF}(\\text{gives},d)  & = \n",
    "\\begin{array}[t]{l}\n",
    "3 \\cdot \\text{log-idf}(\\text{gives}, D)\\\\\n",
    "\\log \\frac{1726}{1191}\\\\\n",
    "1.11\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "So as desired the occurrence of *computer* is more significant, in fact more than twice\n",
    "as significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working through the insult data\n",
    "\n",
    "Having talked through what scikit learn is going to do,  let's get back to\n",
    "the insult data and demonstrate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "H-fPVjYV-rRl"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets FIRST\n",
    "T_train,T_test, y_train,y_test = train_test_split(df['Comment'],df['Insult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "H-fPVjYV-rRl"
   },
   "outputs": [],
   "source": [
    "tf = text.TfidfVectorizer()\n",
    "# Train your vectorizer oNLY on the trainingh data.\n",
    "X_train = tf.fit_transform(T_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X-train` is our **term-document** matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUihdpHIgA7e",
    "outputId": "1ee65f02-31a4-4e83-b0f5-4e684bf4100d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2960, 13638)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of columns is over 10,000.  That means our vcabulary size is over 10,000.  The actual vocabulary size of the data is a little larger, because not all of the word in the training set are being used as features.  We'll skip over the details of how those decisions are made.  For now, let's get tp the main idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important computational fact about the term-document matrix is its sparseness,\n",
    "the fact that it consists mostly of 0s, because,\n",
    "for any given document, most of the words in the 10,000 word vocabulary don't occur in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4p8xlL7b-rRl",
    "outputId": "26e3f550-7a2c-4ef4-eddb-e2a9e47826f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2,960 x 13,787)  Non-zero entries: 75,034\n"
     ]
    }
   ],
   "source": [
    "# Shape and Number of non zero entries\n",
    "print(f'Shape: ({X_train.shape[0]:,} x {X_train.shape[1]:,})  Non-zero entries: {X_train.nnz:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfUbNDjt-rRm"
   },
   "source": [
    "Let's estimate the sparsity of this feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7o8Oa6JP-rRm",
    "outputId": "4778d0d4-0c30-4fa6-ef76-47f7beca151f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document matrix X is ~0.18% non-zero features.\n"
     ]
    }
   ],
   "source": [
    "print((\"The document matrix X is ~{0:.2%} non-zero features.\".format(\n",
    "          X_train.nnz / float(X_train.shape[0] * X_train.shape[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2960x13787 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 75034 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21HEUruW-rRm"
   },
   "source": [
    "The `sklearn` module stores many of its internally computed arrays as **sparse matrices**.  This is basically a \n",
    "very clever computer science device for not wasting all the space that very sparse matrices \n",
    "waste.  Natural language representations are often **quite** sparse.  The .15% non zero features\n",
    "firgure we just looked at was typical.  Sparse matrices come at a cost, however; although some\n",
    "computations can be done while the matrix is in sparse form, some cannot, and to do those\n",
    "you have to convert the matrix to a nonsparse matrix, do what you need to do, and then, probably,\n",
    "convert it back.  This is costly.  We're going to do it now, but only because we're goofing\n",
    "around. Conversion to non-sparse format should in general be avoided whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "s501JX4y-rRn"
   },
   "outputs": [],
   "source": [
    "# Sparse matrix rep => ordinary 2D numpy array/\n",
    "XA = X_train.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4dXO87k-rRm"
   },
   "source": [
    "The number of the column that represents a word in the term document matrix is called its **encoding**.\n",
    "\n",
    "A `TdidfVectorizer` instance stores its encoding dictionary in the attribute `vocabulary_` (note\n",
    "the trailing underscore!).\n",
    "\n",
    "Let's consider the insult word *moron*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4DfZgRC-rRm",
    "outputId": "862caf13-840b-44b9-986c-be5609bad1dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7295"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moron_ind = tf.vocabulary_['moron']\n",
    "moron_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlqPV8sk-rRn"
   },
   "source": [
    "Let's find a comment that contains 'moron' and remember its\n",
    "positional index in the training data so we can look up that doc in X_train.\n",
    "\n",
    "To make it more random, let's pick the fourth document containing \"moron\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Who is the real idiot, Dogtwon or me ? You have to pick one you know. Everybody can't be the stupidest person you talk to unless you talk to yourself. You asked for\\xa0this\\xa0with your lovely comments on how stupid everyone is that didn't vote for Ron Paul.What did he wind u with ? Last place\\xa0and about 3 percent of the vote ? So 97 % of the other primary voters are all stupid morons and you are enlightened ?\"\n"
     ]
    }
   ],
   "source": [
    "ctr=0\n",
    "for (i,comment) in enumerate(T_train):\n",
    "    if 'moron' in comment and i == 99:\n",
    "        ctr += 1\n",
    "        if ctr == 1:\n",
    "            break\n",
    "\n",
    "moron_comment = i\n",
    "doc_i = T_train.iloc[moron_comment]\n",
    "print(doc_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83msbB2F-rRn"
   },
   "source": [
    "Ok, now we can check the TFIDF matrix for the statistic for `'moron'` in this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkOJ8G5l-rRn",
    "outputId": "e9b75a5f-7bce-4ce0-e94f-b32da7258e1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XA[moron_comment][moron_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwJGxnst-rRn"
   },
   "source": [
    "Wait!  That didn't work! That zero means the word *moron* doesn't occur in this document.\n",
    "\n",
    "If you go back and look carefully at line 3 in this document, the word is actually *morons*, not *moron*.\n",
    "\n",
    "Since our test for \"moron\"-documents was to use `in` directly in the document string,\n",
    "it didn't distinguish occurrences of \"moron\" from occurrences of \"morons\";\n",
    "\"moron\" is a substring of both:\n",
    "\n",
    "```python\n",
    ">>> \"moron\" in \"you morons!\"\n",
    "True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmLm4OTm-rRo"
   },
   "source": [
    "And of course \"morons\" is a totally different word from *moron*, found at a totally different place in `XA`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObsyKu1n-rRo",
    "outputId": "9a59e053-8e78-43c8-dd32-cd9b75e715ee"
   },
   "outputs": [],
   "source": [
    "new_moron_ind = tf.vocabulary_['morons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKxndHoW-rRo",
    "outputId": "02d97065-602e-4035-f051-c48d075df8c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14533295914427363"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XA[moron_comment][new_moron_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moral is that the first preprocessing step in the vectorization process is to break\n",
    "a document string into a sequence of words.  This step is called **tokenization** and\n",
    "it's a little more sophisticated than calling `.split()` on the document string,\n",
    "but the result is similar. The bottom line is that unless you do something \n",
    "special to change things, the basic units of analysis for a document in text classification\n",
    "are going to be words, not subsequences of characters as they were in our adjective example.\n",
    "\n",
    "Summary: In this part of the discussion, we have learned about **vectorization**, the computational\n",
    "process of going from a sequence of documents to a term-document matrix.\n",
    "\n",
    "The key point is that the term document matrix is now exactly the sort of thing we used to\n",
    "train classifier to recoignize iris types: a matrix whose rows reoresent exemplars\n",
    "and whose columns represent features.  That mean we can just pass the the\n",
    "term document matrix X_train (along with some labels) to a classifier instance to\n",
    "train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0wp6RbS-rRo"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88mG_1C0-rRo"
   },
   "source": [
    "Now, we are going to train a classifier as usual. We \n",
    "have already split the data and labels into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf-3XQDN-rRo"
   },
   "source": [
    "We use a **Bernoulli Naive Bayes classifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "uj7vvTS--rRo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb =nb.BernoulliNB()\n",
    "\n",
    "bnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done.  How'd we do?  Now we  test on the test set.  Before we can do that we need to\n",
    "vectorize the test set.  But don't just copy what we did with the training data:\n",
    "\n",
    "```\n",
    "X_test = tf.fit_transform(T_test)\n",
    "```\n",
    "\n",
    "That would retrain the vectorizer from scratch.  Any words that occurred in the training texts\n",
    "but not in the test texts would be forgotten!  Plus training the vectorizer \n",
    "is part of the classifier training pipeline.  If we let the vectorizer see\n",
    "the test data during its training phase, we'd be compromising the whole\n",
    "idea of splitting training and test data.  So what we want to do\n",
    "with the test data is just apply the transform part of vectorizing:\n",
    "\n",
    "```\n",
    "X_test = tf.transform(T_test)\n",
    "```\n",
    "\n",
    "That is, build a representation of the test data using only the vocabulary you learned\n",
    "about in training.  Ignore any new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rewbP2vT-rRp",
    "outputId": "ccbb8785-df2f-4000-8095-8de2e57c6f3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7588652482269503"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = tf.transform(T_test)\n",
    "bnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing everything up till now\n",
    "\n",
    "Let's summarize what we did by gathering the steps into one cell without all the discussion and re-executing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7933130699088146"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_train,T_test, y_train,y_test = train_test_split(df['Comment'],df['Insult'])\n",
    "tf = text.TfidfVectorizer()\n",
    "X_train = tf.fit_transform(T_train)\n",
    "bnb =nb.BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "X_test = tf.transform(T_test)\n",
    "bnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p38bf79W-rRp"
   },
   "source": [
    "The result should be the same as when we stepped through it with lots of discussion, right?\n",
    "\n",
    "Well, is it?  \n",
    "\n",
    "Ok, re-execute the same cell above again.  Now one more time. \n",
    "\n",
    "Now try the following\n",
    "piece of code, which wraps our classification pipeline into \n",
    "a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-bt36GeMpFW"
   },
   "source": [
    "#### Basic train and test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vectorize_and_fit(docs,labels,clf):\n",
    "    \"\"\"\n",
    "    Given labeled data (docs, labels) and a classifier,\n",
    "    do the training test split.  Train the vectorizer and the classifier.\n",
    "    Transform the test data and return a set of preducted labels\n",
    "    for the test data,\n",
    "    \"\"\"\n",
    "    T_train,T_test, y_train,y_test = train_test_split(docs,labels)\n",
    "    tf = text.TfidfVectorizer()\n",
    "    X_train = tf.fit_transform(T_train)\n",
    "    clf_inst = clf()\n",
    "    clf_inst.fit(X_train, y_train)\n",
    "    X_test = tf.transform(T_test)\n",
    "    return clf_inst.predict(X_test), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwsayH6F-rRp",
    "outputId": "a9f16658-90d0-4899-cd1e-3148ada56515",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.753\n",
      "0.757\n",
      "0.762\n",
      "0.779\n",
      "0.750\n",
      "0.771\n",
      "0.751\n",
      "0.764\n",
      "0.784\n",
      "0.769\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "for test_run in range(num_runs):\n",
    "    predicted, actual = split_vectorize_and_fit(df['Comment'],df['Insult'], nb.BernoulliNB)\n",
    "    print('{0:.3f}'.format(accuracy_score(predicted, actual)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdxesQwr-rRp"
   },
   "source": [
    "What's happening?  \n",
    "\n",
    "The training test split function takes a random sample of all the data to use as training data.\n",
    "Each time there's a train test split we get a different classifier.  Sometimes the\n",
    "training data is a better preparation for the test than others.   And so the actual\n",
    "variation in performance is significant.\n",
    "\n",
    "How should we deal this with this when we report our evaluations?\n",
    "To get a realistic picture of how good our classifier is,\n",
    "we need to take the average of multiple training runs, each with a different train/test split of our working\n",
    "data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2SjpCsKMvfh"
   },
   "source": [
    "### Refined train and test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdxesQwr-rRp"
   },
   "source": [
    "Explain the purpose of the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3HVmOUb-rRp",
    "outputId": "7c2ac905-1389-4ff3-ae0a-0d448536bce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.76\n",
      "Precision 0.14\n",
      "Recall 0.89\n",
      "Pct Insults 0.27\n"
     ]
    }
   ],
   "source": [
    "num_runs = 100\n",
    "\n",
    "stats = np.zeros((4,))\n",
    "for test_run in range(num_runs):\n",
    "    predicted, actual = split_vectorize_and_fit(df['Comment'],df['Insult'],nb.BernoulliNB)\n",
    "    y_array = actual.values\n",
    "    prop_insults = y_array.sum()/len(y_array)\n",
    "    stats = stats + np.array([accuracy_score(predicted, actual),\n",
    "                              precision_score(predicted, actual),\n",
    "                              recall_score(predicted, actual),\n",
    "                              prop_insults])\n",
    "normed_stats = stats/num_runs\n",
    "labels = ['Accuracy','Precision','Recall','Pct Insults']\n",
    "for (i,s) in enumerate(normed_stats):\n",
    "    print(f'{labels[i]} {s:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we look at what features\n",
    "are the most important in insult detection.\n",
    "Despite the title of this notebook, we're not going to use\n",
    "Naive Bayes; we're going to a different\n",
    "classifier, because it works a little better for this\n",
    "task.\n",
    "\n",
    "For this experiment, we leave out the training test split;\n",
    "in fact, we leave out anything to do with testing.\n",
    "\n",
    "Our goal is to get a slightly better understanding of what it means to find a linear separator\n",
    "in a classification setting.  We started our tour of classifiers by looking at \n",
    "iris classificstion, a simple problem with 4 features.  Now in insult detection\n",
    "we have over 16,000 features.  Mathematically the only change is that\n",
    "we seek a separator in 16,000 dimensions instead of 4.\n",
    "\n",
    "But what does that that really mean?\n",
    "What it boils down is that we are looking for an assignment of weights \n",
    "to all our features such that a linear combination of the weighted feature values\n",
    "does the best job we can at classifying our data.  And in this\n",
    "setting, whereb our features are words, that means that with the \n",
    "right kind of classifier, and the right implementation, we\n",
    "can ask the classifier what features got the most wieght.\n",
    "In  the case of classifying insults, that mean we can ask what\n",
    "words were the most important in classifying something as an insult.\n",
    "\n",
    "In the cell below, we give a very simple function for doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: mouth asshole faggot bitch stupid you loser moron dumb idiot\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm as svm\n",
    "\n",
    "\n",
    "def print_topn(vectorizer, clf, top_n=10, class_labels=(True,)):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        ## Look at the feature weights the classifier learned for class i.\n",
    "        ## ArgSort the weights (what feature indexes have the highest weights)\n",
    "        word_importance = np.argsort(clf.coef_[i])\n",
    "        ## Get the topn\n",
    "        top_inds = word_importance[-top_n:]\n",
    "        ## What the words for the topn features\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \" \".join(feature_names[j] for j in top_inds)))\n",
    "\n",
    "\n",
    "tf = text.TfidfVectorizer()\n",
    "X_train = tf.fit_transform(df['Comment'])\n",
    "est = svm.LinearSVC()\n",
    "est.fit(X_train, df['Insult'])\n",
    "# Now find the most heavily weighted features [= words]\n",
    "print_topn(tf,est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvxe-F1n-rRp"
   },
   "source": [
    "\n",
    "We found the words with the top 10 weights and printed them out.\n",
    "\n",
    "They are indeed nasty insulting words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the classifier on a list of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0th-hPo-rRq"
   },
   "source": [
    "Finally, let's look at how to test our estimator on a few test sentences.\n",
    "\n",
    "Even sentences that we've made up.\n",
    "\n",
    "Simply be sure to transform your dat before you pass it to the predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "9PkNvp0m-rRq",
    "outputId": "7a675e1b-5ac0-4a7c-fad6-94346973bf87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n"
     ]
    }
   ],
   "source": [
    "predicted = est.predict(tf.transform([\n",
    "    \"I totally agree with you\",\n",
    "    \"You are so stupid\",\n",
    "    \"That you are an idiot who understands neither taxation nor women\\'s health.\"\n",
    "    ]))\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoYRjj7n-rRr"
   },
   "source": [
    "Not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Insults_with_Naive_Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "94px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
