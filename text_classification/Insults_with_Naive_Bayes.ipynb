{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification:  Insults with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "MejilF82-rRZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV as gs\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.naive_bayes as nb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qildTjvw-rRb"
   },
   "source": [
    "## Loading and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28jZSTW_-rRb"
   },
   "source": [
    "Let's open the CSV file with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MtGQlB1q-rRc"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "site = 'https://raw.githubusercontent.com/gawron/python-for-social-science/master/'\\\n",
    "'text_classification/'\n",
    "#site = 'https://gawron.sdsu.edu/python_for_ss/course_core/book_draft/_static/'\n",
    "df = pd.read_csv(os.path.join(site,\"troll.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3ncLUYx-rRc"
   },
   "source": [
    "Each row is a comment  taken from a blog or online forum. There are three columns: whether the comment is insulting (1) or not (0), the data, and the unicode-encoded contents of the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pFeNaW-m-rRd",
    "outputId": "0d5e5e36-697f-4fd8-ff00-ef8d164a3c35"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>\"you are both morons and that is never happening\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>\"You're all upset, defending this hipster band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult                                            Comment\n",
       "3942       1  \"you are both morons and that is never happening\"\n",
       "3943       0  \"Many toolbars include spell check, like Yahoo...\n",
       "3944       0  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...\n",
       "3945       0  \"How about Felix? He is sure turning into one ...\n",
       "3946       0  \"You're all upset, defending this hipster band..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Insult', 'Comment']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sEjygA0-rRe"
   },
   "source": [
    "Write a pandas command to give you just the insults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "b8QDhTRJ-rRe"
   },
   "outputs": [],
   "source": [
    "# Solution replaces df on the RHS\n",
    "insult_df = df[df['Insult'] ==1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "9Mse4iGA-rRf",
    "outputId": "2d251430-ed4b-4b94-b242-9575a9147132",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"shut the fuck up. you and the rest of your fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502173553Z</td>\n",
       "      <td>\"Either you are fake or extremely stupid...may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620160512Z</td>\n",
       "      <td>\"That you are an idiot who understands neither...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>20120611090207Z</td>\n",
       "      <td>\"FOR SOME REASON U SOUND RETARDED. LOL. DAMN. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320162532Z</td>\n",
       "      <td>\"You with the 'racist' screen name\\n\\nYou are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320075347Z</td>\n",
       "      <td>\"your such a dickhead...\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320203947Z</td>\n",
       "      <td>\"Your a retard go post your head up your #%&amp;*\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>20120515132156Z</td>\n",
       "      <td>\"Allinit123, your\\xa0hypocrisy\\xa0is sickening...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620161958Z</td>\n",
       "      <td>\"I can't believe the stupid people on this sit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>20120609163743Z</td>\n",
       "      <td>\"fucking behave then you prick!\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620075045Z</td>\n",
       "      <td>\"Is that it??  your sum total to any conversat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619074710Z</td>\n",
       "      <td>\"faggot\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>20120529130844Z</td>\n",
       "      <td>\"Aimless Abe you are a delusion weirdo that sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320131007Z</td>\n",
       "      <td>\"Even as a troll you are a pathetic failure.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502173058Z</td>\n",
       "      <td>\"You're a moron, truth is beyond your reach\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619203151Z</td>\n",
       "      <td>\"Your anti-Semitic rants are not welcomed here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619033424Z</td>\n",
       "      <td>\"god, you're tiresome. get a life, you loser.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619194023Z</td>\n",
       "      <td>\"Fact : Georgia passed a strict immigration po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>20120530000452Z</td>\n",
       "      <td>\"Of course you would bottom feeder ...\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619162450Z</td>\n",
       "      <td>\"You are\\xa0 a fukin moron. \\xa0\\xa0 You are j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619171832Z</td>\n",
       "      <td>\"You are a fucking dumb ass!.  Go back to you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502203704Z</td>\n",
       "      <td>\"Lets see your papers arealconservati.\\n\\nTill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Correction Bitch! You don't think it's superb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>20120611215519Z</td>\n",
       "      <td>\"I think the only trickle that effected you wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Insult             Date                                            Comment\n",
       "0        1  20120618192155Z                               \"You fuck your dad.\"\n",
       "7        1              NaN  \"shut the fuck up. you and the rest of your fa...\n",
       "8        1  20120502173553Z  \"Either you are fake or extremely stupid...may...\n",
       "9        1  20120620160512Z  \"That you are an idiot who understands neither...\n",
       "15       1  20120611090207Z  \"FOR SOME REASON U SOUND RETARDED. LOL. DAMN. ...\n",
       "16       1  20120320162532Z  \"You with the 'racist' screen name\\n\\nYou are ...\n",
       "18       1  20120320075347Z                          \"your such a dickhead...\"\n",
       "19       1  20120320203947Z     \"Your a retard go post your head up your #%&*\"\n",
       "34       1  20120515132156Z  \"Allinit123, your\\xa0hypocrisy\\xa0is sickening...\n",
       "37       1  20120620161958Z  \"I can't believe the stupid people on this sit...\n",
       "38       1  20120609163743Z                   \"fucking behave then you prick!\"\n",
       "41       1  20120620075045Z  \"Is that it??  your sum total to any conversat...\n",
       "45       1  20120619074710Z                                           \"faggot\"\n",
       "47       1  20120529130844Z  \"Aimless Abe you are a delusion weirdo that sh...\n",
       "51       1  20120320131007Z      \"Even as a troll you are a pathetic failure.\"\n",
       "55       1  20120502173058Z       \"You're a moron, truth is beyond your reach\"\n",
       "59       1  20120619203151Z  \"Your anti-Semitic rants are not welcomed here...\n",
       "61       1  20120619033424Z     \"god, you're tiresome. get a life, you loser.\"\n",
       "79       1  20120619194023Z  \"Fact : Georgia passed a strict immigration po...\n",
       "80       1  20120530000452Z            \"Of course you would bottom feeder ...\"\n",
       "82       1  20120619162450Z  \"You are\\xa0 a fukin moron. \\xa0\\xa0 You are j...\n",
       "88       1  20120619171832Z  \"You are a fucking dumb ass!.  Go back to you ...\n",
       "93       1  20120502203704Z  \"Lets see your papers arealconservati.\\n\\nTill...\n",
       "95       1              NaN  \"Correction Bitch! You don't think it's superb...\n",
       "96       1  20120611215519Z  \"I think the only trickle that effected you wa..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insult_df[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are documents of a **variety** of lengths, from various kinds of social media.  From pretty long..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "uPevaZIm-rRg",
    "outputId": "9b616c95-b275-4517-f6b9-55cfe5942318",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Fact : Georgia passed a strict immigration policy and most of the Latino farm workers left the area. Vidalia Georgia now has over 3000 agriculture job openings and they have been able to fill about 250 of them in past year. All you White Real Americans who are looking for work that the Latinos stole from you..Where are you ? The jobs are i Vadalia just waiting for you..Or maybe its the fact that you would rather collect unemployment like the rest of the Tea Klaners.. You scream..you complain..and you sit at home in your wife beaters and drink beer..Typical Real White Tea Klan....\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Comment'][79]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To very very short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_PkM5-Y-rRh",
    "outputId": "f5fa1949-6819-46c6-84e5-0cd7b48831f7",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Insult                   1\n",
       "Date       20120620121441Z\n",
       "Comment           \"Retard\"\n",
       "Size                     8\n",
       "Name: 755, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insult_df.loc[755]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the range.  This is part of the challenge of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AWuiuBO-rRg",
    "outputId": "5234c1f8-0c0b-439b-fcbf-5a86cb1af6dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3208    4016\n",
       "3931    1600\n",
       "581     1548\n",
       "1348    1269\n",
       "3924    1022\n",
       "        ... \n",
       "3109      11\n",
       "2180      11\n",
       "3919       8\n",
       "45         8\n",
       "755        8\n",
       "Name: Size, Length: 1049, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insult_df['Size'] = df['Comment'].apply(len)\n",
    "insult_df['Size'].sort_values(ascending = False)[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DeYXbqK-rRW"
   },
   "source": [
    "## Analyzing insults with Naive Bayes: pandas and sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jPBwaTr-rRi"
   },
   "source": [
    "We want to use one of the linear classifiers in `sklearn`,\n",
    "but the learners in `sklearn` only work with numerical arrays. How to convert text into a matrix of numbers?\n",
    "Obtaining the feature matrix from the text is not trivial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSvV0ToN-rRi"
   },
   "source": [
    "The classical solution is to first extract a **vocabulary**: a list of words used throughout the corpus. Then, we can count, for each document in the sample, the frequency of each word. We end up with a **sparse matrix**: a huge matrix containing mostly zeros. Here, `sklearn` and `pandas` make it possible to do this in two lines. \n",
    "\n",
    "The text processing component that goes from text to sparse matrix is called a **vectorizer**.\n",
    "\n",
    "We will use a `TfidfVectorizer`, a version that has been very successul in various Natural Language Processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rk9HaGLJ-rRk",
    "outputId": "9d85eaf7-398b-4b55-e6c4-16497634cd35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "\n",
      "    Equivalent to :class:`CountVectorizer` followed by\n",
      "    :class:`TfidfTransformer`.\n",
      "\n",
      "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    input : {'filename', 'file', 'content'}, default='content'\n",
      "        - If `'filename'`, the sequence passed as an argument to fit is\n",
      "          expected to be a list of filenames that need reading to fetch\n",
      "          the raw content to analyze.\n",
      "\n",
      "        - If `'file'`, the sequence items must have a 'read' method (file-like\n",
      "          object) that is called to fetch the bytes in memory.\n",
      "\n",
      "        - If `'content'`, the input is expected to be a sequence of items that\n",
      "          can be of type string or byte.\n",
      "\n",
      "    encoding : str, default='utf-8'\n",
      "        If bytes or files are given to analyze, this encoding is used to\n",
      "        decode.\n",
      "\n",
      "    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "        Instruction on what to do if a byte sequence is given to analyze that\n",
      "        contains characters not of the given `encoding`. By default, it is\n",
      "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "        values are 'ignore' and 'replace'.\n",
      "\n",
      "    strip_accents : {'ascii', 'unicode'}, default=None\n",
      "        Remove accents and perform other character normalization\n",
      "        during the preprocessing step.\n",
      "        'ascii' is a fast method that only works on characters that have\n",
      "        an direct ASCII mapping.\n",
      "        'unicode' is a slightly slower method that works on any characters.\n",
      "        None (default) does nothing.\n",
      "\n",
      "        Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "        :func:`unicodedata.normalize`.\n",
      "\n",
      "    lowercase : bool, default=True\n",
      "        Convert all characters to lowercase before tokenizing.\n",
      "\n",
      "    preprocessor : callable, default=None\n",
      "        Override the preprocessing (string transformation) stage while\n",
      "        preserving the tokenizing and n-grams generation steps.\n",
      "        Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "    tokenizer : callable, default=None\n",
      "        Override the string tokenization step while preserving the\n",
      "        preprocessing and n-grams generation steps.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      "        Whether the feature should be made of word or character n-grams.\n",
      "        Option 'char_wb' creates character n-grams only from text inside\n",
      "        word boundaries; n-grams at the edges of words are padded with space.\n",
      "\n",
      "        If a callable is passed it is used to extract the sequence of features\n",
      "        out of the raw, unprocessed input.\n",
      "\n",
      "        .. versionchanged:: 0.21\n",
      "            Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n",
      "            is first read from the file and then passed to the given callable\n",
      "            analyzer.\n",
      "\n",
      "    stop_words : {'english'}, list, default=None\n",
      "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
      "        list is returned. 'english' is currently the only supported string\n",
      "        value.\n",
      "        There are several known issues with 'english' and you should\n",
      "        consider an alternative (see :ref:`stop_words`).\n",
      "\n",
      "        If a list, that list is assumed to contain stop words, all of which\n",
      "        will be removed from the resulting tokens.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "        If None, no stop words will be used. max_df can be set to a value\n",
      "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
      "        words based on intra corpus document frequency of terms.\n",
      "\n",
      "    token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      "        Regular expression denoting what constitutes a \"token\", only used\n",
      "        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      "        or more alphanumeric characters (punctuation is completely ignored\n",
      "        and always treated as a token separator).\n",
      "\n",
      "        If there is a capturing group in token_pattern then the\n",
      "        captured group content, not the entire match, becomes the token.\n",
      "        At most one capturing group is permitted.\n",
      "\n",
      "    ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "        The lower and upper boundary of the range of n-values for different\n",
      "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      "        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
      "        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
      "        only bigrams.\n",
      "        Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "    max_df : float or int, default=1.0\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly higher than the given threshold (corpus-specific\n",
      "        stop words).\n",
      "        If float in range [0.0, 1.0], the parameter represents a proportion of\n",
      "        documents, integer absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    min_df : float or int, default=1\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly lower than the given threshold. This value is also\n",
      "        called cut-off in the literature.\n",
      "        If float in range of [0.0, 1.0], the parameter represents a proportion\n",
      "        of documents, integer absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    max_features : int, default=None\n",
      "        If not None, build a vocabulary that only consider the top\n",
      "        max_features ordered by term frequency across the corpus.\n",
      "\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    vocabulary : Mapping or iterable, default=None\n",
      "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "        indices in the feature matrix, or an iterable over terms. If not\n",
      "        given, a vocabulary is determined from the input documents.\n",
      "\n",
      "    binary : bool, default=False\n",
      "        If True, all non-zero term counts are set to 1. This does not mean\n",
      "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      "        is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
      "\n",
      "    dtype : dtype, default=float64\n",
      "        Type of the matrix returned by fit_transform() or transform().\n",
      "\n",
      "    norm : {'l1', 'l2'}, default='l2'\n",
      "        Each output row will have unit norm, either:\n",
      "\n",
      "        - 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "          similarity between two vectors is their dot product when l2 norm has\n",
      "          been applied.\n",
      "        - 'l1': Sum of absolute values of vector elements is 1.\n",
      "          See :func:`preprocessing.normalize`.\n",
      "\n",
      "    use_idf : bool, default=True\n",
      "        Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
      "\n",
      "    smooth_idf : bool, default=True\n",
      "        Smooth idf weights by adding one to document frequencies, as if an\n",
      "        extra document was seen containing every term in the collection\n",
      "        exactly once. Prevents zero divisions.\n",
      "\n",
      "    sublinear_tf : bool, default=False\n",
      "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    vocabulary_ : dict\n",
      "        A mapping of terms to feature indices.\n",
      "\n",
      "    fixed_vocabulary_ : bool\n",
      "        True if a fixed vocabulary of term to indices mapping\n",
      "        is provided by the user.\n",
      "\n",
      "    idf_ : array of shape (n_features,)\n",
      "        The inverse document frequency (IDF) vector; only defined\n",
      "        if ``use_idf`` is True.\n",
      "\n",
      "    stop_words_ : set\n",
      "        Terms that were ignored because they either:\n",
      "\n",
      "          - occurred in too many documents (`max_df`)\n",
      "          - occurred in too few documents (`min_df`)\n",
      "          - were cut off by feature selection (`max_features`).\n",
      "\n",
      "        This is only available if no vocabulary was given.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      "\n",
      "    TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      "        matrix of counts.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The ``stop_words_`` attribute can get large and increase the model size\n",
      "    when pickling. This attribute is provided only for introspection and can\n",
      "    be safely removed using delattr or set to None before pickling.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    >>> corpus = [\n",
      "    ...     'This is the first document.',\n",
      "    ...     'This document is the second document.',\n",
      "    ...     'And this is the third one.',\n",
      "    ...     'Is this the first document?',\n",
      "    ... ]\n",
      "    >>> vectorizer = TfidfVectorizer()\n",
      "    >>> X = vectorizer.fit_transform(corpus)\n",
      "    >>> vectorizer.get_feature_names_out()\n",
      "    array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
      "           'this'], ...)\n",
      "    >>> print(X.shape)\n",
      "    (4, 9)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(text.TfidfVectorizer.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "        'This is the first document.',\n",
    "         'This document is the second document.',\n",
    "         'And this is the third one.',\n",
    "         'Is this the first document?',\n",
    "    ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'the', 'first', 'document.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Explaining TFIDF\n",
    "\n",
    "Here are some statistics from the **British National Corpus**:\n",
    "\n",
    "```\n",
    "BNC\n",
    "\n",
    "Corpus size   51,994,153\n",
    " Vocab size      511,928\n",
    "   Num docs        1,726\n",
    "```\n",
    "\n",
    "And here are some interesting cases where word frequency\n",
    "is close and doc frequency  isn't\"\n",
    "\n",
    "```\n",
    "social                                             18,419          1,083         \n",
    "want                                               18,284          1,415         \n",
    "\n",
    "allow                                               5,285          1,232* \n",
    "computer                                            5,262            715\n",
    "treatment                                           5,250            906 \n",
    "gives                                               5,258          1,191*\n",
    "easily                                              5,218          1,212*\n",
    "```\n",
    "\n",
    "What we're seeing is that certain words are **clumpier** than others,\n",
    "in fact, clumpier than you'd expect given their frequency.  Once\n",
    "they occur once in a document, they are much more likely to occur\n",
    "again in that same document than you'd expect given their frequency.  Take, for example,\n",
    "*computer*.  Once you see this word, it's likely\n",
    "that the document it occurs in deals with\n",
    "some technical or computer-related topic, and\n",
    "so chances of seeing the word again are high.\n",
    "On the other hand, take *gives*, whose overall\n",
    "frequency is nearly the same as *computer*.  This word\n",
    "doesn't tell you nearly as much about the topic of the document\n",
    "we're looking at, and the chances of seeing it again in the same document\n",
    "are neither higher nor lower than you'd expect for\n",
    "a word of that frequency: *computer* is clumpy (it's 5K occurences are distributed\n",
    "over relatively few documents); *gives* is not.\n",
    "\n",
    "The TFIDF statistic takes into account not just the relative frequency of a word\n",
    "in a document (the **Term Frequency**). It also takes into account its clumpiness.  \n",
    "Clumpiness is measured by **Inverse Document Frequency**.\n",
    "\n",
    "The term frequency of a term in a document is just its **relative frequency** (frequency\n",
    "divided by document size)\n",
    "\n",
    "\n",
    "$$\n",
    "(1) \\; \\text{tf}(t,d) = \\frac{f_{t,d}}{\\mid d \\mid}\n",
    "$$\n",
    "\n",
    "\n",
    "The inverse document frquency of a term $t$ in a set of documents D\n",
    "is the inverse of its relative frequency in D:\n",
    "\n",
    "$$\n",
    "(2) \\; \\text{idf}(t, D) = \\frac{\\mid D \\mid}{\\mid\\lbrace d \\mid d \\in \\text{D} \\text{ and } t \\in d  \\rbrace\\mid}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}[t]{ll}\n",
    "\\text{D}   & \\text{the set of  documents in the training data}\\\\\n",
    "\\mid\\text{D}\\mid   & \\text{ the number of docs in D}\\\\\n",
    "t          & \\text{the term or word}\\\\\n",
    "\\mid\\lbrace d \\mid d \\in \\text{D} \\text{ and } t \\in d  \\rbrace\\mid &\n",
    "\\text{the number of documents } t \\text{ occurs in }\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "An important refinement is \n",
    "\n",
    "$$\n",
    "(3) \\; \\text{log-idf}(t,\\text{D})  = \\log (\\text{idf}(t, \\text{D}))\n",
    "$$\n",
    "\n",
    "The expression $\\log \\text{idf}(t, D)$ is $- \\log \\text{prob}_{D}(t)$,\n",
    "which in information theory is the amount of the information\n",
    "gained by knowing $t$ occurs in a document in the corpus.  So TFIDF\n",
    "weights the term frequency by the information value of the term.\n",
    "\n",
    "\n",
    "A very popular version of TFIDF is the product of \n",
    "the log inverse document frequency and the term count.\n",
    "\n",
    "$$\n",
    "(4)\\; \\text{TFIDF}(t,d)  = \\text{tf}(t,d)  \\cdot \\text{log-idf}(t, D)\n",
    "$$\n",
    "\n",
    "Just weight the term frequency of $t$ in $d$ by the information value of $t$.\n",
    "\n",
    "Another popular version of TFIDF is the product of \n",
    "the log inverse document frequency and the term count.\n",
    "\n",
    "$$\n",
    "(5)\\; \\text{TFIDF}(t,d)  = f_{t,d} \\cdot \\text{log-idf}(t, D)\n",
    "$$\n",
    "\n",
    "The raw term frequency is often used rather than the relative frequency\n",
    "because the document vectors are going to be normalized to\n",
    "unit length, so the document size will  be taken into\n",
    "account, but in a slightly different way.\n",
    "\n",
    "\n",
    "Equation (5) \n",
    "is essentially what scikit learn uses, although there are some technical details \n",
    "discussed [here](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "being left out.\n",
    "\n",
    "\n",
    "Let's finish with an example.  Suppose we have a document in which\n",
    "the word *given* and the word *computer* both occur 3 times.  Let's\n",
    "use equation (5) and the statistics above to compute the 2 IDFs,\n",
    "\n",
    "$$\n",
    "(a)\\; \\text{TFIDF}(\\text{computer},d)  = \n",
    "\\begin{array}[t]{l}\n",
    "3 \\cdot \\text{log-idf}(\\text{computer}, D)\\\\\n",
    "\\log \\frac{1726}{715}\\\\\n",
    "2.64\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(a)\\; \\text{TFIDF}(\\text{gives},d)  = \n",
    "\\begin{array}[t]{l}\n",
    "3 \\cdot \\text{log-idf}(\\text{gives}, D)\\\\\n",
    "\\log \\frac{1726}{1191}\\\\\n",
    "1.11\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "So as desired the occurrence of *computer* is more significant, in fact more than twice\n",
    "as significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1130399068642194"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*np.log(1726/1191)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how we would use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "H-fPVjYV-rRl"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets FIRST\n",
    "T_train,T_test, y_train,y_test = train_test_split(df['Comment'],df['Insult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "H-fPVjYV-rRl"
   },
   "outputs": [],
   "source": [
    "tf = text.TfidfVectorizer()\n",
    "# Train your vectorizer oNLY on the trainingh data.\n",
    "X_train = tf.fit_transform(T_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X-train` is a **term-document** matrix.  Each row represents a document.  Each column represents a **term** or vocabulary item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUihdpHIgA7e",
    "outputId": "1ee65f02-31a4-4e83-b0f5-4e684bf4100d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2960, 13538)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of columns is over 10,000.  That means there  are over 10,000 features in our representation of every document.  Most but not all of the word in the training set are being used as features.  We'll skip\n",
    "over the details of how those decisions are made.  For now, let;s get the main idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKgEXAZV-rRl"
   },
   "source": [
    "\n",
    "The TFIDF vectorizer uses a simple formula to assign a significance score --- called **TFIDF value** --- to the\n",
    "count of each vocabulary item in each document. Our term document matrix  `X_train`\n",
    "contains those TFIDF values.\n",
    "\n",
    "Say the word \"moron\" occurs 3 times in a document.\n",
    "TFIDF is a very popular measure of the significance of that fact\n",
    "first proven to be useful in\n",
    "document retrieval.  It has some competitors in classification, but\n",
    "we have used it here mainly because it's the easiest **feature weighting scheme**\n",
    "to use in `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the most important fact about the term-document matrix is that it consists mostly of 0s, because,\n",
    "for any given document, most of the words in the vocabulary don't occur in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4p8xlL7b-rRl",
    "outputId": "26e3f550-7a2c-4ef4-eddb-e2a9e47826f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2,960 x 13,538)  Non-zero entries: 73,754\n"
     ]
    }
   ],
   "source": [
    "# Shape and Number of non zero entries\n",
    "print(f'Shape: ({X_train.shape[0]:,} x {X_train.shape[1]:,})  Non-zero entries: {X_train.nnz:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfUbNDjt-rRm"
   },
   "source": [
    "Let's estimate the sparsity of this feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7o8Oa6JP-rRm",
    "outputId": "4778d0d4-0c30-4fa6-ef76-47f7beca151f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document matrix X is ~0.18% non-zero features.\n"
     ]
    }
   ],
   "source": [
    "print((\"The document matrix X is ~{0:.2%} non-zero features.\".format(\n",
    "          X_train.nnz / float(X_train.shape[0] * X_train.shape[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2960x13538 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 73754 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4dXO87k-rRm"
   },
   "source": [
    "So each word in the vocabulary has to be associated with a column number.  That;s called its **encoding**.\n",
    "\n",
    "A `TdidfVectorizer` instance stores its encoding dictionary in the attribute `vocabulary_` (note\n",
    "the trailing underscore!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4DfZgRC-rRm",
    "outputId": "862caf13-840b-44b9-986c-be5609bad1dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7129"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moron_ind = tf.vocabulary_['moron']\n",
    "moron_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21HEUruW-rRm"
   },
   "source": [
    "The `sklearn` module stores many of its internally computed arrays as **sparse matrices**.  This is basically a \n",
    "very clever computer science device for not wasting all the space that very sparse matrices \n",
    "waste.  Natural language representations are often **quite** sparse.  The .15% non zero features\n",
    "firgure we just looked at was typical.  Sparse matrices come at a cost, however; although some\n",
    "computations can be done while the matrix is in sparse form, some cannot, and to do those\n",
    "you have to convert the matrix to a nonsparse matrix, do what you need to do, and then, probably,\n",
    "convert it back.  This is costly.  We're going to do it now, but only because we're goofing\n",
    "around. Conversion to non-sparse format should in general be avoided whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "s501JX4y-rRn"
   },
   "outputs": [],
   "source": [
    "XA = X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"You fuck your dad.\"'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2789228014509435"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XA[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlqPV8sk-rRn"
   },
   "source": [
    "Let's find a comment that contains 'moron' and remember its\n",
    "positional index in the training data so we can look up that doc in X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"santorum is the only real conservative running.\\\\xc2\\\\xa0 \\\\n\\\\nAs to the occupiers..... what a bunch of selfish, spoiled brats who are morons. What have these occupiers accomplished / anything of real value.... let's see, murder, rape, robbery, vandalism, created a huge financial burden for the local taxpayers(clean up their messes) even child neglect/endangerment as been reported...\\\\xc2\\\\xa0 great accomplishment !\\\\xc2\\\\xa0 Mom must be so very proud. Lost all credibility.\"\n"
     ]
    }
   ],
   "source": [
    "for (i,comment) in enumerate(T_train):\n",
    "    if 'morons' in comment:\n",
    "        break\n",
    "\n",
    "moron_comment = i\n",
    "print(T_train.iloc[moron_comment])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83msbB2F-rRn"
   },
   "source": [
    "Ok, now we can check the TFIDF matrix for the statistic for `'moron'` in this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkOJ8G5l-rRn",
    "outputId": "e9b75a5f-7bce-4ce0-e94f-b32da7258e1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XA[moron_comment][moron_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwJGxnst-rRn"
   },
   "source": [
    "Oh, maybe we didn't learn that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObsyKu1n-rRo",
    "outputId": "9a59e053-8e78-43c8-dd32-cd9b75e715ee"
   },
   "outputs": [],
   "source": [
    "moron_ind = tf.vocabulary_['morons']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmLm4OTm-rRo"
   },
   "source": [
    "Totally different word, found at a totally different place in XA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKxndHoW-rRo",
    "outputId": "02d97065-602e-4035-f051-c48d075df8c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12517565622441323"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XA[moron_comment][moron_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: In this part of the discussion, we have learned about **vectorization**, the computational\n",
    "procdess of going from a sequence of documents to a term-document matrix.\n",
    "\n",
    "The key point is that the term document matrix is now exactly the sort of thing we used to\n",
    "train classifier to recoignize iris types: a matrix whose rows reoresent exemplars\n",
    "and whose columns reoresents features.  That mean we can just pass the the\n",
    "term document matrix X_train (aloing with some labels) to a classifier instance to\n",
    "train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0wp6RbS-rRo"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88mG_1C0-rRo"
   },
   "source": [
    "Now, we are going to train a classifier as usual. We first split the data into a train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf-3XQDN-rRo"
   },
   "source": [
    "We use a **Bernoulli Naive Bayes classifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "uj7vvTS--rRo"
   },
   "outputs": [],
   "source": [
    "bnb =nb.BernoulliNB()\n",
    "\n",
    "bnb.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done.  How'd we do?  Now we  test on the test set.  Before we can do that we need to\n",
    "vectorize the test set.  But don't just copy what we did with the training data:\n",
    "\n",
    "```\n",
    "X_test = tf.fit_transform(T_test)\n",
    "```\n",
    "\n",
    "That would retrain the vectorizer from scratch.  Any words that occurred in the training texts\n",
    "but not in the test texts would be forgotten!  Plus training the vectorizer \n",
    "is part of the classifier training pipeline.  If we let the vectorizer see\n",
    "the test data, we'd be compromising the whole\n",
    "idea of splitting training and test data.  So what we want to do\n",
    "with the test data is just apply the transform part of vectorizing:\n",
    "\n",
    "```\n",
    "X_test = tf.transform(T_test)\n",
    "```\n",
    "\n",
    "That is, build a representation of the test data using only the vocabulary you learned\n",
    "about in training.  Ignore any new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rewbP2vT-rRp",
    "outputId": "ccbb8785-df2f-4000-8095-8de2e57c6f3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7416413373860182"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = tf.transform(T_test)\n",
    "bnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize what we did by gathering the steps into one cell without all the discussion and re-executing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7608915906788247"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_train,T_test, y_train,y_test = train_test_split(df['Comment'],df['Insult'])\n",
    "tf = text.TfidfVectorizer()\n",
    "X_train = tf.fit_transform(T_train)\n",
    "bnb =nb.BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "X_test = tf.transform(T_test)\n",
    "bnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p38bf79W-rRp"
   },
   "source": [
    "The result should be the same as when we stepped through it with lots of discussion, right?\n",
    "\n",
    "Well, is it?  \n",
    "\n",
    "Ok, re-execute the same cell above again.  Now one more time. \n",
    "\n",
    "Now try the following\n",
    "piece of code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-bt36GeMpFW"
   },
   "source": [
    "#### Basic train and test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vectorize_and_fit(docs,labels,clf):\n",
    "    T_train,T_test, y_train,y_test = train_test_split(docs,labels)\n",
    "    tf = text.TfidfVectorizer()\n",
    "    X_train = tf.fit_transform(T_train)\n",
    "    clf_inst = clf()\n",
    "    clf_inst.fit(X_train, y_train)\n",
    "    X_test = tf.transform(T_test)\n",
    "    return clf_inst.predict(X_test), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwsayH6F-rRp",
    "outputId": "a9f16658-90d0-4899-cd1e-3148ada56515",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.753\n",
      "0.769\n",
      "0.746\n",
      "0.784\n",
      "0.772\n",
      "0.757\n",
      "0.780\n",
      "0.763\n",
      "0.791\n",
      "0.773\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "for test_run in range(num_runs):\n",
    "    predicted, actual = split_vectorize_and_fit(df['Comment'],df['Insult'], nb.BernoulliNB)\n",
    "    print('{0:.3f}'.format(accuracy_score(predicted, actual)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdxesQwr-rRp"
   },
   "source": [
    "What's happening?  \n",
    "\n",
    "The training test split function takes a random sample of all the data to use as training data.\n",
    "Each time there's a train test split we get a different classifier.  Sometimes the\n",
    "training data is a better preparation for the test than others.   And so the actual\n",
    "variation in performance is significant.\n",
    "\n",
    "How should we deal this with this when we report our evaluations?\n",
    "To get a realistic picture of how good our classifier is,\n",
    "we need to take the average of multiple training runs, each with a different train/test split of our working\n",
    "data set. This is called **cross validation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2SjpCsKMvfh"
   },
   "source": [
    "### Refined train and test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdxesQwr-rRp"
   },
   "source": [
    "Explain the purpose of the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3HVmOUb-rRp",
    "outputId": "7c2ac905-1389-4ff3-ae0a-0d448536bce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.77\n",
      "Precision 0.14\n",
      "Recall 0.90\n",
      "Pct Insults 0.27\n"
     ]
    }
   ],
   "source": [
    "num_runs = 100\n",
    "\n",
    "stats = np.zeros((4,))\n",
    "for test_run in range(num_runs):\n",
    "    predicted, actual = split_vectorize_and_fit(df['Comment'],df['Insult'],nb.BernoulliNB)\n",
    "    y_array = actual.values\n",
    "    prop_insults = y_array.sum()/len(y_array)\n",
    "    stats = stats + np.array([accuracy_score(predicted, actual),\n",
    "                              precision_score(predicted, actual),\n",
    "                              recall_score(predicted, actual),\n",
    "                              prop_insults])\n",
    "normed_stats = stats/num_runs\n",
    "labels = ['Accuracy','Precision','Recall','Pct Insults']\n",
    "for (i,s) in enumerate(normed_stats):\n",
    "    print(f'{labels[i]} {s:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's back to the core code sequence and take a look at what features\n",
    "are the most important in insult detection.\n",
    "\n",
    "For this experiment we leave out the training test split;\n",
    "in fact, we leave out anything to do with testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BernoulliNB' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w9/bx4mylnd27g_kqqgn5hrn2x40000gr/T/ipykernel_76962/2460594321.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Now find the most heavily weighted features [= words]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint_topn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/w9/bx4mylnd27g_kqqgn5hrn2x40000gr/T/ipykernel_76962/2460594321.py\u001b[0m in \u001b[0;36mprint_topn\u001b[0;34m(vectorizer, clf, top_n, class_labels)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mword_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtop_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_importance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         print(\"%s: %s\" % (class_label,\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BernoulliNB' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "def print_topn(vectorizer, clf, top_n=10, class_labels=(True,)):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        word_importance = np.argsort(clf.coef_[i])\n",
    "        top_inds = word_importance[-top_n:]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \" \".join(feature_names[j] for j in top_inds)))\n",
    "\n",
    "\n",
    "tf = text.TfidfVectorizer()\n",
    "X_train = tf.fit_transform(df['Comment'])\n",
    "bnb =nb.BernoulliNB()\n",
    "bnb.fit(X_train, df['Insult'])\n",
    "\n",
    "# Now find the most heavily weighted features [= words]\n",
    "print_topn(tf,bnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvxe-F1n-rRp"
   },
   "source": [
    "The model essentially consists of a set of weights attached to vocab items and stored in\n",
    "\n",
    "```\n",
    "bnb.coef_\n",
    "```\n",
    "We found the words with the top 10 weights and printed them out.\n",
    "So the  top 10 best predictors in this model of being an insult is a bunch of function words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some more words.  We some some more natural insult candidates showing up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "sH4qNRcD-rRq",
    "outputId": "b4ad438e-854c-488f-95ca-32e93f270f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: need still should her than right by dick nyou make were shut has time some see loser them ignorant too there even more say think from life then why now he moron at would when one off shit really yourself how we will out they my people was back fucking dumb here little me bitch because who about ass know if or but as no can stupid do don go fuck this up get what be idiot with all xa0 so an just re not for on have like in it is that of and to the your are you\n"
     ]
    }
   ],
   "source": [
    "print_topn(tf,bnb,top_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the classifier on a list of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0th-hPo-rRq"
   },
   "source": [
    "Finally, let's look at how to test our estimator on a few test sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = bnb.predict(tf.transform(df['Comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "9PkNvp0m-rRq",
    "outputId": "7a675e1b-5ac0-4a7c-fad6-94346973bf87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "predicted = bnb.predict(tf.transform([\n",
    "    \"I totally agree with you\",\n",
    "    \"You are so stupid\",\n",
    "    \"That you are an idiot who understands neither taxation nor women\\'s health.\"\n",
    "    ]))\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoYRjj7n-rRr"
   },
   "source": [
    "Not real impressive.  The word *stupid* was not recognized as an insult.\n",
    "\n",
    "Naive Bayes is not the best classifier.  On your homework assignment you will try some others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Accuracy, and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUNTZQ5kVlqu"
   },
   "source": [
    "The next cell takes the first step toward testing a classifier a little more seriously.  It defines some code for evaluating classifier output.  The evaluation metrics defined are precision, recall, and accuracy.  Call the examples the system predicts to be positive (whether correctly or not) ppos and and the examples it predicts to be negative pneg; consider the following performance on 100 examples:\n",
    "\n",
    "$$\\begin{array}[t]{ccc} &  pos &neg\\\\ ppos& 31 & 5\\\\ pneg & 14  & 50 \\end{array}$$\n",
    "\n",
    "The performance of the system has been sorted into 4 classes:\n",
    "\n",
    "$$\\begin{array}[t]{ccc} &  pos &neg\\\\ ppos& tp & fp\\\\ pneg & fn & tn \\end{array}$$\n",
    "\n",
    "The $tp$ and $tn$ examples (true positive and true negative) are those the system labeled correctly,\n",
    "while $fp$ and $fn$ (false positive and false negative) are those labeled incorrectly.\n",
    "Let N stand for the total number of examples,\n",
    "100 in our case. \n",
    "\n",
    "The three most important measures of system performance are:\n",
    "  \n",
    "  1. **Accuracy**: Accuracy is the percentage of correct examples out of the total corpus \n",
    "\n",
    "  $$Acc = \\frac{tp+tn}{N} = \\frac{31 +50}{100}$$ \n",
    "  \n",
    "  This is .81 in our case.\n",
    "\n",
    "  2. **Precision**: Precision is the percentage of true positives out of all positive guesses the system made \n",
    "  \n",
    "  $$Prec= \\frac{tp}{tp + fp} = \\frac{31}{31+5}.$$\n",
    "  \n",
    "  This is .86 in our case.\n",
    "  3. **Recall**: Recall is the percentage of true positives out of all positives \n",
    "  \n",
    "  $$Rec = \\frac{tp}{tp + fn} = \\frac{31}{31+14}.$$\n",
    "  \n",
    "  This is .69 in our case.\n",
    "\n",
    "\n",
    "  The function `do_evaluation`, defined in the next cell, computes precision, recall and accuracy for a test set\n",
    "  using the scikit learn implementations of those metrics; `do_evaluation` takes as its argument a sequence of docs and labels, as well as a classifier creation function.\n",
    "  \n",
    "It also takes as an argument `pos_label`, the label we are trying to predict.  Changing the\n",
    "label we are trying to \"detect\" (True or False for our insult detection data) has no effect\n",
    "on accuracy but it computes different scores for precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_evaluation(clf, docs, labels, num_runs=100, pos_label=True):\n",
    "    stats = np.zeros((4,))\n",
    "    for test_run in range(num_runs):\n",
    "        predicted, actual = split_vectorize_and_fit(df['Comment'],df['Insult'],clf=clf)\n",
    "        y_array = actual.values\n",
    "        prop_insults = y_array.sum()/len(y_array)\n",
    "        stats = stats + np.array([accuracy_score(predicted, actual),\n",
    "                                  precision_score(predicted, actual,pos_label=pos_label),\n",
    "                                  recall_score(predicted, actual,pos_label=pos_label),\n",
    "                                  prop_insults])\n",
    "    normed_stats = stats/num_runs\n",
    "    labels = ['Accuracy','Precision','Recall','Pct Insults']\n",
    "    for (i,s) in enumerate(normed_stats):\n",
    "        print(f'{labels[i]} {s:.2f}')\n",
    "    return normed_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkEWXFnJVlqv"
   },
   "source": [
    "The code in the next cell evaluates our NB classifier.  Note that precision and recall give different results depending on which  class we think of ourselves as detecting (which class we think of as positive).  We give evaluation numbers with respect to detecting insults and detecting non insults.  These show that our classifier \n",
    "often calls something an insult when it isn't (low precision in detecting insults)\n",
    "but therefore misses a lot of non insults (not so great recall in detecting non insults)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate our ability to detect insults\n",
      "\n",
      "Accuracy 0.77\n",
      "Precision 0.14\n",
      "Recall 0.87\n",
      "Pct Insults 0.26\n",
      "\n",
      "Now evaluate our ability to detect NON insults\n",
      "\n",
      "Accuracy 0.77\n",
      "Precision 0.99\n",
      "Recall 0.77\n",
      "Pct Insults 0.26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.77092199, 0.99379989, 0.76514169, 0.26443769])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Evaluate our ability to detect insults')\n",
    "print()\n",
    "do_evaluation(nb.BernoulliNB, df['Comment'],df['Insult'],num_runs=10,pos_label=True)\n",
    "print()\n",
    "print('Now evaluate our ability to detect NON insults')\n",
    "print()\n",
    "do_evaluation(nb.BernoulliNB, df['Comment'],df['Insult'],num_runs=10,pos_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkEWXFnJVlqv"
   },
   "source": [
    "So why does our classifier guess insult more often than it should, given that only about a quarter of the data is insults?  Well, probably because it had more success finding strong positive indicators than it did finding strong negative indicators, as our glance at the most informative features suggested.  This is something we might want to worry about as we design good classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Insults_with_Naive_Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "94px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
