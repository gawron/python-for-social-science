{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhtQXWFRrWSd"
   },
   "source": [
    "# Text Classification:  Insults with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "MejilF82-rRZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV as gs\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.naive_bayes as nb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qildTjvw-rRb"
   },
   "source": [
    "## Loading and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28jZSTW_-rRb"
   },
   "source": [
    "Let's open the CSV file with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "MtGQlB1q-rRc"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "site = 'https://raw.githubusercontent.com/gawron/python-for-social-science/master/'\\\n",
    "'text_classification/'\n",
    "#site = 'https://gawron.sdsu.edu/python_for_ss/course_core/book_draft/_static/'\n",
    "df = pd.read_csv(os.path.join(site,\"troll.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3ncLUYx-rRc"
   },
   "source": [
    "Each row is a comment  taken from a blog or online forum. There are three columns: whether the comment is insulting (1) or not (0), the data, and the unicode-encoded contents of the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pFeNaW-m-rRd",
    "outputId": "0d5e5e36-697f-4fd8-ff00-ef8d164a3c35"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>\"you are both morons and that is never happening\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>\"You're all upset, defending this hipster band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult                                            Comment\n",
       "3942       1  \"you are both morons and that is never happening\"\n",
       "3943       0  \"Many toolbars include spell check, like Yahoo...\n",
       "3944       0  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...\n",
       "3945       0  \"How about Felix? He is sure turning into one ...\n",
       "3946       0  \"You're all upset, defending this hipster band..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Insult', 'Comment']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sEjygA0-rRe"
   },
   "source": [
    "Write a pandas command to create a DataFrame containing just the insults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvWxdURsrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pw5i9M7rWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjQStC3OrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_aomjzyrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmKZH__WrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGGbYGMBrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aGmTFjkrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPGo41zzrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zBy8z2YrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTJ-iIZHrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npLKZaqkrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwmLLJcUrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "b8QDhTRJ-rRe"
   },
   "outputs": [],
   "source": [
    "# Solution replaces df on the RHS\n",
    "insult_df = df[df['Insult'] ==1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "9Mse4iGA-rRf",
    "outputId": "2d251430-ed4b-4b94-b242-9575a9147132",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"shut the fuck up. you and the rest of your fa...</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502173553Z</td>\n",
       "      <td>\"Either you are fake or extremely stupid...may...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620160512Z</td>\n",
       "      <td>\"That you are an idiot who understands neither...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>20120611090207Z</td>\n",
       "      <td>\"FOR SOME REASON U SOUND RETARDED. LOL. DAMN. ...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320162532Z</td>\n",
       "      <td>\"You with the 'racist' screen name\\n\\nYou are ...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320075347Z</td>\n",
       "      <td>\"your such a dickhead...\"</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320203947Z</td>\n",
       "      <td>\"Your a retard go post your head up your #%&amp;*\"</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>20120515132156Z</td>\n",
       "      <td>\"Allinit123, your\\xa0hypocrisy\\xa0is sickening...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620161958Z</td>\n",
       "      <td>\"I can't believe the stupid people on this sit...</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>20120609163743Z</td>\n",
       "      <td>\"fucking behave then you prick!\"</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620075045Z</td>\n",
       "      <td>\"Is that it??  your sum total to any conversat...</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619074710Z</td>\n",
       "      <td>\"faggot\"</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>20120529130844Z</td>\n",
       "      <td>\"Aimless Abe you are a delusion weirdo that sh...</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320131007Z</td>\n",
       "      <td>\"Even as a troll you are a pathetic failure.\"</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502173058Z</td>\n",
       "      <td>\"You're a moron, truth is beyond your reach\"</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619203151Z</td>\n",
       "      <td>\"Your anti-Semitic rants are not welcomed here...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619033424Z</td>\n",
       "      <td>\"god, you're tiresome. get a life, you loser.\"</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619194023Z</td>\n",
       "      <td>\"Fact : Georgia passed a strict immigration po...</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>20120530000452Z</td>\n",
       "      <td>\"Of course you would bottom feeder ...\"</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619162450Z</td>\n",
       "      <td>\"You are\\xa0 a fukin moron. \\xa0\\xa0 You are j...</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619171832Z</td>\n",
       "      <td>\"You are a fucking dumb ass!.  Go back to you ...</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502203704Z</td>\n",
       "      <td>\"Lets see your papers arealconservati.\\n\\nTill...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Correction Bitch! You don't think it's superb...</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>20120611215519Z</td>\n",
       "      <td>\"I think the only trickle that effected you wa...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Insult             Date  \\\n",
       "0        1  20120618192155Z   \n",
       "7        1              NaN   \n",
       "8        1  20120502173553Z   \n",
       "9        1  20120620160512Z   \n",
       "15       1  20120611090207Z   \n",
       "16       1  20120320162532Z   \n",
       "18       1  20120320075347Z   \n",
       "19       1  20120320203947Z   \n",
       "34       1  20120515132156Z   \n",
       "37       1  20120620161958Z   \n",
       "38       1  20120609163743Z   \n",
       "41       1  20120620075045Z   \n",
       "45       1  20120619074710Z   \n",
       "47       1  20120529130844Z   \n",
       "51       1  20120320131007Z   \n",
       "55       1  20120502173058Z   \n",
       "59       1  20120619203151Z   \n",
       "61       1  20120619033424Z   \n",
       "79       1  20120619194023Z   \n",
       "80       1  20120530000452Z   \n",
       "82       1  20120619162450Z   \n",
       "88       1  20120619171832Z   \n",
       "93       1  20120502203704Z   \n",
       "95       1              NaN   \n",
       "96       1  20120611215519Z   \n",
       "\n",
       "                                              Comment  Size  \n",
       "0                                \"You fuck your dad.\"    20  \n",
       "7   \"shut the fuck up. you and the rest of your fa...    89  \n",
       "8   \"Either you are fake or extremely stupid...may...    57  \n",
       "9   \"That you are an idiot who understands neither...    76  \n",
       "15  \"FOR SOME REASON U SOUND RETARDED. LOL. DAMN. ...    65  \n",
       "16  \"You with the 'racist' screen name\\n\\nYou are ...    70  \n",
       "18                          \"your such a dickhead...\"    25  \n",
       "19     \"Your a retard go post your head up your #%&*\"    46  \n",
       "34  \"Allinit123, your\\xa0hypocrisy\\xa0is sickening...    68  \n",
       "37  \"I can't believe the stupid people on this sit...   237  \n",
       "38                   \"fucking behave then you prick!\"    32  \n",
       "41  \"Is that it??  your sum total to any conversat...   203  \n",
       "45                                           \"faggot\"     8  \n",
       "47  \"Aimless Abe you are a delusion weirdo that sh...   279  \n",
       "51      \"Even as a troll you are a pathetic failure.\"    45  \n",
       "55       \"You're a moron, truth is beyond your reach\"    44  \n",
       "59  \"Your anti-Semitic rants are not welcomed here...    77  \n",
       "61     \"god, you're tiresome. get a life, you loser.\"    46  \n",
       "79  \"Fact : Georgia passed a strict immigration po...   588  \n",
       "80            \"Of course you would bottom feeder ...\"    39  \n",
       "82  \"You are\\xa0 a fukin moron. \\xa0\\xa0 You are j...   243  \n",
       "88  \"You are a fucking dumb ass!.  Go back to you ...    89  \n",
       "93  \"Lets see your papers arealconservati.\\n\\nTill...    82  \n",
       "95  \"Correction Bitch! You don't think it's superb...   242  \n",
       "96  \"I think the only trickle that effected you wa...   196  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insult_df[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6SYFSGJrWSh"
   },
   "source": [
    "Write a line of code to compute what proportion of the data classified as an Insul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXzfwo7SrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYe160alrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdVATZFmrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nVs2MjJrWSh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLYSdEd1rWSi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTnNLeldrWSi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZN8Bf3RrWSi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwgNYmY0rWSi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rh0C23B-rWSi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9o90Fc8rWSi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jX7Mh_4YrWSi",
    "outputId": "0454643d-889d-4da5-8848-30fbb81230cb",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(insult_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZsnNo8XhrWSi",
    "outputId": "29f5ff89-5f0f-4abe-fb6e-550fa42bb9bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3947"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fSFRW1eprWSi",
    "outputId": "5da6a584-9574-45e8-e9bf-a965f030ae81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2657714720040537"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Insult'].sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTNSCIf7rWSi"
   },
   "source": [
    "There are documents of a **variety** of lengths, from various kinds of social media.  From pretty long..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "uPevaZIm-rRg",
    "outputId": "9b616c95-b275-4517-f6b9-55cfe5942318",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Fact : Georgia passed a strict immigration policy and most of the Latino farm workers left the area. Vidalia Georgia now has over 3000 agriculture job openings and they have been able to fill about 250 of them in past year. All you White Real Americans who are looking for work that the Latinos stole from you..Where are you ? The jobs are i Vadalia just waiting for you..Or maybe its the fact that you would rather collect unemployment like the rest of the Tea Klaners.. You scream..you complain..and you sit at home in your wife beaters and drink beer..Typical Real White Tea Klan....\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Comment'][79]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXp5M1AKrWSl"
   },
   "source": [
    "To very very short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_PkM5-Y-rRh",
    "outputId": "f5fa1949-6819-46c6-84e5-0cd7b48831f7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Insult                   1\n",
       "Date       20120620121441Z\n",
       "Comment           \"Retard\"\n",
       "Name: 755, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insult_df.loc[755]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9946gwJrWSl"
   },
   "source": [
    "A look at the range.  Some very long documents have to processed and\n",
    "correctly taggeda s insulting.  This is part of the challenge of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kIJQvpPWrWSm",
    "outputId": "fe6d07b0-36c0-4f92-bd3d-1092fdab25ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         20\n",
       "7         89\n",
       "8         57\n",
       "9         76\n",
       "15        65\n",
       "        ... \n",
       "3929     303\n",
       "3931    1600\n",
       "3934     334\n",
       "3935      89\n",
       "3942      49\n",
       "Name: Comment, Length: 1049, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insult_df[\"Comment\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8z7VmVw2rWSm",
    "outputId": "ade629ab-8b31-40ca-9fab-c98f328c937b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "(fig, ax) = plt.subplots(1,1,figsize=(8,8))\n",
    "insult_df[\"Size\"] = insult_df[\"Comment\"].str.len()\n",
    "\n",
    "bar = insult_df[\"Size\"].plot(kind=\"hist\",bins=[5, 10, 20, 30,40, 50,60,\n",
    "                                         70, 80, 90, 100, 120,140, 150], width=4,color=\"dodgerblue\",\n",
    "                          title=\"Length Distribution for Insults\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DeYXbqK-rRW"
   },
   "source": [
    "## Analyzing insults with Naive Bayes: sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jPBwaTr-rRi"
   },
   "source": [
    "We want to use one of the linear classifiers in `sklearn`,\n",
    "but the learners in `sklearn` only work with 2D numerical arrays.\n",
    "We next discuss how to convert text into an array  of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSvV0ToN-rRi"
   },
   "source": [
    "Given a collection of documents of size D and some class labels L,\n",
    "the classical solution can be described in several steps:\n",
    "\n",
    "1. extract a **vocabulary** of size V by scanning all the texts in . Then, we build a DxV **term-document matrix** `M`.\n",
    "2. For each document `d` and each word `w` fill cell `[d,w]` of `M` with a number representing the importance of word `w` in document `d`, in the simplest case, its count.  Thus one row of M represents one document in the collection. Since most documents contain only a small portion of the total vocabulary of the document collection, M is almost always **sparse**; that is, it consists almost entirely of zeros.\n",
    "3. Train a classifer on the term document matrix M and class labels L.\n",
    "\n",
    "The `sklearn` module allows us to\n",
    "complete steps 1 and 2 in a few lines of code using what is called a **vectorizer**.  Step\n",
    "3 is another few lines of code using a scikit learn  **classifier**. Although\n",
    "there are a few things to watch out for, the same classifiers that work for other classification\n",
    "problems generally work for text classification.\n",
    "\n",
    "We first encountered a scikit learn vectorizer in the regression and classification\n",
    "notebook in building an adjective classifier.  There, we were classifying words, not documents.\n",
    "and the features used for classification were letter sequences from 2 to 4 characters long;\n",
    "the feature values were counts of how many times a given letter sequence occurred in the word.\n",
    "The Count Vectorizer learned 78,696  features, so in\n",
    "the training data matrix, each word in the training data was rrepsented as\n",
    "a row (or **vector**) of 78, 696 integers, most of them zero.\n",
    "\n",
    "For example, the nonzero feature counts for the word \"alfalfa\" looked like this\n",
    "\n",
    "\n",
    "```python\n",
    "' a'  : 1\n",
    "' al' : 1\n",
    "' alf': 1\n",
    "'a '  : 1\n",
    "'al'  : 2\n",
    "'alf' : 2\n",
    "'alfa': 2\n",
    "'fal' : 1\n",
    "'fa'  : 2\n",
    "'fa ' : 1\n",
    "'falf': 1\n",
    "'lfa' : 2\n",
    "'lf'  : 2\n",
    "'lfa ': 1\n",
    "'lfal': 1\n",
    "```\n",
    "\n",
    "Based on these feature counts\n",
    "the classifier computed a probability that a word was an adjective.\n",
    "Although a vectorizer that measures the importance of a word\n",
    "by its count is always feasible, it doesn't work as well as you might think\n",
    "when the features are words.  Words counts are not very reliable\n",
    "indicators of the importance of a word because the most frequent\n",
    "words (*the*, *of*, *a*) tell us nothing about what a document is about.\n",
    "\n",
    "To represent the importance of a word in a document,  we will use a **TFIDF score**.\n",
    "Although there are a number of metric that has had success  inNatural Language Processing tasks; the\n",
    "TFIDF score has consistently been one of the more successful, and scikit_learn\n",
    "provides an implmenetation through its `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Rk9HaGLJ-rRk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "\n",
      "    Equivalent to :class:`CountVectorizer` followed by\n",
      "    :class:`TfidfTransformer`.\n",
      "\n",
      "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    input : {'filename', 'file', 'content'}, default='content'\n",
      "        - If `'filename'`, the sequence passed as an argument to fit is\n",
      "          expected to be a list of filenames that need reading to fetch\n",
      "          the raw content to analyze.\n",
      "\n",
      "        - If `'file'`, the sequence items must have a 'read' method (file-like\n",
      "          object) that is called to fetch the bytes in memory.\n",
      "\n",
      "        - If `'content'`, the input is expected to be a sequence of items that\n",
      "          can be of type string or byte.\n",
      "\n",
      "    encoding : str, default='utf-8'\n",
      "        If bytes or files are given to analyze, this encoding is used to\n",
      "        decode.\n",
      "\n",
      "    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "        Instruction on what to do if a byte sequence is given to analyze that\n",
      "        contains characters not of the given `encoding`. By default, it is\n",
      "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "        values are 'ignore' and 'replace'.\n",
      "\n",
      "    strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
      "        Remove accents and perform other character normalization\n",
      "        during the preprocessing step.\n",
      "        'ascii' is a fast method that only works on characters that have\n",
      "        a direct ASCII mapping.\n",
      "        'unicode' is a slightly slower method that works on any characters.\n",
      "        None (default) does nothing.\n",
      "\n",
      "        Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "        :func:`unicodedata.normalize`.\n",
      "\n",
      "    lowercase : bool, default=True\n",
      "        Convert all characters to lowercase before tokenizing.\n",
      "\n",
      "    preprocessor : callable, default=None\n",
      "        Override the preprocessing (string transformation) stage while\n",
      "        preserving the tokenizing and n-grams generation steps.\n",
      "        Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "    tokenizer : callable, default=None\n",
      "        Override the string tokenization step while preserving the\n",
      "        preprocessing and n-grams generation steps.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      "        Whether the feature should be made of word or character n-grams.\n",
      "        Option 'char_wb' creates character n-grams only from text inside\n",
      "        word boundaries; n-grams at the edges of words are padded with space.\n",
      "\n",
      "        If a callable is passed it is used to extract the sequence of features\n",
      "        out of the raw, unprocessed input.\n",
      "\n",
      "        .. versionchanged:: 0.21\n",
      "            Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n",
      "            is first read from the file and then passed to the given callable\n",
      "            analyzer.\n",
      "\n",
      "    stop_words : {'english'}, list, default=None\n",
      "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
      "        list is returned. 'english' is currently the only supported string\n",
      "        value.\n",
      "        There are several known issues with 'english' and you should\n",
      "        consider an alternative (see :ref:`stop_words`).\n",
      "\n",
      "        If a list, that list is assumed to contain stop words, all of which\n",
      "        will be removed from the resulting tokens.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "        If None, no stop words will be used. max_df can be set to a value\n",
      "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
      "        words based on intra corpus document frequency of terms.\n",
      "\n",
      "    token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      "        Regular expression denoting what constitutes a \"token\", only used\n",
      "        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      "        or more alphanumeric characters (punctuation is completely ignored\n",
      "        and always treated as a token separator).\n",
      "\n",
      "        If there is a capturing group in token_pattern then the\n",
      "        captured group content, not the entire match, becomes the token.\n",
      "        At most one capturing group is permitted.\n",
      "\n",
      "    ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "        The lower and upper boundary of the range of n-values for different\n",
      "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      "        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
      "        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
      "        only bigrams.\n",
      "        Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "    max_df : float or int, default=1.0\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly higher than the given threshold (corpus-specific\n",
      "        stop words).\n",
      "        If float in range [0.0, 1.0], the parameter represents a proportion of\n",
      "        documents, integer absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    min_df : float or int, default=1\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly lower than the given threshold. This value is also\n",
      "        called cut-off in the literature.\n",
      "        If float in range of [0.0, 1.0], the parameter represents a proportion\n",
      "        of documents, integer absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    max_features : int, default=None\n",
      "        If not None, build a vocabulary that only consider the top\n",
      "        max_features ordered by term frequency across the corpus.\n",
      "\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    vocabulary : Mapping or iterable, default=None\n",
      "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "        indices in the feature matrix, or an iterable over terms. If not\n",
      "        given, a vocabulary is determined from the input documents.\n",
      "\n",
      "    binary : bool, default=False\n",
      "        If True, all non-zero term counts are set to 1. This does not mean\n",
      "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      "        is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
      "\n",
      "    dtype : dtype, default=float64\n",
      "        Type of the matrix returned by fit_transform() or transform().\n",
      "\n",
      "    norm : {'l1', 'l2'} or None, default='l2'\n",
      "        Each output row will have unit norm, either:\n",
      "\n",
      "        - 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "          similarity between two vectors is their dot product when l2 norm has\n",
      "          been applied.\n",
      "        - 'l1': Sum of absolute values of vector elements is 1.\n",
      "          See :func:`preprocessing.normalize`.\n",
      "        - None: No normalization.\n",
      "\n",
      "    use_idf : bool, default=True\n",
      "        Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
      "\n",
      "    smooth_idf : bool, default=True\n",
      "        Smooth idf weights by adding one to document frequencies, as if an\n",
      "        extra document was seen containing every term in the collection\n",
      "        exactly once. Prevents zero divisions.\n",
      "\n",
      "    sublinear_tf : bool, default=False\n",
      "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    vocabulary_ : dict\n",
      "        A mapping of terms to feature indices.\n",
      "\n",
      "    fixed_vocabulary_ : bool\n",
      "        True if a fixed vocabulary of term to indices mapping\n",
      "        is provided by the user.\n",
      "\n",
      "    idf_ : array of shape (n_features,)\n",
      "        The inverse document frequency (IDF) vector; only defined\n",
      "        if ``use_idf`` is True.\n",
      "\n",
      "    stop_words_ : set\n",
      "        Terms that were ignored because they either:\n",
      "\n",
      "          - occurred in too many documents (`max_df`)\n",
      "          - occurred in too few documents (`min_df`)\n",
      "          - were cut off by feature selection (`max_features`).\n",
      "\n",
      "        This is only available if no vocabulary was given.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      "\n",
      "    TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      "        matrix of counts.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The ``stop_words_`` attribute can get large and increase the model size\n",
      "    when pickling. This attribute is provided only for introspection and can\n",
      "    be safely removed using delattr or set to None before pickling.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    >>> corpus = [\n",
      "    ...     'This is the first document.',\n",
      "    ...     'This document is the second document.',\n",
      "    ...     'And this is the third one.',\n",
      "    ...     'Is this the first document?',\n",
      "    ... ]\n",
      "    >>> vectorizer = TfidfVectorizer()\n",
      "    >>> X = vectorizer.fit_transform(corpus)\n",
      "    >>> vectorizer.get_feature_names_out()\n",
      "    array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
      "           'this'], ...)\n",
      "    >>> print(X.shape)\n",
      "    (4, 9)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(text.TfidfVectorizer.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSoyP8hhrWSm"
   },
   "source": [
    "Let's try this idea out on a very small data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "4eJ4MSuPrWSm",
    "outputId": "b2cc814a-63a1-4504-a937-b2e160d944da",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "        'This is the first document.',\n",
    "         'This document is the second document.',\n",
    "         'And this is the third one.',\n",
    "         'Is this the first document?',\n",
    "    ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feats = vectorizer.get_feature_names_out()\n",
    "print(len(feats))\n",
    "print(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wztwZAaSrWSm"
   },
   "source": [
    "So the vocabulary size is 9, so assuming this is  all the training data,\n",
    "the document vectors of any future documents will be represented as vectors\n",
    "of length 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbhxXXQJrWSm"
   },
   "source": [
    "Here is the output of `.fit_transform(...)` , a 4 x 9  term document matrix,\n",
    "where 4 is the number of documents and 9 is the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "Epk4Nm80rWSm",
    "outputId": "fd78c811-461f-4b0b-9955-e2f912e7111b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYHiP-1ArWSm"
   },
   "source": [
    "For each document and word the numerical value is a TFIDF score indicating the weigh/importance\n",
    "of that word in that document.  In many cases that importance is 0,\n",
    "indicating that word did not occur in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnSyrTiKrWSn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmED_l4crWSn"
   },
   "source": [
    "####  Explaining TFIDF\n",
    "\n",
    "Here are some statistics from the **British National Corpus**:\n",
    "\n",
    "```\n",
    "BNC\n",
    "\n",
    "Corpus size   51,994,153\n",
    " Vocab size      511,928\n",
    "   Num docs        1,726\n",
    "```\n",
    "\n",
    "And here are some interesting cases where word frequency\n",
    "is close and document frequency  isn't:\n",
    "\n",
    "```\n",
    "social                                             18,419          1,083         \n",
    "want                                               18,284          1,415         \n",
    "\n",
    "allow                                               5,285          1,232*\n",
    "computer                                            5,262            715\n",
    "treatment                                           5,250            906\n",
    "gives                                               5,258          1,191*\n",
    "easily                                              5,218          1,212*\n",
    "```\n",
    "\n",
    "What we're seeing is that certain words are **burstier** than others.  Once\n",
    "they occur once in a document, they are much more likely to occur\n",
    "again in that same document than you'd expect given their frequency.  Take, for example,\n",
    "*computer*.  Once you see this word, it's likely\n",
    "that the document it occurs in has something to say about\n",
    "some technical or computer-related topic, and\n",
    "so the chances of seeing the word again are high.\n",
    "On the other hand, consider the word *gives*, whose overall\n",
    "frequency is nearly the same as *computer*.  This word\n",
    "doesn't tell you nearly as much about the topic of the document\n",
    "we're looking at, and the chances of seeing it again in the same document\n",
    "are neither higher nor lower than you'd expect for\n",
    "a word of that frequency: *computer* is bursty (it's 5K occurences are distributed\n",
    "over relatively few documents); *gives* is not.\n",
    "\n",
    "The TFIDF statistic takes into account not just the relative frequency of a word\n",
    "in a document (the **Term Frequency**). It also takes into account its burstiness.  Burstiness is measured by **Inverse Document Frequency**.\n",
    "\n",
    "The term frequency of a term in a document is just its **relative frequency** (frequency\n",
    "divided by document size).  That depends on both the word and the document\n",
    "\n",
    "\n",
    "$$\n",
    "(1) \\; \\text{tf}(t,d) = \\frac{f_{t,d}}{\\mid d \\mid}\n",
    "$$\n",
    "\n",
    "\n",
    "The inverse document frquency of a term $t$ in a set of documents D\n",
    "is the inverse of its relative frequency in D:\n",
    "\n",
    "$$\n",
    "(2) \\; \\text{idf}(t, D) = \\frac{\\mid D \\mid}{\\mid\\lbrace d \\mid d \\in \\text{D} \\text{ and } t \\in d  \\rbrace\\mid}\n",
    "$$\n",
    "\n",
    "This metric depends on the  word and document **set**.\n",
    "\n",
    "$$\n",
    "\\begin{array}[t]{ll}\n",
    "\\text{D}   & \\text{the set of  documents in the training data}\\\\\n",
    "\\mid\\text{D}\\mid   & \\text{ the number of docs in D}\\\\\n",
    "t          & \\text{the term or word}\\\\\n",
    "\\mid\\lbrace d \\mid d \\in \\text{D} \\text{ and } t \\in d  \\rbrace\\mid &\n",
    "\\text{the number of documents } t \\text{ occurs in }\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Often, instead of using IDF, what's used is the logarithm of IDF:\n",
    "\n",
    "$$\n",
    "(3) \\; \\text{log-idf}(t,\\text{D})  = \\log (\\text{idf}(t, \\text{D}))\n",
    "$$\n",
    "\n",
    "The expression $\\log \\text{idf}(t, D)$ is $- \\log \\text{prob}_{D}(t)$,\n",
    "which in information theory is the amount of the information\n",
    "gained by knowing $t$ occurs in a document in the corpus.  So TFIDF\n",
    "weights the term frequency by the information value of the term.\n",
    "\n",
    "\n",
    "A very popular version of TFIDF is the product of\n",
    "the log inverse document frequency and the term count.\n",
    "\n",
    "$$\n",
    "(4)\\; \\text{TFIDF}(t,d)  = \\text{tf}(t,d)  \\cdot \\text{log-idf}(t, D)\n",
    "$$\n",
    "\n",
    "Just weight the term frequency of $t$ in $d$ by the information value of $t$.\n",
    "\n",
    "The version of TFIDF used in scikit learn (also popoular) is esentially the product of\n",
    "the log inverse document frequency and the term count.\n",
    "\n",
    "$$\n",
    "(5)\\; \\text{TFIDF}(t,d)  = f_{t,d} \\cdot \\text{log-idf}(t, D)\n",
    "$$\n",
    "\n",
    "The raw term frequency is often used rather than the relative frequency\n",
    "because the document vectors are going to be normalized to\n",
    "unit length, so the document size will  be taken into\n",
    "account, but in a slightly different way.\n",
    "\n",
    "\n",
    "There are some technical details of the scikit learn implementation\n",
    "discussed [here](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "left out of equation (5), but that's the essential idea.\n",
    "\n",
    "\n",
    "Let's finish with an example.  Suppose we have a document in which\n",
    "the word *given* and the word *computer* both occur 3 times.  Let's\n",
    "use equation (5) and the statistics above to compute the 2 TFIDF scores:\n",
    "\n",
    "$$\n",
    "\\begin{array}[t]{ll}\n",
    "(a)\\; \\text{TFIDF}(\\text{computer},d)  &=\n",
    "\\begin{array}[t]{l}\n",
    "3 \\cdot \\text{log-idf}(\\text{computer}, D)\\\\\n",
    "\\log \\frac{1726}{715}\\\\\n",
    "2.64\\\\\n",
    "\\end{array}\\\\\n",
    "(b)\\; \\text{TFIDF}(\\text{gives},d)  & =\n",
    "\\begin{array}[t]{l}\n",
    "3 \\cdot \\text{log-idf}(\\text{gives}, D)\\\\\n",
    "\\log \\frac{1726}{1191}\\\\\n",
    "1.11\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "So as desired the occurrence of *computer* is more significant, in fact more than twice\n",
    "as significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vg9Ner4JrWSn"
   },
   "source": [
    "### Working through the insult data\n",
    "\n",
    "Having talked through what scikit learn is going to do,  let's get back to\n",
    "the insult data and demonstrate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "H-fPVjYV-rRl"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets FIRST\n",
    "T_train,T_test, y_train,y_test = train_test_split(df['Comment'],df['Insult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "4JLT1QylrWSn"
   },
   "outputs": [],
   "source": [
    "#tf = text.TfidfVectorizer(min_df=2,max_df=.8)\n",
    "tf = text.TfidfVectorizer()\n",
    "# Train your vectorizer oNLY on the trainingh data.\n",
    "X_train = tf.fit_transform(T_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IO9ykCgdrWSn"
   },
   "source": [
    "`X-train` is our **term-document** matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUihdpHIgA7e",
    "outputId": "3a69016d-c65e-4994-e021-3451880e8b00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2960, 13586)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDSn70PyrWSn"
   },
   "source": [
    "The number of columns is over 10,000.  That means our vcabulary size is over 10,000.  The actual vocabulary size of the data is a little larger, because not all of the word in the training set are being used as features.  We'll skip over the details of how those decisions are made.  For now, let's get tp the main idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH5ql7EArWSn"
   },
   "source": [
    "The most important computational fact about the term-document matrix is its sparseness,\n",
    "the fact that it consists mostly of 0s, because,\n",
    "for any given document, most of the words in the 10,000 word vocabulary don't occur in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4p8xlL7b-rRl",
    "outputId": "f9631fcd-06f4-4ce3-c43e-20d3b6a9e219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2,960 x 13,586)  Non-zero entries: 74,539\n"
     ]
    }
   ],
   "source": [
    "# Shape and Number of non zero entries\n",
    "print(f'Shape: ({X_train.shape[0]:,} x {X_train.shape[1]:,})  Non-zero entries: {X_train.nnz:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfUbNDjt-rRm"
   },
   "source": [
    "Let's estimate the sparsity of this feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7o8Oa6JP-rRm",
    "outputId": "99e9b8b8-2f93-4d10-a445-06789c4b8dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document matrix X is ~0.19% non-zero features.\n"
     ]
    }
   ],
   "source": [
    "print((\"The document matrix X is ~{0:.2%} non-zero features.\".format(\n",
    "          X_train.nnz / float(X_train.shape[0] * X_train.shape[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L84bZFAfrWSn",
    "outputId": "494a664d-01d2-4077-dc3e-8e9b377f7a11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2960x13586 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 74539 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21HEUruW-rRm"
   },
   "source": [
    "The `sklearn` module stores many of its internally computed arrays as **sparse matrices**.  This is basically a\n",
    "very clever computer science device for not wasting all the space that very sparse matrices\n",
    "waste.  Natural language representations are often **quite** sparse.  The .15% non zero features\n",
    "firgure we just looked at was typical.  Sparse matrices come at a cost, however; although some\n",
    "computations can be done while the matrix is in sparse form, some cannot, and to do those\n",
    "you have to convert the matrix to a nonsparse matrix, do what you need to do, and then, probably,\n",
    "convert it back.  This is costly.  We're going to do it now, but only because we're goofing\n",
    "around. Conversion to non-sparse format should in general be avoided whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "s501JX4y-rRn"
   },
   "outputs": [],
   "source": [
    "# Sparse matrix rep => ordinary 2D numpy array/\n",
    "XA = X_train.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4dXO87k-rRm"
   },
   "source": [
    "The number of the column that represents a word in the term document matrix is called its **encoding**.\n",
    "\n",
    "A `TdidfVectorizer` instance stores its encoding dictionary in the attribute `vocabulary_` (note\n",
    "the trailing underscore!).\n",
    "\n",
    "Let's consider the insult word *moron*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4DfZgRC-rRm",
    "outputId": "2da32521-3671-41b0-ebfe-076e3554869d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7226"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moron_ind = tf.vocabulary_['moron']\n",
    "moron_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlqPV8sk-rRn"
   },
   "source": [
    "Let's find a comment that contains 'moron' and remember its\n",
    "positional index in the training data so we can look up that doc in X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0l8PmsqrWSn",
    "outputId": "9de3e8d6-1878-445f-9e0b-b8312c7ca744",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"All racists like you are morons, dude! \\n\\nIt just happens most pit bull owners are morons too & this has even more convinced me of that.\"\n"
     ]
    }
   ],
   "source": [
    "results = [(i,comment) for (i,comment) in enumerate(T_train) if 'moron' in comment]\n",
    "\n",
    "#for (i,comment) in enumerate(T_train):\n",
    "#   if 'moron' in comment:\n",
    "#        results.append((i,comment))\n",
    "\n",
    "morons_comments = [(i,comment) for (i,comment) in results if 'morons' in comment]\n",
    "moron_comment = morons_comments[0][1]\n",
    "moron_comment_idx = morons_comments[0][0]\n",
    "doc_i = T_train.iloc[moron_comment_idx]\n",
    "print(doc_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83msbB2F-rRn"
   },
   "source": [
    "Ok, now we can check the TFIDF matrix for the statistic for `'moron'` in this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkOJ8G5l-rRn",
    "outputId": "ba603288-8897-48bc-de15-f52c766e4aa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XA[moron_comment_idx][moron_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwJGxnst-rRn"
   },
   "source": [
    "Wait!  That didn't work! That zero means the word *moron* doesn't occur in this document.\n",
    "\n",
    "If you go back and look carefully at the last line in this document, the word is actually *morons*, not *moron*, and the  code for choosing a comment from among the `moron_comments` actually guaranteed that.\n",
    "\n",
    "Since our test for *moron*-documents was to use `in` directly on the document string,\n",
    "it didn't distinguish occurrences of *moron* from occurrences of *morons*;\n",
    "`\"moron\"` is a substring of both words:\n",
    "\n",
    "```python\n",
    ">>> \"moron\" in \"you morons!\"\n",
    "True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmLm4OTm-rRo"
   },
   "source": [
    "And of course \"morons\" is a totally different word from *moron*, found at a totally different place in `XA`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ObsyKu1n-rRo"
   },
   "outputs": [],
   "source": [
    "new_moron_ind = tf.vocabulary_['morons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elnsqWG45yc-",
    "outputId": "a74c7cf6-1726-4bf4-e231-d0aabbb96839"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5065367409808235"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XA[moron_comment_idx][new_moron_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdC3bkw5rWSo"
   },
   "source": [
    "The moral is that the first preprocessing step in the vectorization process is to break\n",
    "a document string into a sequence of words.  This step is called **tokenization** and\n",
    "it's a little more sophisticated than calling `.split()` on the document string,\n",
    "but the result is similar. The bottom line is that unless you do something\n",
    "special to change things, the basic units of analysis for a document in text classification\n",
    "are going to be words, not subsequences of characters as they were in our adjective example.\n",
    "\n",
    "Summary: In this part of the discussion, we have learned about **vectorization**, the computational\n",
    "process of going from a sequence of documents to a term-document matrix.\n",
    "\n",
    "The key point is that the term document matrix is now exactly the sort of thing we used to\n",
    "train classifier to recognize iris types: a matrix whose rows represent exemplars\n",
    "and whose columns represent features.  That mean we can just pass the\n",
    "term document matrix X_train (along with some labels) to a classifier instance to\n",
    "train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0wp6RbS-rRo"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88mG_1C0-rRo"
   },
   "source": [
    "Now, we are going to train a classifier as usual. We\n",
    "have already split the data and labels into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf-3XQDN-rRo"
   },
   "source": [
    "We use a **Bernoulli Naive Bayes classifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "uj7vvTS--rRo",
    "outputId": "c3be5d3a-7801-4311-d8da-45c6a267b36e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb =nb.BernoulliNB()\n",
    "\n",
    "bnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMWe6ZCDrWSo"
   },
   "source": [
    "And we're done.  How'd we do?  Now we  test on the test set.  Before we can do that we need to\n",
    "vectorize the test set.  But don't just copy what we did with the training data:\n",
    "\n",
    "```\n",
    "X_test = tf.fit_transform(T_test)\n",
    "```\n",
    "\n",
    "That would retrain the vectorizer from scratch.  Any words that occurred in the training texts\n",
    "but not in the test texts would be forgotten!  Plus training the vectorizer\n",
    "is part of the classifier training pipeline.  If we let the vectorizer see\n",
    "the test data during its training phase, we'd be compromising the whole\n",
    "idea of splitting training and test data.  So what we want to do\n",
    "with the test data is just apply the transform part of vectorizing:\n",
    "\n",
    "```\n",
    "X_test = tf.transform(T_test)\n",
    "```\n",
    "\n",
    "That is, build a representation of the test data using only the vocabulary you learned\n",
    "about in training.  Ignore any new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rewbP2vT-rRp",
    "outputId": "fa1e71c6-3712-443d-a90f-49faa0d4f013"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7527862208713273"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = tf.transform(T_test)\n",
    "bnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cyzwhh7FrWSo"
   },
   "source": [
    "### Summarizing everything up till now\n",
    "\n",
    "Let's summarize what we did by gathering the steps into one cell without all the discussion and re-executing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "piBN9h0YrWSo",
    "outputId": "b0304c58-d6f9-4a13-cd59-13145e4e7be2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7426545086119554"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_train,T_test, y_train,y_test = train_test_split(df['Comment'],df['Insult'])\n",
    "tf = text.TfidfVectorizer()\n",
    "X_train = tf.fit_transform(T_train)\n",
    "bnb =nb.BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "X_test = tf.transform(T_test)\n",
    "bnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p38bf79W-rRp"
   },
   "source": [
    "The result should be the same as when we stepped through it with lots of discussion, right?\n",
    "\n",
    "Well, is it?  \n",
    "\n",
    "Ok, re-execute the same cell above again.  Now one more time.\n",
    "\n",
    "Now try the following\n",
    "piece of code, which wraps our classification pipeline into\n",
    "a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-bt36GeMpFW"
   },
   "source": [
    "#### Basic train and test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "y09nUF7KrWSp"
   },
   "outputs": [],
   "source": [
    "def split_vectorize_and_fit(docs,labels,clf,**params):\n",
    "    \"\"\"\n",
    "    Given labeled data (docs, labels) and a classifier,\n",
    "    do the training test split.  Train the vectorizer and the classifier.\n",
    "    Transform the test data and return a set of preducted labels\n",
    "    for the test data,\n",
    "    \"\"\"\n",
    "    T_train,T_test, y_train,y_test = train_test_split(docs,labels)\n",
    "    tf = text.TfidfVectorizer(**params)\n",
    "    X_train = tf.fit_transform(T_train)\n",
    "    clf_inst = clf()\n",
    "    clf_inst.fit(X_train, y_train)\n",
    "    X_test = tf.transform(T_test)\n",
    "    return clf_inst.predict(X_test), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwsayH6F-rRp",
    "outputId": "a54b9688-ad25-4337-da01-b87f36b1daa0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.751\n",
      "0.773\n",
      "0.752\n",
      "0.769\n",
      "0.754\n",
      "0.751\n",
      "0.752\n",
      "0.751\n",
      "0.761\n",
      "0.764\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "for test_run in range(num_runs):\n",
    "    predicted, actual = split_vectorize_and_fit(df['Comment'],df['Insult'], nb.BernoulliNB)\n",
    "    print('{0:.3f}'.format(accuracy_score(predicted, actual)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdxesQwr-rRp"
   },
   "source": [
    "What's happening?  \n",
    "\n",
    "The training test split function takes a random sample of all the data to use as training data.\n",
    "Each time there's a train test split we train a slightly different classifier.  Sometimes the\n",
    "training data is a better preparation for the test than others.   And so the actual\n",
    "variation in performance is significant.\n",
    "\n",
    "How should we deal this with this when we report our evaluations?\n",
    "To report a realistic estimate of how good a classifier our classifier is,\n",
    "we need to take the average of multiple training runs, each with a different train/test split of our working\n",
    "data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2SjpCsKMvfh"
   },
   "source": [
    "### Refined train and test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6353p_bprWSp"
   },
   "source": [
    "Explain the purpose of the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3HVmOUb-rRp",
    "outputId": "35e16ede-6c0c-439d-958c-82dab957e1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.77\n",
      "Precision 0.14\n",
      "Recall 0.88\n",
      "Pct Insults 0.27\n"
     ]
    }
   ],
   "source": [
    "num_runs = 100\n",
    "\n",
    "stats = np.zeros((4,))\n",
    "for test_run in range(num_runs):\n",
    "    predicted, actual = split_vectorize_and_fit(df['Comment'],df['Insult'],nb.BernoulliNB)\n",
    "    y_array = actual.values\n",
    "    prop_insults = y_array.sum()/len(y_array)\n",
    "    stats = stats + np.array([accuracy_score(predicted, actual),\n",
    "                              precision_score(predicted, actual),\n",
    "                              recall_score(predicted, actual),\n",
    "                              prop_insults])\n",
    "normed_stats = stats/num_runs\n",
    "labels = ['Accuracy','Precision','Recall','Pct Insults']\n",
    "for (i,s) in enumerate(normed_stats):\n",
    "    print(f'{labels[i]} {s:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMyf3dS8rWSp"
   },
   "source": [
    "### Most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FO_GHBj_rWSp"
   },
   "source": [
    "In this section we look at what features\n",
    "are the most important in insult detection.\n",
    "Despite the title of this notebook, we're not going to use\n",
    "Naive Bayes; we're going to a different\n",
    "classifier, because it works a little better for this\n",
    "task.\n",
    "\n",
    "For this experiment, we leave out the training test split;\n",
    "in fact, we leave out anything to do with testing.\n",
    "\n",
    "Our goal is to get a slightly better understanding of what it means to find a linear separator\n",
    "in a classification setting.  We started our tour of classifiers by looking at\n",
    "iris classificstion, a simple problem with 4 features.  Now in insult detection\n",
    "we have over 16,000 features.  Mathematically the only change is that\n",
    "we seek a separator in 16,000 dimensions instead of 4.\n",
    "\n",
    "But what does that that really mean?\n",
    "What it boils down is that we are looking for an assignment of weights\n",
    "to all our features such that a linear combination of the weighted feature values\n",
    "does the best job we can at classifying our data.  And in this\n",
    "setting, where our features are words, that means that with the\n",
    "right kind of classifier, and the right implementation, we\n",
    "can ask the classifier what features got the most weight.\n",
    "In  the case of classifying insults, that mean we can ask what\n",
    "words were the most important in classifying something as an insult.\n",
    "\n",
    "In the cell below, we give a very simple function for doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hozdOKLirWSp",
    "outputId": "0f7f7813-b892-4eda-d261-751d2921ad5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos: idiot dumb moron loser you stupid bitch faggot asshole mouth\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm as svm\n",
    "\n",
    "def get_feature_weights (clf):\n",
    "    if hasattr(clf,\"coef_\"):\n",
    "        return clf.coef_\n",
    "    elif hasattr(clf,'feature_log_prob_'):\n",
    "        return clf.feature_log_prob_\n",
    "    else:\n",
    "        raise Exception(f\"Can\\'t find weights for classifier {clf}\")\n",
    "\n",
    "def print_topn(clf, vectorizer, top_n=10, class_labels=(True,), feature_names=None):\n",
    "    \"\"\"\n",
    "    Prints features with the highest coefficient values, per class\n",
    "\n",
    "    If supplied features_names shd be a numopy array.\n",
    "    \"\"\"\n",
    "    if feature_names is None:\n",
    "        ## NB: feature_names is a numpy array\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "    ## For a binary classification problem with LogisticRegression or an SVM this is a 2D array with one row\n",
    "    ## For multiclass problem or for cetrain classifiers evena 1 class problem this has more than one row\n",
    "    weights = get_feature_weights (clf)\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        ## Look at the feature weights the classifier learned for class i.\n",
    "        ## ArgSort the weights (what feature indexes have the highest weights)\n",
    "        word_importance = np.argsort(weights[i])\n",
    "        ## Get the topn (reverse the order because the sort goes from smallest to biggest)\n",
    "        top_inds = word_importance[-top_n:][::-1]\n",
    "        ## What the words for the topn features are\n",
    "        print(f\"{class_label}: {' '.join(feature_names[top_inds])}\")  ## Using fancy indexing on feature_names\n",
    "\n",
    "#stop_words\n",
    "tf = text.TfidfVectorizer()\n",
    "X_train = tf.fit_transform(df['Comment'])\n",
    "est = svm.LinearSVC()\n",
    "est.fit(X_train, df['Insult'])\n",
    "# Now find the most heavily weighted features [= words]\n",
    "print_topn(est, tf, class_labels=(\"Pos\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvxe-F1n-rRp"
   },
   "source": [
    "We found the words with the top 10 weights and printed them out.  The inclusion of *mouth* is not exactly a surprise but probably not one of the words we would have guessed would make the top 10.  The inclusion of *you* is also\n",
    "interesting.  Its function is undoubtedly ambiguous (you can deliver some very nice compliments using *you*), but its appearance in a number of insults is enough to give it plenty of weight as an indicator.\n",
    "\n",
    "All in all, this is indeed a nasty collection of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sbosh1SrWSp"
   },
   "source": [
    "The idea this code implements.  Words are features.  The features get assigned weights\n",
    "in training the linear classifier.  The linear classifer makes its decision\n",
    "based on a linear combination of the feature values that uses those weights\n",
    "as coefficents,\n",
    "\n",
    "The weight of the highest-weighted feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Z3Rl7SzrWSp",
    "outputId": "7e655863-d09c-4b19-c8d6-21ccbf5b8c63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4082133368833216"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights=est.coef_[0]\n",
    "# The idxs of weights in value-order: from lowest-valued idx to highest-valued idx\n",
    "word_importance = np.argsort(weights)\n",
    "# Reordering the values in weights from lowest to highest\n",
    "sorted_vals = weights[word_importance]\n",
    "sorted_vals[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvo96mrGrWSp"
   },
   "source": [
    "The word corresponding to the highest weighted feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8q_6Wa8brWSp",
    "outputId": "d579a030-4722-4bfb-9c7c-512db40a6572"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idiot'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tf.get_feature_names_out()\n",
    "# The idx for the highest-weighted feature\n",
    "# is also the idx of the corresponding word (in feature_names)\n",
    "feature_names[word_importance[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gip6lPgZrWSp"
   },
   "source": [
    "### Running the classifier on a list of sentences (how feature-hacking isn't easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0th-hPo-rRq"
   },
   "source": [
    "Finally, let's look at how to test our estimator on a few invented test sentences and think a little about\n",
    "how our word features work.\n",
    "\n",
    "We must be sure to `tf.transform` the data before passing it to `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "9PkNvp0m-rRq",
    "outputId": "7a675e1b-5ac0-4a7c-fad6-94346973bf87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n"
     ]
    }
   ],
   "source": [
    "predicted = est.predict(tf.transform([\n",
    "    \"I totally agree with you\",\n",
    "    \"You are so stupid\",\n",
    "    \"That you are an idiot who understands neither taxation nor women\\'s health.\"\n",
    "    ]))\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoYRjj7n-rRr"
   },
   "source": [
    "Not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMlLf9B1rWSp"
   },
   "source": [
    "Still thinking about the feature `you` (we have embarked on the perilous enterprise known as **feature-hacking**).  Let's try removing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37zSpyD_rWSp",
    "outputId": "a6a61505-3cd0-454e-d9fb-a3ee9f738251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n"
     ]
    }
   ],
   "source": [
    "predicted = est.predict(tf.transform([\n",
    "    \"I totally agree with\",\n",
    "    \"are so stupid\",\n",
    "    \"That are an idiot who understands neither taxation nor women\\'s health.\"\n",
    "    ]))\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxNr1wMxrWSq"
   },
   "source": [
    "No change.\n",
    "\n",
    "One-word versions, using our best guess for the strong indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jc6z-E7FrWSq",
    "outputId": "a72ddbe9-c15a-446c-ea27-663972b85770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n"
     ]
    }
   ],
   "source": [
    "predicted = est.predict(tf.transform([\n",
    "    \"agree\",\n",
    "    \"stupid\",\n",
    "    \"idiot.\"\n",
    "    ]))\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZH6Y00PrWSq"
   },
   "source": [
    "Alternate one-word versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_yZXgI1rWSq",
    "outputId": "83094c42-f78a-48f8-e356-ce858bc856b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n"
     ]
    }
   ],
   "source": [
    "predicted = est.predict(tf.transform([\n",
    "    \"you\",\n",
    "    \"so\",\n",
    "    \"understands.\"\n",
    "    ]))\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq6BiRjSrWSq"
   },
   "source": [
    "Given its role as a strong predictor (shown above), it's not surprising that the one-word utterance \"you\"\n",
    "is classified as an insult.   But is that always a good thing?  Could it possibly hurt\n",
    "more than it helps? For example, could it contribute to the low precision score?  Whenever\n",
    "we see the word *you*, we hallucinate an insult.\n",
    "\n",
    "Let's try an experiment.  Same classifier, same data, leave *you* out of the vocabulary\n",
    "(by making it a so-called **stop word** for the vectorizer, one of a list of words that should be ignored).\n",
    "Apart from that tiny change, same code as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvYI7nfsrWSq",
    "outputId": "34c00c9a-bddd-4770-9ee3-a80f94aa9f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.76\n",
      "Precision 0.12\n",
      "Recall 0.88\n",
      "Pct Insults 0.27\n"
     ]
    }
   ],
   "source": [
    "num_runs = 100\n",
    "\n",
    "stats = np.zeros((4,))\n",
    "for test_run in range(num_runs):\n",
    "    predicted, actual = split_vectorize_and_fit(df['Comment'],df['Insult'],nb.BernoulliNB,\n",
    "                                               stop_words=[\"you\"])\n",
    "    y_array = actual.values\n",
    "    prop_insults = y_array.sum()/len(y_array)\n",
    "    stats = stats + np.array([accuracy_score(predicted, actual),\n",
    "                              precision_score(predicted, actual),\n",
    "                              recall_score(predicted, actual),\n",
    "                              prop_insults])\n",
    "\n",
    "normed_stats = stats/num_runs\n",
    "labels = ['Accuracy','Precision','Recall','Pct Insults']\n",
    "for (i,s) in enumerate(normed_stats):\n",
    "    print(f'{labels[i]} {s:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQnvElvzrWSq"
   },
   "source": [
    "Compare with our results above:\n",
    "\n",
    "```\n",
    "Accuracy 0.76\n",
    "Precision 0.14\n",
    "Recall 0.89\n",
    "Pct Insults 0.27\n",
    "```\n",
    "\n",
    "Describe in your own words how the feature *you* affects performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5Aa5MGArWSq"
   },
   "source": [
    "Might as well see how the top 10 insult words are affected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "_bMKLBowrWSq",
    "outputId": "5e8373fb-6891-421e-fada-e9d5aee41ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos: idiot dumb moron loser stupid asshole bitch faggot ignorant mouth\n"
     ]
    }
   ],
   "source": [
    "#stop_words\n",
    "tf = text.TfidfVectorizer(stop_words=[\"you\"])\n",
    "X_train = tf.fit_transform(df['Comment'])\n",
    "est = svm.LinearSVC()\n",
    "est.fit(X_train, df['Insult'])\n",
    "# Now find the most heavily weighted features [= words]\n",
    "print_topn(est,tf, class_labels=(\"Pos\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leC0vcFvrWSq"
   },
   "source": [
    "The word *ignorant* has now slipped into the top 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8490374873353597, 0.7477477477477478, 0.640926640926641)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "T_train,T_test, y_train,y_test = train_test_split(df['Comment'],df['Insult'])\n",
    "tf = text.TfidfVectorizer()\n",
    "X_train = tf.fit_transform(T_train)\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "X_test = tf.transform(T_test)\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy_score(y_test, predictions), precision_score(y_test, predictions), recall_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"soooo......Having a man (I'm assuming) sucking your dick is doing you a favour? \\xa0Really? \\xa0To each their own I guess.\"\n",
      "\n",
      "\"if u gotta still trash talk phi phi o'hara ur pathetic, truly\"\n",
      "\n",
      "\"You're no longer the \"victim\", unless your mind is weak....\\xa0 you are a survivor if your mind is strong!\"\n",
      "\n",
      "\"You're really not helping your cause.\"\n",
      "\n",
      "\"I will go back to my lenses now and try to spice it up a little.\"\n",
      "\n",
      "\"You really are a funny guy!\"\n",
      "\n",
      "\"A little lipstick on your palms, & off to the jerk match.\"\n",
      "\n",
      "\"Shit\"\n",
      "\n",
      "\"Have you had this yourself?\"\n",
      "\n",
      "\"Obama is a friggin idiot, nuff said\"\n",
      "\n",
      "\"my screen is stuck in the black screen help somebody\"\n",
      "\n",
      "\"Where are you getting your information from?\"\n",
      "\n",
      "\"Sounds like you just make it up as you go along!! This a bunch of yahoos hell bent on a witch hunt, not a criminal court!\\n\\nReach down and way under, grab and pull out your head....There now, you are free!\"\n",
      "\n",
      "\"http://yo.assbag.go-fuck.yourself-bitch.on-facebook.you-stincky-hoe.now.click-motherfucker.com\"\n",
      "\n",
      "\"You're a little fella, aren't you?\"\n",
      "\n",
      "\"awww you go back checking your posts for likes. how sad.\"\n",
      "\n",
      "\"I was messing with that girl and you guys all stepped up like its your business lol get a life people...it's a forum...not real life\"\n",
      "\n",
      "\"Welfare queen into a whore, not a crack whore. \\xa0I think a whore is a more productive worker than a welfare queen.\"\n",
      "\n",
      "\"Type your comment here.sfsdfasdfsad\"\n",
      "\n",
      "\"If you love the Argentines so much Waters.....why dont you pack up that turd of a son of yours and piss off to live there with your new mates...lets see what the Argentinians do when they find that little cretin son of oyur hanging off one of their memorials\"\n",
      "\n",
      "\"Please site that source, considering over half of americans think gay marraige should be legal\"\n",
      "\n",
      "\"some fukin sketchers thats sad omg lol fuckin shitty ass sketchers why the fuck are they even named sketchers sketcherz fuckin suck ass\"\n",
      "\n",
      "\"Fuck hard, then fuck slow, then fuck hard, and then start again.\"\n",
      "\n",
      "\"afvet1991,\\\\n\\\\nI'm not gay, you sad little freak! Not everyone who support gay marriage are. We have empathy and compassion for others unlike you religious bigots who can't wait to judge others.\"\n",
      "\n",
      "\"You fail miserably in thwarting logic. Ever hear of the \"rule of law\" in your obviously pathetic educational backround? Idiots like you make blogs fun to laugh at, but you have no place in society as anything other than a clown. How many commendations from Congress or Presidential administrations? Zero. Who are you and what significance are you to our world? I think zero sums it up. A big fat waste of resources on an obviously flawed specimen of the human race.\"\n",
      "\n",
      "\"@Kick Ass @firenze_30fps \\\\\\\\n\\\\\\\\nAflack, or Hijack, is that you?\"\n",
      "\n",
      "\"You calling others ignorant assholes!!! HAHAHAH that is a fucking joke\"\n",
      "\n",
      "\"Ha ha wait until you have your own kids:)\"\n",
      "\n",
      "\"One day you'll let your child eat lights naked? You are one sick puppy\"\n",
      "\n",
      "\"No, it's because your liberal Messiah is SUCH a monumental LOSER!\"\n",
      "\n",
      "\"Never said anything derogatory about having tattoos, only posting the one on your ass on Facebook.\\xa0 And quit trying to pretty up what you say, you wanna say whore...say whore... I've never been a gardening implement.\"\n",
      "\n",
      "\"You mean like your post.  You read the column...\"\n",
      "\n",
      "\"you troll so hard\"\n",
      "\n",
      "\"Flute as fuck!\"\n",
      "\n",
      "\"We are crap, you said it yourself.\"\n",
      "\n",
      "\"I'll betcha $10K you won't get your money back...\"\n",
      "\n",
      "\"And Bolin's all like \"Fuck you Mako, I'll break up you and Asami!\"\"\n",
      "\n",
      "\"What are you going to do, watch your party keep minority people ignorant? Compassionate liberal, my.ass!\"\n",
      "\n",
      "\"money diggin bitch !! she looks like a rat\"\n",
      "\n",
      "\"@thejeanrose\\\\\\\\n\\\\\\\\n \\\\\\\\n\\\\\\\\n\" Imprisoning black men disproportionately. Obama does it\"\\\\\\\\n\\\\\\\\n \\\\\\\\n\\\\\\\\nAre you seriuos?\\\\\\\\n\\\\\\\\n \\\\\\\\n\\\\\\\\nThat\\\\\\\\\\\\'s a terribly ignorant statement.\"\n",
      "\n",
      "\"You know your history...\"\n",
      "\n",
      "\"But, Dante, with all due respect, you ARE a hard core partisan.\\n\\nYou DO NOT accept facts which counter your stated positions.\\n\\nYou falsely call government websites \"right wing blogs\"\\n\\nYou NEVER furnish a documented fact in support of your own positions while demanding them of others.\\n\\nYou REFUSE to even DISCUSS any opposing positions in a civil manner, choosing instead to call your conversants, liars, stupid, etc.\\n\\nIn short, your posts DEMONSTRATE you to be a hard core ideologue, devoid of ANY ability to think critically.\"\n",
      "\n",
      "\"@thebaskett\\xa0\\nCool. \\xa0How the fuck are you and how do you grade the Vikes' draft?\"\n",
      "\n",
      "\"All that hatred inside of you must have eaten away what little capacity for clear thought you may have had. You lack the basic intelligence and knowledge to have real discussions or debates, so you come here insulting people into having arguments. Your insults are childish. You're proving all the negatives about your party every time you type. How about posting your picture... what are you ashamed of? You wouldn't have the balls to push all 'teh stupid' under your real name.\"\n",
      "\n",
      "\"No G Goon was right, you're an idiot, plain and simple!!!\"\n",
      "\n",
      "\"I'm amused by the fact that WeBlog cons drop any issue once it turns into a loser.\\n\\nThis is an expression of that amusement.\"\n",
      "\n",
      "\"Conservatives are social leeches and the scum of the earth.\"\n",
      "\n",
      "\"you fucked it u up with stupid music like most these whack niggas do. weak sauce homie\"\n",
      "\n",
      "\"\"ignorant ass liberals, i swear...\"\\n\\nPssst...you are talking to the wrong end.\"\n",
      "\n",
      "\"Why are you here?\"\n",
      "\n",
      "\"That was fucking sick\"\n",
      "\n",
      "\"pirate402307 liked your comment\"\n",
      "\n",
      "\"We were thinking back in time when a young GHWB was goaded by his ambitious young wife to eliminate JFK., \\xa0it would go something like this,Babs, I can't stand that JFK, get rid of that white trash. \\xa0Then a little while later Babs said I can't stand that MM she's gonna run her mouth and then, RK is finished. \\xa0Now that would be a much better HBO movie...the white-washed Bush Legacy is getting tough to swallow.\"\n",
      "\n",
      "\"I'm a vet rob! How bout you? Now you know why some of us are not impressed with the street thug culture. Their are some bad $#@ people in this world. Expand your horizons, son. I used to think like you until I came up aginst some other culture that wasn't afraid of me and my culture.\"\n",
      "\n",
      "\"and like it matters, you are waiting for next year...I am still living in the playoffs...suck on it loser\"\n",
      "\n",
      "\"you sound like you need a hug :)\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_texts (predictions, y, p_cls, cls, verbose=False):\n",
    "    idxs = [idx for idx in range(len(predictions)) \n",
    "                if ((predictions[idx] == p_cls) and (y[idx] == cls))]\n",
    "    res=[T_test.iloc[idx] for idx in idxs]\n",
    "    if verbose:\n",
    "        for txt in res:\n",
    "            print(txt,end=\"\\n\\n\")\n",
    "    return res,idxs\n",
    "\n",
    "FPs,FP_idxs = get_texts(predictions,y_test.values, p_cls=1, cls=0,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Yo this my nigga right here! my n my boi lil b fuck bitches on the regular, we fucking everyone bitches. tell yo boys that yo bitches aint safe when Mel Gibson n Lil B rollin out on a bitch hunt\"\n",
      "\n",
      "\"In the north, social service would take away her brother. Stupid!\"\n",
      "\n",
      "\"and fuckface, in order to be situationally aware one would have to read the blog of last week in its entirety. from the point i called ew stupid and told many why they were wrong, till the point where u faggot started making separate threads discussing my trollness\"\n",
      "\n",
      "\"More idiots posting on a subject they nothing about.\"\n",
      "\n",
      "\"Ever eat AIDS infected feces? Just eat your own and you will..\"\n",
      "\n",
      "\"cockneywideboy\\n\\nSo your the obese twat taking up all the pavement...get a diet going and come back when you're cockneyslimboy.\"\n",
      "\n",
      "\"Wow!  we got us a real literate einstien here don't we???  \"dush bag\"?  And such grammar!!!  Maybe you should have stayed in school past the 2nd grade retard!  What happened,  momma spend all her welfare chk. on booze and crack???\"\n",
      "\n",
      "\"Commenter?\\xa0 Your very childish.\\xa0 Grow up and be a man.\\xa0 Stupid remarks like yours just proves that you are a little boy and have yet to understand what it means to be an adult.\"\n",
      "\n",
      "\"@Crissa:disqus LaRaza (The Race), NBP. Nation of Islam, Aztlan to name a few are very racist and very left wing. You guys are just scared to call out a black person or a hispanic for racism. I am not scared to call it like I see it. It is easy to attack whites because it makes you feel good about yourself, you think the minorities will like you if you attack whites too. Be freaking honest with yourself. This has to be one of your dumbest replies ever Crissa. \\n\\nI view the world through the eyes of a \"Native American\" as you liberals labeled us. All I see is a self loathing person who wants to feel good about herself and have others like her \"because she cares\". Still all I see is a person who sits on land once hunted and lived upon by the native peoples. How many thousands of Indians died so you can live where you live and feel good about yourself?\\n\\nYour second sentence makes no sense, because even my staunchest enemies never accuse me broad brushing. When I attack, I am very specific about my targets.\"\n",
      "\n",
      "\"who told you that....stupid ask yourself who kick his daughter out of the house at the age 16.....when it's comes protecting weaman you guys the last ppl on earth who can open their mounths..\"\n",
      "\n",
      "\"@justin_mia Perkins is a knucklehead\"\n",
      "\n",
      "\"That was so 45 minutes ago troll. Get with the times.\"\n",
      "\n",
      "\"If its so lame why did you read this or for that matter search this topic maybe you should think a little more before you make yourself look like a fucking hippocrate.\"\n",
      "\n",
      "\"Oh I think I do. Let's put it this way, it's not like watching United.\\n\\nOnly muppets like yourself can see that's good football. Listen good on them for qualifying, but the football is dross and a 100 odd idiots posting here isn't going to change that.\\n\\nSo think again about who the thickos are and add your name to the list.\"\n",
      "\n",
      "\"Evel dick gets my vote James R is a very creepy looking \"man\" Mr Rogers look alike plus he whined like a little bitch when Janelle destroyed \"him\" in the OTEV POV comp\"\n",
      "\n",
      "\"I can't believe the stupid people on this site that have no idea what is going on.\\xa0 You fools.\\xa0 You are all useful idiots.\\xa0 They are hiding the truth because you fools who protect him would look like the useful idiots you are.\"\n",
      "\n",
      "\"It's not the acting I'm talking about, it's the rediculous salaries for something that doesn't benefit humanity other than entertainment purposes.\\xa0 Sorry you misunderstood...you must be an actor...if you are, I hope you do great things with your money to help our society instead of spending it on material objects that won't help you when you get sick...just saying!\"\n",
      "\n",
      "\"by the way, Leidiot is a jerk.\"\n",
      "\n",
      "\"She can act like she thinks she's perfect and be a naive bitch ;)\"\n",
      "\n",
      "\"the same with write people . every one do some drug in there life ok not all black people is drug addict so i would like you to stop hating on blacks ok you prick . have some respect for him god know y he do what he did ok have a bless night u prick\"\n",
      "\n",
      "\"try speaking in correct english as well. what dummies are on here today.\"\n",
      "\n",
      "\"Now you went and...fucked..up this whole page.\"\n",
      "\n",
      "\"Why is manber such a shit rapper?\"\n",
      "\n",
      "\"Another worhless post from a worthless person.\"\n",
      "\n",
      "\"Yea because I waste my time\\xa0documenting\\xa0your lunacy. Sorry dikhead we all know you and what you post. Unlike you we don't suffer from ADD\"\n",
      "\n",
      "\"Are you and Higher(replying to mesaman above) the same loser or are you sniffing the same glue pot? \\xa0Perhaps you are just libtard products of our sorry education system, in which case you should be offered a real education in the USMC, if they would take you.\"\n",
      "\n",
      "\"What are you guys smoking?\"\n",
      "\n",
      "\"W T F are you ranting about adolf?\"\n",
      "\n",
      "\"you are a hater..plian and simple....\"\n",
      "\n",
      "\"Clearly you're a fucktard. \\xa0Not saying your right (or wrong), just saying who the fuck just randomly compares a rap group or any other MC on a post about a specific video and song about the dude's mama. \\xa0Wtf does SH or any other MC or Hammerdance have to do with this song?\"\n",
      "\n",
      "\"Senator Bryd\\xa0 Democrat\\n\\nSaul Alinsky aka libtards\\xa0 Socialist\"\n",
      "\n",
      "\"A couple of greedy, non-appreciative hos!\"\n",
      "\n",
      "\"I want to know why you don't have a job - is it because you were busted at work with your kiddieporn?\"\n",
      "\n",
      "\"We're not the losers but\\xa0 you and this country are.\"\n",
      "\n",
      "\"Thats what you call there a fucking mexican 100% there for you niggers & crackers\"\n",
      "\n",
      "\"I bet it took awhile for this guy to extract his foot from his mouth.\"\n",
      "\n",
      "\"You're certainly what Lenin called a \"useful idiot\".\\xa0 If you like Islam so much, go where it is practiced \"just the way you like it\".\\xa0 Get to beat your wife and will, kill your kids for whatever reason turns you on, lie to anyone about anything just because \"it furthers the cause of Islam\" and others around you ACTUALLY BELIEVE YOUR LIES.\"\n",
      "\n",
      "\"#nWo4Life\\n#FuckTrolls\"\n",
      "\n",
      "\"Listen you fucking wanker -- we approve all comments, so long as they are not inflammatory against a type of people. We censor blatant bigotry. But little dickwrinkles like you calling me a pussy? I'd never censor that. Look at other posts...people have wished AIDS on me dude, so...I've heard worse fuckhead.That said, if you have to sleep with a woman in her 60s, then that's pretty sad, dude. Try dating girls your own age. It's kind of weird you're into way older chicks. You might wanna rethink hugging your own mom when you have a boner going forward.Plus, I have had more ass than every male in your entire family line -- with their fucked up teeth and small dicks -- has ever seen. Just a point of clarification, you bucket of cunt slop.\"\n",
      "\n",
      "\"That guy is a real fuckstick.\"\n",
      "\n",
      "\"Look here, cunt. I hope you get raped by the devil. I hope you get cancer, and experience the most absurdly horrible pain a human being can feel. I hope Rick James slaps you right in the fuckdamn mouth. Then, after all of that. I'll walk right up to you, and shoot in the fucking throat.\\\\xc2\\\\xa0\"\n",
      "\n",
      "\"Foolish rube. We blew $4 trillion already on that stupidity.\"\n",
      "\n",
      "\"Many\\xa0Sri Lankans\\xa0talking shit\\xa0as usual.\\xa0we\\xa0must build\\xa0sri\\xa0lanka\\xa0we\\xa0must work together.\\xa0when\\xa0will you\\xa0understand that??\"\n",
      "\n",
      "\"LOL!!!! Read the ENTIRE paper REIDtard. Typical poorly educated LIBIDIOT.\"\n",
      "\n",
      "\"you are soooo inteligent white girl. Tell us black people more about ourselves. Im sure u know more about us then we do\"\n",
      "\n",
      "\"You are a traitor.\"\n",
      "\n",
      "\"it means you are stupider than average, unable to punch your way out of a wet paper bag. \\xa0But at least you have bragging rights to bully others around with your insane liberal concepts.\"\n",
      "\n",
      "\"If u didnt give a fuck what i was saying why is ur dumbass replying? Clearly u are ignorant and unsophisticated.\"\n",
      "\n",
      "\"Hey right wing moron \"You Kidding Me\", you're the cutest little GOP whine baby on this forum.\"\n",
      "\n",
      "\"What is the point of switching between accounts?\\\\nJust so you know, you are being software tracked.\\\\nIts useless you stupid imbecile. \"\n",
      "\n",
      "\"WTF KIM KWANG SOO THATS MOSTLY T-ARA MONEY U FUCKING LEECH\"\n",
      "\n",
      "\"YEAH,I BET YOU INBRED HILLBILLY.\"\n",
      "\n",
      "\"You're not the world's best debater.\"\n",
      "\n",
      "\"Dickhead will just copy and use it, don't bother. We know Rooster is all mouth and no balls.\"\n",
      "\n",
      "\"You're probably a phuuking dyke....aint ya beotch ?\"\n",
      "\n",
      "\"Last year, you're saying we ARE bankrupt, Understand grammar?\"\n",
      "\n",
      "\"Whaaaa whaaaaaa, When you lose intellectually you freaks either blame Bush or name call.  Newt went in debt a few mill. \\xa0But the god-damned comunity organizer sunk us down 6 TRILLION. \\xa0 You dicks are pitiful.\"\n",
      "\n",
      "\"Notheism:\\xa0 Are you serious? How can you say this is \"revenge\".\\xa0 The rapist was caught in the ACT.\\xa0 I understand your ignorance, but for god sakes, pick up a dictionary.\"\n",
      "\n",
      "\"More people on here than you, you cunt.\"\n",
      "\n",
      "\"haha you don't own an iphone? man your fucking poor\"\n",
      "\n",
      "\"Your notion that you are sexually abused because you are hitting on newcomers and want to be justified in that sounds exactly like the pathology of a predator. Better check yourself.\"\n",
      "\n",
      "\"Sure, it was. I asked Mr. Hawkins why we you allowed to continually call others pedophiles and he banned your ass. I can provide the email copy if you like. Don't be a pussy and deny it. \"\n",
      "\n",
      "\"That girl kinda look cock eye'd but its ok I'd fuck her till that shit turnt straight\"\n",
      "\n",
      "\"The same can be said about your posts, red. Dull, unimaginative, unsupported, jaded, biased, racist, homophobic.\\xa0 You are essentially a miserable human being.\\xa0 Not as miserable as Crapkiller or sitm, but pitiful nonetheless.\"\n",
      "\n",
      "\"Hush little negro don't say a word,\\nMama gonna buy yawz some Thunderbird.\"\n",
      "\n",
      "\"HahahahahahahahaThe fucking dyke deserved it!!!!!!!!!!!!!!! A bit of\\\\xc2\\\\xa0 karmic payback forall the degeneracy & misandry that lesbo feminists have inflictedupon the Western world. That degenerate dyke bitch that's still alive should not subject the rest of us to her{?} disgusting emotional diarrhea over her{?} fat dead land whale. Our world would be far better off were it not polluted by degenerate lesbo feminist pieces of shit!!!!!!!!!!!!!!\"\n",
      "\n",
      "\"oh all of a sudden u changed ur picture to A WHITE GUY WITH PENNY LOAFERS....MUFUCKER U LAME\"\n",
      "\n",
      "\"Aimless Abe you are a delusion weirdo that should be locked up inside and kept away from computers.\\xa0 Anyone who doubts the lunacy of Abe the moronic prog frog from NYC please read this thread and you will see Abe completely come apart from the seams and go off the deep end.\"\n",
      "\n",
      "\"Oh NO! Somebody DELETED my comment. You all KNOW the RULE: 1 delete = 50 REPOSTS. Here we go:\\n#38 Bill O is furious that Obama isn't getting the US more involved in the fuckedup places in the world. You know Internationalist Rubish Murdouchebag signs his paycheck funded by Internationalist Saudi ROYAL Aldickweed. Yeah Bill get the tea-tards stirred up the US is not playing Murdouchebag-Aldickweeds agenda like the Bushies. Ron Paul was right about you foxtarrds.\"\n",
      "\n",
      "\"your ego makes you less qualified.\"\n",
      "\n",
      "\"Leftist White race traitor\\xa0or shit skinned\\xa0degenerate; you are a hard one to call.\\n\\nMongoloid.\"\n",
      "\n",
      "\"All I see in these comments is niggas bitching more then biggest bitches i know..\"\n",
      "\n",
      "\"You're naive and don't have a basic grasp on biology.\"\n",
      "\n",
      "\"Hey, Buttfuckle, when your wife is hip deep in the lime light destroying Republicans, will she still have time to vacuum your rug and wipe your bottom for you?\\n\\nWhat color will your nanny be?\"\n",
      "\n",
      "\"_go_eat_your_dog_you_pos_libtard\"\n",
      "\n",
      "\"It would only be news to you if it were reversed.  We get it now and get it with your every post.  You are a liberal puke that champions every liberal cause.  Got it glea.\"\n",
      "\n",
      "\"This.\\n\\nTravis, Cornell, Boyce--all them nukkas--I will NEVER forget nor forgive them for showing their black asses out.  Fuck them today, fuck them tomorrow, fuck them forever.\"\n",
      "\n",
      "\"do the research, I'll bet you don't know jacksquat about obies past.\"\n",
      "\n",
      "\"Another big tale coming from someone that has no ideal what in hell he is talking about.\"\n",
      "\n",
      "\"To the idiots posting on here who think racism is alright....\\nWell it is nor and if you were a victim of racism then you would know...\\nI feel like Balli too and I would kick off as well even if meant i got arrested....\\nDid you seem them dumb fkin poles and ukranians racist bigots.....\\nRacists have a place; sure they do.....in the fkin gutter....\\nAnd you know what would stop racism immediately or bring it to the forefront....if the racists were to throw swastikas at all the jewish players; I bet the football bodies would take note immediately.....\"\n",
      "\n",
      "\"Another idi0t who did not RTFA.\"\n",
      "\n",
      "\"WHAT THE FUCK SUN!!! WHY ARE YOU NOT POSTING MY COMMENTS!!! ILL TELL YOU WHAT I WONT FUCKING BOTHER WRITING MY OPINIONS THEN!!! ILL JUST FUCKING SAY NICE THINGS ABOUT ALL THESE SYSTEM ABUSING SCUM BAGS SHALL I.............DECENT\"\n",
      "\n",
      "\"Laffin?\\xa0 You don't laugh at anything you jihadist puke.\"\n",
      "\n",
      "\"Hey right winger \"just4thefax\", immigration isn't a \"wedge\" issue, it's a POLICY issue.\\n\\nGet a clue, moron.\"\n",
      "\n",
      "\"Well said. That you're all mouth and no facts has no bearing. Keep repeating it, though. Eventually you'll convince other idiots that you know something.\"\n",
      "\n",
      "\"its comedy u fuckin hater. nigga prolly is smashin her off camera\"\n",
      "\n",
      "\"Don't keep Nag waiting.\\xa0 It's hard on his knees.\\xa0 'Twisted'.\\xa0 Tonight.\\xa0 Bromance.\"\n",
      "\n",
      "\"FUCK ALL HEAT HATERS.YALL CAN SUCK MY DICK , I GOT NOTHING AGAINST OKC BUT FUCK EM TOOO !! WE WINNING THIS YEAR WADE COUNTY KING JAMES BITCHES , 305\"\n",
      "\n",
      "\"The way this fucking hoodrat is standing in that picture is boiling my blood. I'd like to meet this cunt in an alley for a few minutes. Selfish fucking twat.\"\n",
      "\n",
      "\"Thanks for confirming that conservative men only see their women as a pair of tits and an azz.\"\n",
      "\n",
      "\"No KKKonservative SunMedia and fans here are the true racist, they are here all day everyday spitting hateful venom, I just do the same in reverse. If you have a problem with me, then say something to them! Unless you are a KKKonservative racist too? hmm.\"\n",
      "\n",
      "\"Are you asking because your Mom wants to sell your anus to her crack dealer?\"\n",
      "\n",
      "\"Idiot: if they try to land the plane it is too heavy & will have a very bad accident & could kill everyone. Crawl back under your rock where it is dark. Better to dump fuel than kill everyone & more on the ground.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FNs,FN_idxs = get_texts(predictions,y_test.values, p_cls=0, cls=1,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = text.CountVectorizer()\n",
    "X_train_cts = cv.fit_transform(T_train).toarray()\n",
    "vocab_cts = X_train_cts.sum(axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPM = tf.transform(FPs).toarray()\n",
    "#tfs = FPM/tf.idf_\n",
    "FP_TFs = FPM.sum(axis=0)\n",
    "FP_idxs0 = FP_TFs.argsort()#[np.newaxis,:]\n",
    "FP_words = tf.get_feature_names_out()[FP_idxs0]\n",
    "# Counts for our FP words\n",
    "#FP_cts = vocab_cts[FP_idxs]\n",
    "#FN_cts = vocab_cts[FN_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', 'practically', 'practice', ..., 'are', 'your', 'you'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3616"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_cts[cv.vocabulary_[\"the\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_cts[cv.vocabulary_[\"all\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['plain', 'need', 'with', 'having', 'trash', 'son', 'so', 'we',\n",
       "       'mind', 'checking', 'people', 'assholes', 'xa0how', 'grade',\n",
       "       'messiah', 'break', 'culture', 'guy', 'palms', 'gay', 'spice',\n",
       "       'think', 'know', 'get', 'rat', 'kick', 'crap', 'own', 'getting',\n",
       "       'one', 'black', 'what', 'weak', 'information', 'bitch', 'funny',\n",
       "       'in', 'helping', 'then', 'hijack', 'had', 'this', 'troll', 'obama',\n",
       "       'all', 'sound', 'friggin', 'now', 'screen', 'fella', 'idiot', 'my',\n",
       "       'liberal', 'ha', 'how', 'no', 'do', 'liked', 'history', 'really',\n",
       "       'on', 'fucking', 'type', 'sad', 'others', 'as', 'money', 'not',\n",
       "       'go', 'was', 'have', 'loser', 'whore', 'sick', 'why', 'shit',\n",
       "       'back', 'll', 'comment', 're', 'said', 'here', 'to', 'yourself',\n",
       "       'hard', 'ignorant', 'up', 'of', 'ass', 'little', 'is', 'it',\n",
       "       'that', 'and', 'the', 'like', 'fuck', 'are', 'your', 'you'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100_FP_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checking',\n",
       " 'assholes',\n",
       " 'palms',\n",
       " 'spice',\n",
       " 'fella',\n",
       " 'xa0how',\n",
       " 'grade',\n",
       " 'hijack',\n",
       " 'helping',\n",
       " 'friggin',\n",
       " 'messiah',\n",
       " 'culture',\n",
       " 'rat',\n",
       " 'kick',\n",
       " 'plain',\n",
       " 'screen',\n",
       " 'liked',\n",
       " 'information',\n",
       " 'type',\n",
       " 'break',\n",
       " 'son',\n",
       " 'weak',\n",
       " 'trash',\n",
       " 'whore',\n",
       " 'sad',\n",
       " 'history',\n",
       " 'crap',\n",
       " 'funny',\n",
       " 'mind',\n",
       " 'ha',\n",
       " 'sick',\n",
       " 'troll',\n",
       " 'sound',\n",
       " 'loser',\n",
       " 'liberal',\n",
       " 'others',\n",
       " 'hard',\n",
       " 'having',\n",
       " 'ignorant',\n",
       " 'getting',\n",
       " 'black',\n",
       " 'guy',\n",
       " 'comment',\n",
       " 'bitch',\n",
       " 'own',\n",
       " 'yourself',\n",
       " 'll',\n",
       " 'money',\n",
       " 'said',\n",
       " 'gay',\n",
       " 'shit',\n",
       " 'idiot',\n",
       " 'fucking',\n",
       " 'need',\n",
       " 'ass',\n",
       " 'little',\n",
       " 'obama',\n",
       " 'had',\n",
       " 'back',\n",
       " 'then',\n",
       " 'really',\n",
       " 'here',\n",
       " 'fuck',\n",
       " 'now',\n",
       " 'why',\n",
       " 'go',\n",
       " 'think',\n",
       " 'know',\n",
       " 'how',\n",
       " 're',\n",
       " 'my',\n",
       " 'one',\n",
       " 'get',\n",
       " 'people',\n",
       " 'up',\n",
       " 'no',\n",
       " 'do',\n",
       " 'was',\n",
       " 'we',\n",
       " 'what',\n",
       " 'so',\n",
       " 'all',\n",
       " 'as',\n",
       " 'this',\n",
       " 'with',\n",
       " 'like',\n",
       " 'have',\n",
       " 'not',\n",
       " 'on',\n",
       " 'your',\n",
       " 'it',\n",
       " 'are',\n",
       " 'in',\n",
       " 'is',\n",
       " 'that',\n",
       " 'of',\n",
       " 'and',\n",
       " 'to',\n",
       " 'you',\n",
       " 'the']"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100_FP_words = FP_words[-100:]\n",
    "\n",
    "def get_cts(wd):\n",
    "    return vocab_cts[cv.vocabulary_[wd]]\n",
    "\n",
    "# Pyt most frequent words last\n",
    "sorted(top_100_FP_words,key=get_cts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 14147)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_std = tfs.argsort()\n",
    "freqs_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045318625119797826"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs[0][freqs_std[0][-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yourselves', 'yourselvesss', 'yoursilf', 'yousaid', 'youself',\n",
       "       'yout', 'youtube', 'ypu', 'yqdpnkvm', 'yr', 'yrs', 'yt', 'ytmnd',\n",
       "       'yum', 'yummy', 'yup', 'yuppie', 'yvszmvv2irw', 'yxvuisfr', 'za',\n",
       "       'zach', 'zama', 'zealand', 'zealot', 'zealots', 'zebra', 'zemun',\n",
       "       'zero', 'zeshkanen', 'zhang', 'zhark', 'ziggler', 'zimbabwe',\n",
       "       'zimmerman', 'zimmermans', 'zinya', 'zionist', 'zit', 'zito',\n",
       "       'zoanthropes', 'zobrist', 'zombies', 'zone', 'zoo', 'zoobs',\n",
       "       'zooey', 'zooming', 'zuccotti', 'zuckenberg', 'zuckerberg'],\n",
       "      dtype='<U95')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.inverse_transform(FP_idxs)[0][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', 'practically', 'practice', ..., 'are', 'your', 'you'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf.get_feature_names_out()[top_TFs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,  9344,  9345, ...,   824, 14091, 14085])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_TFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abs__',\n",
       " '__add__',\n",
       " '__array_priority__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__idiv__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmatmul__',\n",
       " '__rmul__',\n",
       " '__round__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '_add_dense',\n",
       " '_add_sparse',\n",
       " '_arg_min_or_max',\n",
       " '_arg_min_or_max_axis',\n",
       " '_ascontainer',\n",
       " '_asindices',\n",
       " '_binopt',\n",
       " '_bsr_container',\n",
       " '_container',\n",
       " '_coo_container',\n",
       " '_cs_matrix__get_has_canonical_format',\n",
       " '_cs_matrix__get_sorted',\n",
       " '_cs_matrix__set_has_canonical_format',\n",
       " '_cs_matrix__set_sorted',\n",
       " '_csc_container',\n",
       " '_csr_container',\n",
       " '_deduped_data',\n",
       " '_dia_container',\n",
       " '_divide',\n",
       " '_divide_sparse',\n",
       " '_dok_container',\n",
       " '_get_arrayXarray',\n",
       " '_get_arrayXint',\n",
       " '_get_arrayXslice',\n",
       " '_get_columnXarray',\n",
       " '_get_dtype',\n",
       " '_get_intXarray',\n",
       " '_get_intXint',\n",
       " '_get_intXslice',\n",
       " '_get_sliceXarray',\n",
       " '_get_sliceXint',\n",
       " '_get_sliceXslice',\n",
       " '_get_submatrix',\n",
       " '_imag',\n",
       " '_inequality',\n",
       " '_insert_many',\n",
       " '_is_array',\n",
       " '_lil_container',\n",
       " '_major_index_fancy',\n",
       " '_major_slice',\n",
       " '_maximum_minimum',\n",
       " '_min_or_max',\n",
       " '_min_or_max_axis',\n",
       " '_minor_index_fancy',\n",
       " '_minor_reduce',\n",
       " '_minor_slice',\n",
       " '_mul_dispatch',\n",
       " '_mul_multivector',\n",
       " '_mul_scalar',\n",
       " '_mul_sparse_matrix',\n",
       " '_mul_vector',\n",
       " '_prepare_indices',\n",
       " '_process_toarray_args',\n",
       " '_raise_on_1d_array_slice',\n",
       " '_real',\n",
       " '_rmul_dispatch',\n",
       " '_rsub_dense',\n",
       " '_scalar_binopt',\n",
       " '_set_arrayXarray',\n",
       " '_set_arrayXarray_sparse',\n",
       " '_set_dtype',\n",
       " '_set_intXint',\n",
       " '_set_many',\n",
       " '_set_self',\n",
       " '_setdiag',\n",
       " '_shape',\n",
       " '_sub_dense',\n",
       " '_sub_sparse',\n",
       " '_swap',\n",
       " '_validate_indices',\n",
       " '_with_data',\n",
       " '_zero_many',\n",
       " 'arcsin',\n",
       " 'arcsinh',\n",
       " 'arctan',\n",
       " 'arctanh',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'asformat',\n",
       " 'asfptype',\n",
       " 'astype',\n",
       " 'ceil',\n",
       " 'check_format',\n",
       " 'conj',\n",
       " 'conjugate',\n",
       " 'copy',\n",
       " 'count_nonzero',\n",
       " 'data',\n",
       " 'deg2rad',\n",
       " 'diagonal',\n",
       " 'dot',\n",
       " 'dtype',\n",
       " 'eliminate_zeros',\n",
       " 'expm1',\n",
       " 'floor',\n",
       " 'format',\n",
       " 'getH',\n",
       " 'get_shape',\n",
       " 'getcol',\n",
       " 'getformat',\n",
       " 'getmaxprint',\n",
       " 'getnnz',\n",
       " 'getrow',\n",
       " 'has_canonical_format',\n",
       " 'has_sorted_indices',\n",
       " 'indices',\n",
       " 'indptr',\n",
       " 'log1p',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'maxprint',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'minimum',\n",
       " 'multiply',\n",
       " 'ndim',\n",
       " 'nnz',\n",
       " 'nonzero',\n",
       " 'power',\n",
       " 'prune',\n",
       " 'rad2deg',\n",
       " 'reshape',\n",
       " 'resize',\n",
       " 'rint',\n",
       " 'set_shape',\n",
       " 'setdiag',\n",
       " 'shape',\n",
       " 'sign',\n",
       " 'sin',\n",
       " 'sinh',\n",
       " 'sort_indices',\n",
       " 'sorted_indices',\n",
       " 'sqrt',\n",
       " 'sum',\n",
       " 'sum_duplicates',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'toarray',\n",
       " 'tobsr',\n",
       " 'tocoo',\n",
       " 'tocsc',\n",
       " 'tocsr',\n",
       " 'todense',\n",
       " 'todia',\n",
       " 'todok',\n",
       " 'tolil',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'trunc']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(FPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 12,\n",
       " 14,\n",
       " 24,\n",
       " 68,\n",
       " 74,\n",
       " 77,\n",
       " 100,\n",
       " 108,\n",
       " 114,\n",
       " 129,\n",
       " 157,\n",
       " 170,\n",
       " 171,\n",
       " 196,\n",
       " 205,\n",
       " 246,\n",
       " 262,\n",
       " 332,\n",
       " 352,\n",
       " 358,\n",
       " 364,\n",
       " 382,\n",
       " 383,\n",
       " 392,\n",
       " 394,\n",
       " 492,\n",
       " 509,\n",
       " 525,\n",
       " 526,\n",
       " 530,\n",
       " 540,\n",
       " 596,\n",
       " 597,\n",
       " 652,\n",
       " 672,\n",
       " 684,\n",
       " 721,\n",
       " 791,\n",
       " 810,\n",
       " 818,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 857,\n",
       " 875,\n",
       " 877,\n",
       " 903,\n",
       " 911,\n",
       " 945,\n",
       " 951,\n",
       " 959,\n",
       " 966,\n",
       " 968,\n",
       " 980,\n",
       " 985]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbWpNTCCrWSq"
   },
   "source": [
    "### Other vocab experiments (using LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "PBoCp6dmrWSq",
    "outputId": "942f67c6-ccea-46db-ea24-dc2a267bacac"
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition as dec\n",
    "from sklearn import linear_model\n",
    "\n",
    "#Going to try what's known as LSI (Latent Semantic Indexing)\n",
    "reducer = dec.TruncatedSVD(n_components=200)\n",
    "# Using output of vectorizer\n",
    "X_train_reduced = reducer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "0dBNrutnrWSq",
    "outputId": "05245886-2d03-48cf-f11d-82e3c9c8b99a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3947, 200)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "oOGsEciErWSq",
    "outputId": "2310c998-a550-4cfa-b119-35494fef303b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3947, 16468)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LamDmRnhrWSq"
   },
   "outputs": [],
   "source": [
    "# You may need to do this if the import below fails\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "Sims = X_reduced@X_reduced.T\n",
    "ind2word = {i:wd for (wd,i) in tf.vocabulary_.items()}\n",
    "stopwords = stopwords.words('english')\n",
    "def get_nearest_neighbors (word,n=10):\n",
    "    word_ind = tf.vocabulary_[word]\n",
    "    sim_list = Sims[word_ind].argsort()[::-1]\n",
    "    ctr,result = 0,[]\n",
    "    for i in sim_list:\n",
    "        wd = ind2word[i]\n",
    "        if wd in stopwords:\n",
    "            continue\n",
    "        result.append(wd)\n",
    "        ctr += 1\n",
    "        if ctr == n:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG8mrcL2rWSq"
   },
   "source": [
    "Words that  tend to co-occur with \"idiot\" excluding stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPTPipT_rWSq",
    "outputId": "906dbc4a-731d-4bfa-8749-19b2723864f5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idiot',\n",
       " 'complete',\n",
       " 'fucking',\n",
       " 'xa0',\n",
       " 'sir',\n",
       " 'like',\n",
       " 'wrong',\n",
       " 'parents',\n",
       " 'apparently',\n",
       " 'even',\n",
       " 'really',\n",
       " 'stop',\n",
       " 'bleeping',\n",
       " 'jpcali',\n",
       " 'another',\n",
       " 'please',\n",
       " 'call',\n",
       " 'ezra',\n",
       " 'get',\n",
       " 'stupid']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nearest neighbors\n",
    "word,n =\"idiot\",20\n",
    "get_nearest_neighbors (word,n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "FivC0V6vrWSq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502172717Z</td>\n",
       "      <td>\"you are both morons and that is never happening\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528164814Z</td>\n",
       "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620142813Z</td>\n",
       "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528205648Z</td>\n",
       "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>20120515200734Z</td>\n",
       "      <td>\"You're all upset, defending this hipster band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3947 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult             Date  \\\n",
       "0          1  20120618192155Z   \n",
       "1          0  20120528192215Z   \n",
       "2          0              NaN   \n",
       "3          0              NaN   \n",
       "4          0  20120619094753Z   \n",
       "...      ...              ...   \n",
       "3942       1  20120502172717Z   \n",
       "3943       0  20120528164814Z   \n",
       "3944       0  20120620142813Z   \n",
       "3945       0  20120528205648Z   \n",
       "3946       0  20120515200734Z   \n",
       "\n",
       "                                                Comment  \n",
       "0                                  \"You fuck your dad.\"  \n",
       "1     \"i really don't understand your point.\\xa0 It ...  \n",
       "2     \"A\\\\xc2\\\\xa0majority of Canadians can and has ...  \n",
       "3     \"listen if you dont wanna get married to a man...  \n",
       "4     \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...  \n",
       "...                                                 ...  \n",
       "3942  \"you are both morons and that is never happening\"  \n",
       "3943  \"Many toolbars include spell check, like Yahoo...  \n",
       "3944  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...  \n",
       "3945  \"How about Felix? He is sure turning into one ...  \n",
       "3946  \"You're all upset, defending this hipster band...  \n",
       "\n",
       "[3947 rows x 3 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Insults_with_Naive_Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "94px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
