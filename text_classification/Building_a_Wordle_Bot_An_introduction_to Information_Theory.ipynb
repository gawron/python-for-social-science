{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e8459e",
   "metadata": {},
   "source": [
    "##  Missing: A section introducing Wordl and the concept of a WordlBot.\n",
    "\n",
    "In this notebook we move a significant part of the way toward building a Wordl-bot, a program that can play Wordl significantly better than you.  The  first thing we will focus on is a way of scoring candidate guesses.  Some guesses are  much better than others because on average they knock out more contenders than other candidates.  You need a scoring metric that measures this property: `metric(candidate, all_candidates) = score`.\n",
    "\n",
    "In the rest of this note we give a quick sketch of how a scoring metric works. The intuition\n",
    "we'll try to flesh out is that the best scoring metric knocks out more possibilities than others **on average**.\n",
    "But what does that mean?\n",
    "\n",
    "We saw in the last exercise that the guess \"audio\" left us with 85 candidates.  And we knew that because\n",
    "we executed the function `compatible_words`.  But maybe there was a guess that would\n",
    "have left us with only 50 candidates.  That would have been better. \n",
    "That's an insight, but not an immediately helpful one.\n",
    "How did we arrive at the information that one guess left us with 85 candidates?\n",
    "We computed that by knowing the coloring and then finding the\n",
    "words compatible with that coloring, but computing\n",
    "a coloring requires knowing the target.\n",
    "How do we score guesses when we don't know the target?\n",
    "\n",
    "Here's what we do. We hold the guess constant and we compute\n",
    "the coloring that guess would produce if each of the possible candidate words\n",
    "were the target.  That gives a set of colorings, and each coloring is associated with a set\n",
    "of candidate words (possibly a set of size 1).  This set of sets is called a **partition** and\n",
    "the key insight is that it should be partitions we score. Each of the sets in the partition  is\n",
    "one possible outcome for the set of words left after our guess.  So the\n",
    "simplest idea for scoring the guess is just to compute the average size\n",
    "of those partition sets.  The smaller the average size, the more words \n",
    "eliminated on average, and the better the guess.\n",
    "That's not bad.  You could build a pretty good WordlBot with that idea.\n",
    "\n",
    "But it turns out there is a better idea.  Our problem can be phrased:\n",
    "How do we measure the amount of information gained in going\n",
    "from the original set of candidates to a smaller set? \n",
    "If we can answer that question, then we just compute the\n",
    "average of the information gain for each set in the partition\n",
    "and we have a useful score for our guess.\n",
    "\n",
    "This is exactly the sort of question answered by a branch of computer science called **Information Theory**.\n",
    "The information theoretic approach often settles on the same answer\n",
    "as average partition size, but it's somewhat more principled\n",
    "and a lot more sophisticated because it is grounded in a \n",
    "mathematical theory for measuring the information of an event,\n",
    "which is in turn grounded in a very satisfying way in probability\n",
    "theory.  What follows is a **very brief** sketch of how \n",
    "something called (**information gain** (closely related to entropy)\n",
    "can be used to determine which guess achieves the average gain in\n",
    "information. \n",
    "\n",
    "#### An information-theoretic measure\n",
    "\n",
    "We have a set of candidate words.  Choose one of them as a guess.\n",
    "Consider each candidate word in turn and determine what\n",
    "coloring would result if that word were the target.\n",
    "For example, if the guess is \"mouth\" and the target \n",
    "word is \"snort\", the resulting coloring would be \"kykyk\" because\n",
    "target letters \"o\" and \"t\" are contained in the guess but not in\n",
    "the correct positions. The candidates \"toxic\", \"south\", and \"topic\" would all\n",
    "get the same coloring as the guess \"mouth\".\n",
    "So \"toxic\", \"south\",  and \"topic\" all\n",
    "go in the same coloring set as the guess \"mouth\".\n",
    "The colorings **partition** the set of candidates: every candidate gets a coloring and no candidate\n",
    "gets more than one.  Another way of saying it is that a guess partitions\n",
    "the set of candidates into a set of coloring classes.\n",
    "\n",
    "So how does this help score a guess? Well, it does help.\n",
    "Now consider comparing the partitions that result from different guesses.\n",
    "Better generally means more coloring classes with\n",
    "fewer members.  For example if the current set\n",
    "of candidate has 24 words, the best possible partitioning would be 24 classes with\n",
    "1 member each.  Because then no matter which candidate turns out to be right,\n",
    "we (errorless logical agents that we are)  would take take one look at the resulting\n",
    "coloring and we would be sure to guess the word on the next guess.\n",
    "\n",
    "So how do we quantify this?  Say there are 24 candidates. What's better,\n",
    "guess A which defines a partition with  2 groups of size 3, 4 groups of size 2, and 10 of size 1,\n",
    "or guess B which defines a partition with 1 group of size 3, 8 groups of size 2, and 5 of size 1?\n",
    "\n",
    "$$\n",
    "\\begin{array}{l|c|c}\n",
    "\\text{Name}& \\text{Number of groups} & \\text{Size}\\\\\n",
    "\\hline\n",
    "\\text{A} &  2 & 3\\\\\n",
    "         &  4 & 2\\\\\n",
    "         & 10 & 1 \\\\\n",
    "         \\hline\n",
    "\\text{B} & 1 & 3\\\\\n",
    "         & 8 & 2\\\\\n",
    "         & 5 & 1\\\\\n",
    "         \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Here's  the information theoretic way of looking at. it.\n",
    "When we make a guess, it earns a coloring, and now we have a new (and smaller!) set of candidate words.\n",
    "That's information gained.\n",
    "We score a  guess  by computing the amount of information\n",
    "gained for each of the colorings it might result in\n",
    "and then taking the average of all the possible information gains.\n",
    "\n",
    "Suppose, for example,  the game thus far has led us to a set of\n",
    "candidate words which determines partitioning A above.\n",
    "And suppose our next guess narrows things down to one of the two groups with 3 members.  Then a random\n",
    "guess has a 1/3 chance of being the target.  Suppose,\n",
    "on the other hand, we land in one of the 10 groups with 1 member.\n",
    "Then a random guess has a 100% chance of being the target.\n",
    "(we say the \"random\" guess has a probability of 1.0 of being right).\n",
    "Clearly we have gained more information by landing in the seond\n",
    "group than in the first.  We'd like to quantify the intuition that smaller\n",
    "candidate sets are better, and and we're to do that by using the fact\n",
    "that in smaller groups the probability of guessing the answer is higher.\n",
    "\n",
    "The key step is to define the amount of information gained by determining\n",
    "an event $e$ ($I(e)$) (in our case, finding out what the target is).\n",
    "\n",
    "$$\n",
    "I(e) = - log_{2} \\,p(e)\n",
    "$$\n",
    "\n",
    "Read this as $I(e)$, the information needed to determine $e$, \n",
    "is equal to the negative of the log of the probability of $e$ (to the base of 2).\n",
    "The unit is **bits**, binary choices.\n",
    "For example, suppose we are talking about the information needed to determine\n",
    "that a coin flip X has resulted in heads.  Assuming a fair coin, the probability of\n",
    "a heads outcome is 1/2, so\n",
    "$$\n",
    "I(X=\\text{heads}) = -log_{2} 1/2 = - (- 1) = 1 \\text{ bit}\n",
    "$$\n",
    "So 1 bit of information is needed to determine that X=heads.  Or suppose\n",
    "we are rolling a 6-sided die (Y).  The probability that the outcome\n",
    "is 3 is 1/6, so $I(Y=\\text{3})$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660c0381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.584962500721156"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "- math.log(1/6,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe48bd",
   "metadata": {},
   "source": [
    "Since logs of probabilities are negative, determining events with lower\n",
    "probability takes more information.   Hence the outcome of the die toss carries more information\n",
    "than the outcome of the coin flip.\n",
    "\n",
    "We can now quantify the intuitions\n",
    "above.  In the scenario above we start in a situation\n",
    "with 24 equally probable candidates.  That means the information\n",
    "needed to determine any one of them is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37bcca24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.584962500721157"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ic = - math.log(1/24,2)\n",
    "Ic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2e851",
   "metadata": {},
   "source": [
    "Then we assumed that the next guess narrowed things down to a group of 3 members.  That means the information\n",
    "needed to determine any one of them  went down: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c5b8aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5849625007211563"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Is = - math.log(1/3,2)\n",
    "Is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc8830",
   "metadata": {},
   "source": [
    "So the information gain achieved by the guess that got us from a set of equally likely candidates\n",
    "of size 24 to a set of candidates of size 3 is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb80a22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.000000000000001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.584962500721157 - 1.5849625007211563\n",
    "Ic - Is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267dfde",
   "metadata": {},
   "source": [
    "That's 3 bits, give or take a floating point twitch.  Note that when we went from a set of size 24 to a set\n",
    "of size 3, we shrunk the number of possibilities by a factor of 8.  The 3 bits comes from the fact\n",
    "that $\\log 8 = 3$.  Or to put it another way, halving the number of possibilities takes one bit\n",
    "(as it did when we flipped the coin and went from 2 possible outcomes to 1).  Halving the number of possibilties\n",
    "again takes another bit.  Halving them a third time takes a third bit.  So reducing the number of\n",
    "possibilities by a factor of 8 takes 3 bits.  \n",
    "\n",
    "That's the key idea about what 1 bit of information means: if we are in a state in which'there are N equally likely possibilities and we move to a state in which there are N/2 possibilities then we have received\n",
    "1 bit of information. If we want to end up  in  a state in which there is only 1 possibility\n",
    "it will take $\\log_{2} N$ bits ($\\log_{2} N$ successive halvings of the number of possibilities).\n",
    "\n",
    "What about reducing the number of equally likely possibilities by 1? The amount of information that will\n",
    "take depends on N, the total number of possibilities.  If N=2, it's 1 bit; if N is greater\n",
    "it will be less, in general it's\n",
    "\n",
    "$$\n",
    "\\log_{2} \\frac{N}{N-1} = \\log_{2} {N} - \\log_{2} ({N-1}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00482d",
   "metadata": {},
   "source": [
    "Let's summarize what we just did and introduce some\n",
    "notation.  Call the original set with 24 possibilities C.\n",
    "And let's use $I_{C}$ for the information required to determine any single member of C\n",
    "assuming they're all equally likely, that is,\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "\\text{I}_{C} &=& - \\log_{2} \\frac{1}{\\mid \\text{C} \\mid}\n",
    "             & = & \\log_{2} \\mid \\text{C} \\mid \\\\\n",
    "&=& \\log_{2}(24) = 4.584962500721157\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Then we assumed that the next guess narrowed things down to a set of 3 members.\n",
    "Let's call that set with 3 members $s$\n",
    "And let's use $I_{s}$ for the information required to determine 1 member of $s$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "\\text{I}_{s} &=& - \\log_{2} \\frac{1}{\\mid \\text{s} \\mid}\n",
    "             & = & \\log_{2} \\mid s \\mid \\\\\n",
    "&=& \\log_{2}(3) = 1.5849625007211563\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The information gained by that guess is:\n",
    "\n",
    "$$\n",
    "\\text{I}_{C}  - \\text{I}_{s} = 3 \\text{ bits}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5801d4b5",
   "metadata": {},
   "source": [
    "More generally, we define the Information Gain of a partition of a set C (the set\n",
    "of candidates) as the average of the information gains of \n",
    "narrowing the possibilities down to $s_{i}$ for each $s_{i}$\n",
    "in the partition:\n",
    "\n",
    "$$\n",
    "\\begin{array}{llcl}\n",
    "(a) &\\text{IG}(\\pi) & = & \\sum_{s \\in \\pi} p_{\\pi}\\,(s_{i})\\left \\lbrack \\text{I}_{C} - \\text{I}_{s_{i}} \\right \\rbrack \\\\\n",
    "(b)&                & = & \\sum_{s \\in \\pi} p_{\\pi}(s_{i})\\,\\text{I}_{C} - \\sum_{s \\in \\pi}p_{\\pi}(s_{i})\\cdot \\text{I}_{s_{i}}\\\\\n",
    "(c)&                & = & \\text{I}_{C} - \\sum_{s \\in \\pi}p_{\\pi}(s_{i})\\cdot \\text{I}_{s_{i}}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "Here,the weighting of each set in the average is given by its probability. The probability\n",
    "associated with a set  $s_{i}$ in the partition $\\pi$ is simply the probability that a randomly\n",
    "chosen member of C  will be in $s_{i}$.  Writing that out :\n",
    "\n",
    "$$\n",
    "p_{\\pi}(s_{i}) = \\frac{\\mid s_{i} \\mid}{\\mid \\text{C} \\mid}\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2698aaa8",
   "metadata": {},
   "source": [
    "Given our definition of information for a partition,\n",
    "let's calculate the information gain for the two cases above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fa80a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7295739585136224"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:  2 groups of size 3  4 groups of size 2 10 groups of size 1\n",
    "# the weighted gains \n",
    "GainsA = \\\n",
    "2 * -math.log(1/3,2) * 3/24 + \\\n",
    "4 * -math.log(1/2,2) * 2/24 + \\\n",
    "10 * -math.log(1/1,2) * 1/24\n",
    "GainsA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626440ec",
   "metadata": {},
   "source": [
    "So given this partioning  on average it will take .73 yes-no questions to determine target.  How is that possible?  Bear in mind that this is the **average** number of questions, and that for 10 of the partitions, amounting to \n",
    "10/24 of the probability mass, 0 questions are required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c99f1bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-math.log(1/1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc237187",
   "metadata": {},
   "source": [
    "Our final information gain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1d91417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8553885422075345"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ic - GainsA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1d2bb",
   "metadata": {},
   "source": [
    "Computing the average partition size for partition A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96eb9197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average partitio size\n",
    "(2*3 + 4*2 + 10 * 1)/16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f76daf",
   "metadata": {},
   "source": [
    "Turning to Partition B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6caff32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8647869792568111"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B:  1 groups of size 3  8 groups of size 2 5 groups of size 1\n",
    "# the weighted gains \n",
    "GainsB = \\\n",
    "1 * -math.log(1/3,2) * 3/24 + \\\n",
    "8 * -math.log(1/2,2) * 2/24 + \\\n",
    "5 * -math.log(1/1,2) * 1/24\n",
    "GainsB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b3fe106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7201755214643457"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ic - GainsB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19702519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7142857142857142"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average partition size for Partition B\n",
    "(1*3 + 8*2 + 5 * 1)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89fa845",
   "metadata": {},
   "source": [
    "So both metrics agree that Partition A should be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b081b4",
   "metadata": {},
   "source": [
    "#### Coding exercise\n",
    "\n",
    "**Part One**\n",
    "\n",
    "Write a function that scores a partition using the above definition of information\n",
    "gain for a partition.  Assume a partition is a container of\n",
    "sets.  Assume the sets are disjoint so that we have a genuine\n",
    "partition.  The function `score_partition` has the signature\n",
    "\n",
    "```python\n",
    "score_partition(partition)\n",
    "```\n",
    "\n",
    "and returns a floating point number that is the average information gain for\n",
    "that partition.\n",
    "\n",
    "Check your function  by seeing if you get the same answer for partitions\n",
    "like A and B as we got in  the calculations above.\n",
    "You will have to cook up your own partitions to do the test.\n",
    "Bear in mind that the set members don't matter.  What matters\n",
    "is the total number of members in C (24, for partitions A and B) and the sizes of the sets\n",
    "C is partitioned into.  Also bear in mind the partition sets\n",
    "must not overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44c109df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#  Put the definition of score_partition here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90499525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 3.8553885422075345\n",
      "B 3.7201755214643457\n"
     ]
    }
   ],
   "source": [
    "# Test your score_partition function on partition lkike partition A and partition B above\n",
    "# A:  2 groups of size 3  4 groups of size 2 10 groups of size 1\n",
    "#part_A = \n",
    "    \n",
    "print(\"A\", score_partition(part_A))\n",
    "\n",
    "# B:  1 groups of size 3  8 groups of size 2 5 groups of size 1\n",
    "#part_B = \n",
    "    \n",
    "print(\"B\", score_partition(part_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d59d9",
   "metadata": {},
   "source": [
    "**Part Two**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9042686",
   "metadata": {},
   "source": [
    "Take your partition scorer out for a spin by using it to find the best Wordl opening word.\n",
    "\n",
    "The situation:  All possible words are your candidates. Consider each as a potential guess\n",
    "and use it to partition the set of candidates.  Score your partition using your partition scorer,\n",
    "then rank words by partition score.  Report the highest ranking word.  For fun you should\n",
    "also report the lowest ranking word to answer this too infrequently asked question:  \"What is the worst possible\n",
    "Wordl opener?\" (Keep these words around; they make great target words to test on when you finish building\n",
    "your WordlBot).\n",
    "\n",
    "1. To partition the set of candidates, use the function `color_guess` provided to you below for free.\n",
    "   It handles the case of guesses that have duplicate letters correctly, which is a wrinkle\n",
    "   that introduces some slightly tricky logic. It also represents colorings as strings of length 5,\n",
    "   which means they can be keys in a dictionary.  Hint, hint. You will probably want to write a function\n",
    "   `get_partition` that loops through the candidates and assigns each to a partition set \n",
    "   with `color_guess`.  Perfectly oblique to everything, you may want to know about `defaultdict`\n",
    "   in the `collections` module, which makes it easy to create and maintain a dictionary whose\n",
    "   values are sets.  Its use is illustrated in the score_partition2 functoon below\n",
    "2. Use the wordlist provided to you in the next cell as your set of all possible 5-letter English words.\n",
    "   It's far from perfect, but it's a decent simulation of the unknown set of actual Wordl solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "081a56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3204 words loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_wordl_words():\n",
    "    df = pd.read_csv(\"wordlebot_words.txt\",header=None,names=[\"Word\"])\n",
    "    words = df[\"Word\"].values\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    print(len(words), \"words loaded\")\n",
    "    return words\n",
    "\n",
    "words = get_wordl_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "86329870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def color_guess (target, guess,wd_len=5):\n",
    "    \"\"\"\n",
    "    This version appears to fix the duplicate\n",
    "    letters bug.\n",
    "    \"\"\"\n",
    "    def wd_dict(wd):\n",
    "        dd = defaultdict(set)\n",
    "        for (i,l) in enumerate(wd):\n",
    "            dd[l].add(i)\n",
    "        return dd\n",
    "\n",
    "    coloring = list('k'*5)\n",
    "    tgt_dict,guess_dict = wd_dict(target),wd_dict(guess)\n",
    "    for let in set(guess):\n",
    "        gs_toks,tgt_toks = guess_dict[let],tgt_dict[let]\n",
    "        gs = gs_toks & tgt_toks\n",
    "        hits, max_hits = 0, len(tgt_toks)\n",
    "        for gs_i in gs:\n",
    "            coloring[gs_i] = 'g'\n",
    "            hits += 1\n",
    "        # We have yellows to assign\n",
    "        for gs_i in gs_toks - gs:\n",
    "            if tgt_toks and hits < max_hits:\n",
    "                coloring[gs_i] = 'y'\n",
    "                hits += 1\n",
    "    return ''.join((coloring))\n",
    "\n",
    "\n",
    "def score_partition2(partition):\n",
    "    \"\"\"\n",
    "    Alternative partition scoring metric.  You can use this a s placeholder for \n",
    "    the information theoretic version, while debugging.  It has the same type of inputs\n",
    "    and outputs.  Of course the nu,bers output are different.\n",
    "    \n",
    "    Uses average sz of partition sets as score\n",
    "    \"\"\"\n",
    "    Gains = 0\n",
    "    for s in partition:\n",
    "        Gains += len(s)\n",
    "    return Gains/len(partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116db6d",
   "metadata": {},
   "source": [
    "Solution code 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bc6af734",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a5c514",
   "metadata": {},
   "source": [
    "Worst and best words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e8dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f180acdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "135999c4",
   "metadata": {},
   "source": [
    "## Part Three\n",
    "\n",
    "You have almost written a WordlBot.  \n",
    "\n",
    "Want to finish?  Your Bot can operate in two modes.\n",
    "\n",
    "1.  Write a while loop that is given a value for an initial guess, then waits to have a coloring input before outputting the next guess. Each coloring that is input should update the set of current candidate words.  Updating the current candidates will be quite straightforward if you still have the partition induced by your last guess, especially if you implemented your partition as a dictionary that maps a coloring to the set of words compatible with that coloring. Where are the colorings coming from?  You're playing Wordl online. Your Wordl skill scores should skyrocket.\n",
    "2.  Give the Bot a target word (a solution) and let it play on its own (it can now generate its own colorings).  You can now compare your Wordl games with its games.  You can also search for the hardest Wordl words, the ones that make it take 4 or more guesses.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f059d329",
   "metadata": {},
   "source": [
    "## Given code (either from earlier in this NB or new for this problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5bd1269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def get_wordl_words():\n",
    "    url= \"https://raw.githubusercontent.com/gawron/python-for-social-science/refs/heads/master/\" +\\\n",
    "    \"text_classification/wordlebot_words.txt\"\n",
    "    #df = pd.read_csv(\"wordlebot_words.txt\",header=None,names=[\"Word\"])\n",
    "    df = pd.read_csv(url,header=None,names=[\"Word\"])\n",
    "    words = df[\"Word\"].values\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    print(len(words), \"words loaded\")\n",
    "    return words\n",
    "\n",
    "def get_partition_dict(word, candidates):\n",
    "    partition_dict = defaultdict(set)\n",
    "    for target in candidates:\n",
    "        partition_dict[color_guess (target, word)].add(target)\n",
    "    return partition_dict\n",
    "\n",
    "\n",
    "def color_guess (target, guess,wd_len=5):\n",
    "    \"\"\"\n",
    "    This version appears to fix the duplicate\n",
    "    letters bug.\n",
    "    \"\"\"\n",
    "    def wd_dict(wd):\n",
    "        dd = defaultdict(set)\n",
    "        for (i,l) in enumerate(wd):\n",
    "            dd[l].add(i)\n",
    "        return dd\n",
    "\n",
    "    coloring = list('k'*5)\n",
    "    tgt_dict,guess_dict = wd_dict(target),wd_dict(guess)\n",
    "    for let in set(guess):\n",
    "        gs_toks,tgt_toks = guess_dict[let],tgt_dict[let]\n",
    "        gs = gs_toks & tgt_toks\n",
    "        hits, max_hits = 0, len(tgt_toks)\n",
    "        for gs_i in gs:\n",
    "            coloring[gs_i] = 'g'\n",
    "            hits += 1\n",
    "        # We have yellows to assign\n",
    "        for gs_i in gs_toks - gs:\n",
    "            if tgt_toks and hits < max_hits:\n",
    "                coloring[gs_i] = 'y'\n",
    "                hits += 1\n",
    "    return ''.join((coloring))\n",
    "\n",
    "\n",
    "def score_partition(partition):\n",
    "    \"\"\"\n",
    "    Use information gain to score a partition.\n",
    "    \"\"\"\n",
    "    C = {s for S in partition for s in S}\n",
    "    N = len(C)\n",
    "    Ic = math.log(N,2)\n",
    "    Gains = 0\n",
    "    for S in partition:\n",
    "        sz = len(S)\n",
    "        Gains += (math.log(sz,2)*(sz/N))\n",
    "    return Ic-Gains\n",
    "\n",
    "def score_partition2(partition):\n",
    "    \"\"\"\n",
    "    Alternative scoring metric.  Use average sz of partition sets as score\n",
    "    \"\"\"\n",
    "    Gains = 0\n",
    "    for s in partition:\n",
    "        Gains += len(s)\n",
    "    return Gains/len(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe60cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e6722",
   "metadata": {},
   "source": [
    "## Introducing entropy\n",
    "\n",
    "In tnis small section we discuss the relationship of our definition\n",
    "of Information Gain to some standard concepts of Information Theory.\n",
    "\n",
    "We were able to greatly simplify the discussion of Information Gain above because we\n",
    "were dealing with uniform distributions, quite reasonably so.  From\n",
    "the point of view of a Wordl player every 5-letter has to have an equal\n",
    "chance being the target. So we did quite well by defining something\n",
    "we called $\\text{I}_{s}$, the information required to determine\n",
    "any single member of S, which depended only the size of S. \n",
    ">Note:  If you're a follower of the WordleBot analyses, you may have noticed that the Bot doesn't actually respect this assumption when assigning probabilities to solutions.  Some words have very low probabilities of being targets. It's not at all clear what justifies these numbers and I'm not sure how a Wordl player could know them, so I've left this out of the discussion.  As will become clear below, it made life simpler.\n",
    "\n",
    "But Information Theory is designed  to deal with a general\n",
    "probability distributions, where all the events may have different\n",
    "probabilities, so the average information required to determine\n",
    "an event for a set of events has a slightly more complicated definition.\n",
    "The term in Information Theory is **Entropy** and the definition of Entropy is:\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{x\\in X} p(x) \\log_{2} p(x)\n",
    "$$\n",
    "\n",
    "Your favorite Information Theory textbook\n",
    "will call X a random variable (for example, Cover and Thomas 1991 *Elements of Information\n",
    "Theory*),  but for our purposes\n",
    "we can think of it as a set of events or values with an associated\n",
    "probability distribution $p$. The entropy is the probability-weighted average\n",
    "of the information carried by events in X, also called the **Expected Information**.\n",
    "Let's apply this definition to our set of candidate words C,\n",
    "assuming as usual they all have an equal chance of being the target:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "H(C) &=& - \\sum_{x\\in C} p(x) \\log_{2} p(x)\\\\\n",
    "    & = & \\mid C \\mid \\cdot \\frac{1}{\\mid C \\mid} \\log_{2} \\mid C \\mid   \\\\\n",
    "     & = & \\log_{2} \\mid C \\mid  \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This of course is what we have been calling $I_{C}$.\n",
    "When all the candidates carry an equal amount of information\n",
    "the average information is the same as the information\n",
    "carried by any single candidate. This allowed us to \n",
    "compute the entropy of C just by computing the information carried by one\n",
    "candidate.\n",
    "\n",
    "The next important concept is conditional entropy.\n",
    "\n",
    "$$\n",
    "H(X \\mid Y) = \\sum_{y\\in Y} p(y) \\cdot H(X\\mid y)\n",
    "$$\n",
    "\n",
    "That is the expected conditional entropy of\n",
    "X given Y, or the average amount of information\n",
    "carried by an event in X when the value of Y is known.\n",
    "\n",
    "In our context we  have been talking about\n",
    "C and various sets in a partition of C,\n",
    "$\\pi$.  We computed the information in \n",
    "knowing the Partition Information (PI) as:\n",
    "\n",
    "$$\n",
    "\\text{PI} = \\sum_{s \\in \\pi}p_{\\pi}(s_{i})\\cdot \\text{I}_{s_{i}}\\\\\n",
    "$$\n",
    "\n",
    "And we now know $\\text{I}_{s_{i}}$ is equal to\n",
    "$H(s_{i})$.  Also, since $s_{i}$ is a subset of C,\n",
    "for any $x \\in C$, the probability of $x$\n",
    "being the target given that we know it is in $s_{i}$\n",
    "is the same as the probability of any x in\n",
    " $s_{i}$ the target, so we can write $H(s_{i})$ as \n",
    "$H(\\text{C}\\mid s_{i})$.  So we have:\n",
    "\n",
    "$$\n",
    "H(C \\mid \\pi)  = \\text{IP} = \\sum_{s \\in \\pi}p_{\\pi}(s_{i})\\cdot H(\\text{C}\\mid s_{i})\\\\\n",
    "$$\n",
    "\n",
    "That is, what we've been calling \n",
    "the PI is the Conditional Entropy of $C$ given $\\pi$,\n",
    "or the average information needed determine the target when it is known\n",
    "which of the partitions of $\\pi$ the target is in.\n",
    "\n",
    "When we plug these rewrites back into our definition of\n",
    "Information Gain, we get the standard definition of\n",
    "Information Gain\n",
    "\n",
    "$$\n",
    "IG(C,\\pi) = H(C) - H(C\\mid \\pi)\n",
    "$$\n",
    "\n",
    "This is also equivalent to the **Mutual Information** of C and $\\pi$,\n",
    "written $I(C;\\pi)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e09138",
   "metadata": {},
   "source": [
    "## fragment 1\n",
    "Spelling this out a bit:  the definition of the amount of information in an event with probability $p$ is\n",
    "\n",
    "$$\n",
    "- \\log p\n",
    "$$\n",
    "\n",
    "There are good reasons for this definition.  For example, teh minus sign\n",
    "tells us the more improbable an event is teh more information it carries.\n",
    "Also the infomeation\n",
    "carried by two independent events is just \n",
    "\n",
    "$$\n",
    "- \\log pq = - \\log p - \\log q.\n",
    "$$\n",
    "\n",
    "Thus, the information content of two independent events is just the sum of their respective\n",
    "information contents.  There are other motivations we leave out for lack of space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db2ab7",
   "metadata": {},
   "source": [
    "## fragment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ff69f",
   "metadata": {},
   "source": [
    "Entropy can be thought of as measuring how many binary choices on\n",
    "average we need to determine one event given a probability\n",
    "distribution over events. The units are called bits. The worst case is when all the events are\n",
    "equally probable.  The number above means that to discriminate among 24 equally probable candidates\n",
    "on average it will take 4.585 or so binary choices (answers to yes-no questions).  For example,\n",
    "supposing it's a number between 1 and 24, and supposing we get a series of yes answers, we\n",
    "might preoceed as follows\n",
    "\n",
    "1.  Is it less than 13?\n",
    "2.  Is it less than 7?\n",
    "3.  Is it less than 4?\n",
    "\n",
    "Whereupon we have 3 candidates left to choose among, which will take either 1 or 2 questions,\n",
    "and on average it will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9232a4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5849625007211563"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1787b94",
   "metadata": {},
   "source": [
    "questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac51542",
   "metadata": {},
   "source": [
    "If you know that the target is in a set\n",
    "of size 1, you are done.  It takes 0 binary questions to identify the target. That's a\n",
    "0-entropy distribution.  If you know the target is in a set of size 2, it takes\n",
    "1 yes-no question to identify the target.  That partition set has an entropy of 1 bit.\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "1 * 1 * (- \\log 1) & = & 0\\\\\n",
    "2* .5 * (- \\log .5) & = & 1 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "If you have a set of size 3, as we just saw, on average it takes 1.58 yes-no questions\n",
    "(= 1.58 bits). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
