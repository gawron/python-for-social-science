{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DeYXbqK-rRW"
   },
   "source": [
    "## Classifying text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MejilF82-rRZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV as gs\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.naive_bayes as nb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn to applying machine learning classification methods to text. There are\n",
    "no new principles at stake.  In principle, everything is the same as it was for\n",
    "learning how to classify irises.\n",
    "\n",
    "1.  We need to find labeled data; each of the exemplars in the data should be represented with a fixed set of features.  \n",
    "2. We need to split our data and training and test data.  \n",
    "3. We need to train learner on the training data and evaluate it (test it) it on the test data.\n",
    "\n",
    "The problem is that text data is not in a form  that is compatible with\n",
    "what we have learned about classifiers.  The text must be put in a suitable\n",
    "form before a linear model; can be trained on it. \n",
    "\n",
    "**Training**\n",
    "\n",
    "1.  Labeled data must be loaded (into Python).  It should be a sequence of documents T accompanied by a sequence of labels L. \n",
    "2.  Split T and L into training and test groups, yielding T1 and T2; as well as and L1 and L2.\n",
    "2.  Train or a **feature model** on the training data T1 (or in scikit learn terminology **fit** the model **to** the training data).  The feature model inputs the text sequence and outputs a **term-document** matrix suitable for training a linear classifier.  The feature model is called a **vectorizer**\n",
    "(because it turns a document into a vector, a column of numbers).\n",
    "3.  Using the trained vectorizer, transform T1 into a term document matrix M1.\n",
    "4.  Train a linear model $\\mu$ on M1 and L1.\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "1.  Transform the test data T2 into a term document matrix M2 using the vectorizer fit during step 2 of training;  in particular this means if there are words in the T2 data that were never seen during training, they are ignored in building M2.\n",
    "2.  Use $\\mu$  to classify the texts represented in M2; that is produce a set of predicted labels P2.\n",
    "3.  Compare the actual labels L2 with the predicted labels P2 using standard evaluation metrics such as precision, accuracy, and recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qildTjvw-rRb"
   },
   "source": [
    "## Review the steps with insult detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28jZSTW_-rRb"
   },
   "source": [
    "We looked at the insult detection data in  the text classification notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28jZSTW_-rRb"
   },
   "source": [
    "### Training step 1: Loading the data\n",
    "\n",
    "Let's load the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MtGQlB1q-rRc"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "site = 'https://raw.githubusercontent.com/gawron/python-for-social-science/master/'\\\n",
    "'text_classification/'\n",
    "#site = 'https://gawron.sdsu.edu/python_for_ss/course_core/book_draft/_static/'\n",
    "df = pd.read_csv(os.path.join(site,\"troll.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3ncLUYx-rRc"
   },
   "source": [
    "Each row is a comment  taken from a blog or online forum. There are three columns: whether the comment is insulting (1) or not (0), the date, and the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pFeNaW-m-rRd",
    "outputId": "0d5e5e36-697f-4fd8-ff00-ef8d164a3c35"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502172717Z</td>\n",
       "      <td>\"you are both morons and that is never happening\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528164814Z</td>\n",
       "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620142813Z</td>\n",
       "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528205648Z</td>\n",
       "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>20120515200734Z</td>\n",
       "      <td>\"You're all upset, defending this hipster band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult             Date  \\\n",
       "3942       1  20120502172717Z   \n",
       "3943       0  20120528164814Z   \n",
       "3944       0  20120620142813Z   \n",
       "3945       0  20120528205648Z   \n",
       "3946       0  20120515200734Z   \n",
       "\n",
       "                                                Comment  \n",
       "3942  \"you are both morons and that is never happening\"  \n",
       "3943  \"Many toolbars include spell check, like Yahoo...  \n",
       "3944  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...  \n",
       "3945  \"How about Felix? He is sure turning into one ...  \n",
       "3946  \"You're all upset, defending this hipster band...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ibxc3U3K-rRh"
   },
   "source": [
    "Now we define the text sequences $\\mathbf{T}$ and the label sequence  $\\mathbf{L}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wty7pj42-rRh",
    "outputId": "4fe0a03e-69fd-48f7-f8c3-661e9a5225da"
   },
   "outputs": [],
   "source": [
    "T = df['Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-8xJstOo-rRi"
   },
   "outputs": [],
   "source": [
    "L = df['Insult']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Split the data and labels into training and test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1, T2, L1, L2 = train_test_split(T,L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 and 4:  Fit the feature model (vectorizer) to the training data and Transform  it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "H-fPVjYV-rRl"
   },
   "outputs": [],
   "source": [
    "tf = text.TfidfVectorizer()\n",
    "# Scikit learn has one function that does both fitting and transforming.\n",
    "# M1 is the transformed data\n",
    "# tf is the trained feature model (which will be used to transform the test data)\n",
    "M1 = tf.fit_transform(T1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0wp6RbS-rRo"
   },
   "source": [
    "### Step 5 Training the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88mG_1C0-rRo"
   },
   "source": [
    "Now, we are going to train a classifier as usual. We first split the data into a train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf-3XQDN-rRo"
   },
   "source": [
    "We use a **Bernoulli Naive Bayes classifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "uj7vvTS--rRo"
   },
   "outputs": [],
   "source": [
    "# Create classifer\n",
    "bnb =nb.BernoulliNB()\n",
    "\n",
    "# Fit (train) the classifier  using the training data and labels\n",
    "bnb.fit(M1, L1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the classifier, first using accuracy (what `.score()` returns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rewbP2vT-rRp",
    "outputId": "ccbb8785-df2f-4000-8095-8de2e57c6f3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7588652482269503"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize the test data using the vectorizer trained on T1\n",
    "# Notice we DONT call .fit_transform() because that would retrain the vectorizer on the test data\n",
    "# We call .transform() using the trained model to transform the new data.\n",
    "# Words not seen during training will be ignored.\n",
    "M2 = tf.transform(T2)\n",
    "# Classify the data using the trained classisifer and report the accuracy\n",
    "bnb.score(M2, L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p38bf79W-rRp"
   },
   "source": [
    "Now try re-executing steps 2 through 5.  (Just re-execute the cells)  The results should be the same, right?\n",
    "\n",
    "Well, are they?  \n",
    "\n",
    "What happens:  each training test split produces a different set of test data.  Sometimes the test is harder.\n",
    "Sometimes it's easier.  Or looking at it another way:  Sometimes the training data is a better preparation for the test than others.  \n",
    "\n",
    "To get a realistic view of how our classifier is doing we take the average performance on a  number of \n",
    "train/test splits.  This is called **cross validation**.  We return to that point below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using all three evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's get more evaluation numbers, in particular precision and recall.  We do\n",
    "that by calling a method that returns the predicted labels P2, so we can compare\n",
    "L2 and P2 using different evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76 Precision: 0.14 Recall: 0.95\n"
     ]
    }
   ],
   "source": [
    "P2 = bnb.predict(M2)\n",
    "scores = np.array([accuracy_score(P2, L2),\n",
    "                   precision_score(P2, L2),\n",
    "                   recall_score(P2, L2)])\n",
    "print(f'Accuracy: {scores[0]:.2f} Precision: {scores[1]:.2f} Recall: {scores[2]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy is a bit misleading.  There is a serious precision problem.\n",
    "\n",
    "What does that mean in the setting of insult detection?  It means the BNB classifier is a little too\n",
    "eager to call something an insult.  When it flags something as an insult, it\n",
    "is right only 14% of the time.\n",
    "\n",
    "Why would that be?  Think about how the model is trained and what its weakness might be.\n",
    "This is what it means to try to interpret or discuss a model's performance.  Zoom\n",
    "in the model's weakness. Talk about where that weakness comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-bt36GeMpFW"
   },
   "source": [
    "#### Basic train and test loop\n",
    "\n",
    "How to get the average of a number of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwsayH6F-rRp",
    "outputId": "a9f16658-90d0-4899-cd1e-3148ada56515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 Precision: 0.16 Recall: 0.86\n"
     ]
    }
   ],
   "source": [
    "def split_fit_and_eval(T,L,test_size=.2):\n",
    "    # This code just collects together the training steps 2-5 + the eval\n",
    "    # That is, It does one training,test., eval run\n",
    "    (T1, T2, L1, L2) = train_test_split(T, L, test_size=test_size)\n",
    "    tf = text.TfidfVectorizer()\n",
    "    M1 = tf.fit_transform(T1)\n",
    "    bnb = nb.BernoulliNB()\n",
    "    bnb.fit(M1,L1)\n",
    "    # .fit(), ..fit_transform()\n",
    "    M2 = tf.transform(T2)\n",
    "    P2 = bnb.predict(M2)\n",
    "    return np.array([accuracy_score(P2,L2),\n",
    "                     precision_score(P2, L2),\n",
    "                     recall_score(P2,L2)])\n",
    "\n",
    "# Split, Train, test and eval 10 times\n",
    "num_runs = 10\n",
    "# an accumulator for acc.,pre.,and rec.\n",
    "scores = np.zeros((3,))\n",
    "for test_run in range(num_runs):\n",
    "    scores += split_fit_and_eval(T,L)\n",
    "# Compute the average of the num_runs runs for all metrics\n",
    "normed_stats = scores/num_runs\n",
    "\n",
    "print(f'Accuracy: {normed_stats[0]:.2f} Precision: {normed_stats[1]:.2f} Recall: {normed_stats[2]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "zaE0KDK5-rRr"
   },
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg50G61U-rRr"
   },
   "source": [
    "Read the on line book draft chapter about text classification and and especially\n",
    "about  movie review data.  Note that you will be using a different classifier implementation (`scikit_learn`) than the one used in the book\n",
    "(`nltk`).  Therefore, when it comes to writing code for training the calssifier. focus on the code examples in this notebook, which use `scikit_learn`.\n",
    "\n",
    "Try using two classifiers on the movie review data, the one used in the textbook, an SVM, and\n",
    "the Bernoulli Naive Bayes model used above. Be sure\n",
    "to stick with  scikit learn (it has an SVM implementation).\n",
    "Some points of emphasis;\n",
    "\n",
    "1.  Be sure to get the average of at runs  least 10 runs for **both** classifiers.\n",
    "2.  Be sure to get average accuracy, precision, and recall for both classifiers on those multiple runs. You will probably find `split_fit_and_eval` defined above useful, but you may need to modify it.\n",
    "3.  For your first discussion post turn in the new code you wrote, including the code that labels and shuffles the data (discussed further below).  If you have to do a new import, show that. If you have to rewrite `split_fit_and_eval`, turn in the new version.  Also show the output, which should be a single line giving the accuracy, prcision, and recall.\n",
    "4.  Discuss which classifier does better.  Discuss which metric the best classifier does the worst at and speculate as to why (this will require reviewing the definitions of precision and recall and thinking about what they mean in a movie review setting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_wazBGWTsS4"
   },
   "source": [
    "#### Help with getting the movie reviews data.\n",
    "\n",
    "Execute the next two cells to get the movie review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvlkaI1x-rRr",
    "outputId": "5a8c9f38-c83f-49c9-97cd-0fa65d541ca4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/gawron/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nQdcr4aH_dP",
    "outputId": "2e1e6c67-ca8b-44b3-f5c0-9ac9adfd72d6"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "\n",
    "def get_file_strings (corpus, file_ids):\n",
    "    return [corpus.raw(file_id) for file_id in file_ids]\n",
    "\n",
    "data = dict(pos = mr.fileids('pos'),\n",
    "            neg = mr.fileids('neg'))\n",
    "\n",
    "pos_file_ids = data['pos']\n",
    "neg_file_ids = data['neg']\n",
    "\n",
    "# Get all the positive and negative reviews.\n",
    "pos_file_reviews = get_file_strings (mr, pos_file_ids)\n",
    "neg_file_reviews = get_file_strings (mr, neg_file_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PY6_uHMJvqC"
   },
   "source": [
    "Each review is a string.  In principle, a list of strings like `pos_file_reviews`  can be passed to `text.TfidfVectorizer()` via the `fit_transform` method to train a vectorizer for machine learning.\n",
    "You could code that up.\n",
    "\n",
    "What you'd really like to do is use `split_fit_and_eval`, defined above, which does a lot of the work for you.\n",
    "\n",
    "But hold on. You have a coding problem. You don't have  a sequence of documents and labels.  Instead you have\n",
    "one sequence of positive documents  and another sequence of negative documents.  \n",
    "\n",
    "So you will need to turn those two sequences into a sequence of documents and a sequence of labels\n",
    "because that's what `split_fit_and_eval` wants.  You also want the doc sequence\n",
    "to contain a random mixture of positive and negative documents, because some machine\n",
    "learning algorithms are sensitive to the order in which training data is presented to\n",
    "them.\n",
    "\n",
    "The next cell does **not** do that for you.  But it illustrates an approach using \n",
    "two sets of English letters in place of two sets of English documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCdgmoToRV8z",
    "outputId": "13bdbe23-796e-44db-90e6-abd0277a6bed"
   },
   "outputs": [],
   "source": [
    "# Lets work on documents\n",
    "# There are 2 classes, pos and ne\n",
    "\n",
    "from random import shuffle\n",
    "from string import ascii_lowercase\n",
    "\n",
    "\n",
    "# Now get pairs of letters and labels\n",
    "pos_pairs = [(rev,'pos') for rev in pos_file_reviews]\n",
    "neg_pairs = [(rev,'neg') for rev in neg_file_reviews]\n",
    "\n",
    "\n",
    "\n",
    "###########  Shuffling  ###########################\n",
    "# Way too orderly, the classes arent mixed yet.\n",
    "data = pos_pairs + neg_pairs\n",
    "shuffle(data)\n",
    "###################  Now they're shuffled! ###############\n",
    "\n",
    "# Separate the letters from their labels\n",
    "revs, lbls = zip(*data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vectorize_and_fit(docs,labels,clf,pos_label='pos'):\n",
    "    global predictions\n",
    "    T_train,T_test, y_train,y_test = train_test_split(docs,labels)\n",
    "    tf = text.TfidfVectorizer()\n",
    "    X_train = tf.fit_transform(T_train)\n",
    "    clf_inst = clf()\n",
    "    clf_inst.fit(X_train, y_train)\n",
    "    X_test = tf.transform(T_test)\n",
    "    predictions = clf_inst.predict(X_test)\n",
    "    return precision_score(y_test,predictions,pos_label=pos_label), \\\n",
    "           recall_score(y_test, predictions,pos_label=pos_label), \\\n",
    "           accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution Partial:  Running just one classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB                                        Precision 0.877 Recall 0.676  Accuracy: 0.792\n"
     ]
    }
   ],
   "source": [
    "num_test_runs = 10\n",
    "scores = np.zeros((3,))\n",
    "for test_run in range(num_test_runs):\n",
    "    clf = nb.BernoulliNB\n",
    "    scores += split_vectorize_and_fit(revs,lbls,clf)\n",
    "\n",
    "p_score,r_score,a_score = scores/num_test_runs\n",
    "print(f\"{'BernoulliNB':<50} Precision {p_score:.3f} Recall {r_score:.3f}  Accuracy: {a_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clfs = {\"Logistic Regression\": linear_model.LogisticRegression,\n",
    "        \"Ridge Classifier\":   linear_model.RidgeClassifier,\n",
    "        \"Multinomial NB\" : nb.MultinomialNB,\n",
    "        \"Passive Aggressive Classifier\": linear_model.PassiveAggressiveClassifier,\n",
    "        \"SVM\": LinearSVC,\n",
    "       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing multiple classifiers in one loop with 10 training test splits for each classifier.\n",
    "Reporting average precision, recall and accuracy scores for the 10 test runs for each of\n",
    "the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:                                                    Precision   Recall  Accuracy \n",
      "Clf: Logistic Regression                                0.815        0.836   0.820\n",
      "\n",
      "Clf: Ridge Classifier                                   0.841        0.851   0.842\n",
      "\n",
      "Clf: Multinomial NB                                     0.858        0.722   0.795\n",
      "\n",
      "Clf: Passive Aggressive Classifier                      0.853        0.858   0.853\n",
      "\n",
      "Clf: SVM                                                0.847        0.873   0.860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_test_runs = 10\n",
    "scores = np.zeros((3,))\n",
    "\n",
    "print(f\"Clf: {'':<50} Precision   Recall  Accuracy \")\n",
    "for clf in clfs:\n",
    "    for test_run in range(num_test_runs):\n",
    "        scores += split_vectorize_and_fit(revs,lbls,clfs[clf])\n",
    "    p_score,r_score,a_score = scores/num_test_runs\n",
    "    print(f\"Clf: {clf:<50} {p_score:.3f}        {r_score:.3f}   {a_score:.3f}\")\n",
    "    scores = np.zeros((3,))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multinomial Naive Bayes classifier had the best Precision Score, but it also had the worst accuracy\n",
    "and by far the worst recall.  This is an excellent example of the precision-recall trade-off. The classifier\n",
    "achieves its high precision at the cost of a lot of false negatives, lowering its recall and accuracy.\n",
    "\n",
    "The highest Accuracy and the best recall was achieved by the Passive Aggressive Classifier, with the\n",
    "Ridge Classifier tying for best recall and just a smidge behind in accuracy. Except for the Multinomial Naive Bayes\n",
    "classifier, the precision scores all lagged behind their recall scores, meaning most of the classifiers\n",
    "traded in a little precision for better recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Insults_with_Naive_Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "94px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
