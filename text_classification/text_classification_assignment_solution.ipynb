{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DeYXbqK-rRW"
   },
   "source": [
    "## Classifying text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MejilF82-rRZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV as gs\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.naive_bayes as nb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn to applying machine learning classification methods to text. There are\n",
    "no new principles at stake.  In principle, everything is the same as it was for\n",
    "learning how to classify irises.\n",
    "\n",
    "1.  We need to find labeled data; each of the exemplars in the data should be represented with a fixed set of features.  \n",
    "2. We need to split our data and training and test data.  \n",
    "3. We need to train learner on the training data and evaluate it (test it) it on the test data.\n",
    "\n",
    "The problem is that text data is not in a form  that is compatible with\n",
    "what we have learned about classifiers.  The text must be put in a suitable\n",
    "form before a linear model; can be trained on it. \n",
    "\n",
    "**Training**\n",
    "\n",
    "1.  Labeled data must be loaded (into Python).  It should be a sequence of documents T accompanied by a sequence of labels L. \n",
    "2.  Split T and L into training and test groups, yielding T1 and T2; as well as and L1 and L2.\n",
    "2.  Train or a **feature model** on the training data T1 (or in scikit learn terminology **fit** the model **to** the training data).  The feature model inputs the text sequence and outputs a **term-document** matrix suitable for training a linear classifier.  The feature model is called a **vectorizer**\n",
    "(because it turns a document into a vector, a column of numbers).\n",
    "3.  Using the trained vectorizer, transform T1 into a term document matrix M1.\n",
    "4.  Train a linear model $\\mu$ on M1 and L1.\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "1.  Transform the test data T2 into a term document matrix M2 using the vectorizer fit during step 2 of training;  in particular this means if there are words in the T2 data that were never seen during training, they are ignored in building M2.\n",
    "2.  Use $\\mu$  to classify the texts represented in M2; that is produce a set of predicted labels P2.\n",
    "3.  Compare the actual labels L2 with the predicted labels P2 using standard evaluation metrics such as precision, accuracy, and recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qildTjvw-rRb"
   },
   "source": [
    "## Review the steps with insult detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28jZSTW_-rRb"
   },
   "source": [
    "We looked at the insult detection data in  the text classification notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28jZSTW_-rRb"
   },
   "source": [
    "### Training step 1: Loading the data\n",
    "\n",
    "Let's load the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MtGQlB1q-rRc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "site = 'https://raw.githubusercontent.com/gawron/python-for-social-science/master/'\\\n",
    "'text_classification/'\n",
    "#site = 'https://gawron.sdsu.edu/python_for_ss/course_core/book_draft/_static/'\n",
    "df = pd.read_csv(os.path.join(site,\"troll.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3ncLUYx-rRc"
   },
   "source": [
    "Each row is a comment  taken from a blog or online forum. There are three columns: whether the comment is insulting (1) or not (0), the date, and the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pFeNaW-m-rRd",
    "outputId": "0d5e5e36-697f-4fd8-ff00-ef8d164a3c35"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502172717Z</td>\n",
       "      <td>\"you are both morons and that is never happening\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528164814Z</td>\n",
       "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620142813Z</td>\n",
       "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528205648Z</td>\n",
       "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>20120515200734Z</td>\n",
       "      <td>\"You're all upset, defending this hipster band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult             Date  \\\n",
       "3942       1  20120502172717Z   \n",
       "3943       0  20120528164814Z   \n",
       "3944       0  20120620142813Z   \n",
       "3945       0  20120528205648Z   \n",
       "3946       0  20120515200734Z   \n",
       "\n",
       "                                                Comment  \n",
       "3942  \"you are both morons and that is never happening\"  \n",
       "3943  \"Many toolbars include spell check, like Yahoo...  \n",
       "3944  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...  \n",
       "3945  \"How about Felix? He is sure turning into one ...  \n",
       "3946  \"You're all upset, defending this hipster band...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ibxc3U3K-rRh"
   },
   "source": [
    "Now we define the text sequences $\\mathbf{T}$ and the label sequence  $\\mathbf{L}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wty7pj42-rRh",
    "outputId": "4fe0a03e-69fd-48f7-f8c3-661e9a5225da"
   },
   "outputs": [],
   "source": [
    "T = df['Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-8xJstOo-rRi"
   },
   "outputs": [],
   "source": [
    "L = df['Insult']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Split the data and labels into training and test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1, T2, L1, L2 = train_test_split(T,L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 and 4:  Fit the feature model (vectorizer) to the training data and Transform  it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "H-fPVjYV-rRl"
   },
   "outputs": [],
   "source": [
    "tf = text.TfidfVectorizer()\n",
    "# Scikit learn has one function that does both fitting and transforming.\n",
    "# M1 is the transformed data\n",
    "# tf is the trained feature model (which will be used to transform the test data)\n",
    "M1 = tf.fit_transform(T1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0wp6RbS-rRo"
   },
   "source": [
    "### Step 5 Training the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88mG_1C0-rRo"
   },
   "source": [
    "Now, we are going to train a classifier as usual. We first split the data into a train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf-3XQDN-rRo"
   },
   "source": [
    "We use a **Bernoulli Naive Bayes classifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uj7vvTS--rRo"
   },
   "outputs": [],
   "source": [
    "# Create classifer\n",
    "bnb =nb.BernoulliNB()\n",
    "\n",
    "# Fit (train) the classifier  using the training data and labels\n",
    "bnb.fit(M1, L1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the classifier, first using accuracy (what `.score()` returns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rewbP2vT-rRp",
    "outputId": "ccbb8785-df2f-4000-8095-8de2e57c6f3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7507598784194529"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize the test data using the vectorizer trained on T1\n",
    "# Notice we DONT call .fit_transform() because that would retrain the vectorizer on the test data\n",
    "# We call .transform() using the trained model to transform the new data.\n",
    "# Words not seen during training will be ignored.\n",
    "M2 = tf.transform(T2)\n",
    "# Classify the data using the trained classisifer and report the accuracy\n",
    "bnb.score(M2, L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p38bf79W-rRp"
   },
   "source": [
    "Now try re-executing steps 2 through 5.  (Just re-execute the cells)  The results should be the same, right?\n",
    "\n",
    "Well, are they?  \n",
    "\n",
    "What happens:  each training test split produces a different set of test data.  Sometimes the test is harder.\n",
    "Sometimes it's easier.  Or looking at it another way:  Sometimes the training data is a better preparation for the test than others.  \n",
    "\n",
    "To get a realistic view of how our classifier is doing we take the average performance on a  number of \n",
    "train/test splits.  This is called **cross validation**.  We return to that point below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using all three evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's get more evaluation numbers, in particular precision and recall.  We do\n",
    "that by calling a method that returns the predicted labels P2, so we can compare\n",
    "L2 and P2 using different evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75 Precision: 0.12 Recall: 0.79\n"
     ]
    }
   ],
   "source": [
    "P2 = bnb.predict(M2)\n",
    "scores = np.array([accuracy_score(P2, L2),\n",
    "                   precision_score(P2, L2),\n",
    "                   recall_score(P2, L2)])\n",
    "print(f'Accuracy: {scores[0]:.2f} Precision: {scores[1]:.2f} Recall: {scores[2]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy is a bit misleading.  There is a serious precision problem.\n",
    "\n",
    "What does that mean in the setting of insult detection?  It means the BNB classifier is a little too\n",
    "eager to call something an insult.  When it flags something as an insult, it\n",
    "is right only 14% of the time.\n",
    "\n",
    "Why would that be?  Think about how the model is trained and what its weakness might be.\n",
    "This is what it means to try to interpret or discuss a model's performance.  Zoom\n",
    "in the model's weakness. Talk about where that weakness comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-bt36GeMpFW"
   },
   "source": [
    "#### Basic train and test loop\n",
    "\n",
    "How to get the average of a number of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwsayH6F-rRp",
    "outputId": "a9f16658-90d0-4899-cd1e-3148ada56515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 Precision: 0.14 Recall: 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "def split_fit_and_eval(T,L,test_size=.2):\n",
    "    # This code just collects together the training steps 2-5 + the eval\n",
    "    # That is, It does one training,test., eval run\n",
    "    (T1, T2, L1, L2) = train_test_split(T, L, test_size=test_size)\n",
    "    tf = text.TfidfVectorizer()\n",
    "    M1 = tf.fit_transform(T1)\n",
    "    bnb = nb.BernoulliNB()\n",
    "    bnb.fit(M1,L1)\n",
    "    # .fit(), ..fit_transform()\n",
    "    M2 = tf.transform(T2)\n",
    "    P2 = bnb.predict(M2)\n",
    "    return np.array([accuracy_score(P2,L2),\n",
    "                     precision_score(P2, L2),\n",
    "                     recall_score(P2,L2)])\n",
    "\n",
    "# Split, Train, test and eval 10 times\n",
    "num_runs = 10\n",
    "# an accumulator for acc.,pre.,and rec.\n",
    "scores = np.zeros((3,))\n",
    "for test_run in range(num_runs):\n",
    "    scores += split_fit_and_eval(T,L)\n",
    "# Compute the average of the num_runs runs for all metrics\n",
    "normed_stats = scores/num_runs\n",
    "\n",
    "print(f'Accuracy: {normed_stats[0]:.2f} Precision: {normed_stats[1]:.2f} Recall: {normed_stats[2]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "zaE0KDK5-rRr"
   },
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg50G61U-rRr"
   },
   "source": [
    "Read the on line book draft chapter about text classification and and especially\n",
    "about  movie review data.  Note that you will be using a different classifier implementation (`scikit_learn`) than the one used in the book\n",
    "(`nltk`).  Therefore, when it comes to writing code for training the calssifier. focus on the code examples in this notebook, which use `scikit_learn`.\n",
    "\n",
    "Try using two classifiers on the movie review data, the one used in the textbook, an SVM, and\n",
    "the Bernoulli Naive Bayes model used above. Be sure\n",
    "to stick with  scikit learn (it has an SVM implementation).\n",
    "Some points of emphasis;\n",
    "\n",
    "1.  Be sure to get the average of at runs  least 10 runs for **both** classifiers.\n",
    "2.  Be sure to get average accuracy, precision, and recall for both classifiers on those multiple runs. You will probably find `split_fit_and_eval` defined above useful, but you may need to modify it.\n",
    "3.  For your first discussion post turn in the new code you wrote, including the code that labels and shuffles the data (discussed further below).  If you have to do a new import, show that. If you have to rewrite `split_fit_and_eval`, turn in the new version.  Also show the output, which should be a single line giving the accuracy, prcision, and recall.\n",
    "4.  Discuss which classifier does better.  Discuss which metric the best classifier does the worst at and speculate as to why (this will require reviewing the definitions of precision and recall and thinking about what they mean in a movie review setting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_wazBGWTsS4"
   },
   "source": [
    "#### Help with getting the movie reviews data.\n",
    "\n",
    "Execute the next two cells to get the movie review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvlkaI1x-rRr",
    "outputId": "5a8c9f38-c83f-49c9-97cd-0fa65d541ca4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/gawron/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nQdcr4aH_dP",
    "outputId": "2e1e6c67-ca8b-44b3-f5c0-9ac9adfd72d6"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "\n",
    "def get_file_strings (corpus, file_ids):\n",
    "    return [corpus.raw(file_id) for file_id in file_ids]\n",
    "\n",
    "data = dict(pos = mr.fileids('pos'),\n",
    "            neg = mr.fileids('neg'))\n",
    "\n",
    "pos_file_ids = data['pos']\n",
    "neg_file_ids = data['neg']\n",
    "\n",
    "# Get all the positive and negative reviews.\n",
    "pos_file_reviews = get_file_strings (mr, pos_file_ids)\n",
    "neg_file_reviews = get_file_strings (mr, neg_file_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PY6_uHMJvqC"
   },
   "source": [
    "Each review is a string.  In principle, a list of strings like `pos_file_reviews`  can be passed to `text.TfidfVectorizer()` via the `fit_transform` method to train a vectorizer for machine learning.\n",
    "You could code that up.\n",
    "\n",
    "What you'd really like to do is use `split_fit_and_eval`, defined above, which does a lot of the work for you.\n",
    "\n",
    "But hold on. You have a coding problem. You don't have  a sequence of documents and labels.  Instead you have\n",
    "one sequence of positive documents  and another sequence of negative documents.  \n",
    "\n",
    "So you will need to turn those two sequences into a sequence of documents and a sequence of labels\n",
    "because that's what `split_fit_and_eval` wants.  You also want the doc sequence\n",
    "to contain a random mixture of positive and negative documents, because some machine\n",
    "learning algorithms are sensitive to the order in which training data is presented to\n",
    "them.\n",
    "\n",
    "The next cell does **not** do that for you.  But it illustrates an approach using \n",
    "two sets of English letters in place of two sets of English documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCdgmoToRV8z",
    "outputId": "13bdbe23-796e-44db-90e6-abd0277a6bed"
   },
   "outputs": [],
   "source": [
    "# Lets work on documents\n",
    "# There are 2 classes, pos and neg\n",
    "\n",
    "from random import shuffle\n",
    "from string import ascii_lowercase\n",
    "\n",
    "\n",
    "# Now get pairs of letters and labels\n",
    "pos_pairs = [(rev,'pos') for rev in pos_file_reviews]\n",
    "neg_pairs = [(rev,'neg') for rev in neg_file_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCdgmoToRV8z",
    "outputId": "13bdbe23-796e-44db-90e6-abd0277a6bed"
   },
   "outputs": [],
   "source": [
    "###########  Shuffling  ###########################\n",
    "# Way too orderly, the classes arent mixed yet.\n",
    "data = pos_pairs + neg_pairs\n",
    "shuffle(data)\n",
    "###################  Now they're shuffled! ###############\n",
    "\n",
    "# Separate the letters from their labels\n",
    "revs, lbls = zip(*data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What you were given\n",
    "#def split_fit_and_eval(T,L,test_size=.2):\n",
    "#    # This code just collects together the training steps 2-5 + the eval\n",
    "#    # That is, It does one training,test., eval run\n",
    "#    (T1, T2, L1, L2) = train_test_split(T, L, test_size=test_size)\n",
    "#    tf = text.TfidfVectorizer()\n",
    "#    M1 = tf.fit_transform(T1)\n",
    "#    bnb = nb.BernoulliNB()\n",
    "#    bnb.fit(M1,L1)\n",
    "#    # .fit(), ..fit_transform()\n",
    "#    M2 = tf.transform(T2)\n",
    "#    P2 = bnb.predict(M2)\n",
    "#    return np.array([accuracy_score(P2,L2),\n",
    "#                     precision_score(P2, L2),\n",
    "#                     recall_score(P2,L2)])\n",
    "\n",
    "def split_vectorize_and_fit(T,L,clf,pos_label='pos'):\n",
    "    \"\"\"\n",
    "    Added default value for pos label\n",
    "    Added classifier argument.  \n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test, tf = split_and_vectorize(T,L)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    return precision_score(y_test,predictions,pos_label=pos_label), \\\n",
    "           recall_score(y_test, predictions,pos_label=pos_label), \\\n",
    "           accuracy_score(y_test,predictions)\n",
    "\n",
    "def split_and_vectorize(T,L):\n",
    "    \"\"\"\n",
    "    Added default value for pos label\n",
    "    Added classifier argument.  \n",
    "    \"\"\"\n",
    "    T_train,T_test, y_train,y_test = train_test_split(T,L)\n",
    "    tf = text.TfidfVectorizer()\n",
    "    X_train = tf.fit_transform(T_train)\n",
    "    X_test = tf.transform(T_test)\n",
    "    return X_train, X_test, y_train, y_test, tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution Partial:  Running just one classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB                                        Precision 0.878 Recall 0.669  Accuracy: 0.785\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_test_runs = 10\n",
    "scores = np.zeros((3,))\n",
    "for test_run in range(num_test_runs):\n",
    "    clf = nb.BernoulliNB()\n",
    "    scores += split_vectorize_and_fit(revs,lbls,clf)\n",
    "\n",
    "p_score,r_score,a_score = scores/num_test_runs\n",
    "print(f\"{'BernoulliNB':<50} Precision {p_score:.3f} Recall {r_score:.3f}  Accuracy: {a_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clfs = {\"Logistic Regression\": linear_model.LogisticRegression(),\n",
    "        \"Ridge Classifier\":   linear_model.RidgeClassifier(),\n",
    "        \"Multinomial NB\" : nb.MultinomialNB(),\n",
    "        \"Passive Aggressive Classifier\": linear_model.PassiveAggressiveClassifier(),\n",
    "        \"SVM\": LinearSVC(),\n",
    "       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing multiple classifiers in one loop with 10 training test splits for each classifier.\n",
    "Reporting average precision, recall and accuracy scores for the 10 test runs for each of\n",
    "the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:                                                    Precision   Recall  Accuracy \n",
      "Clf: Logistic Regression                                0.832        0.830   0.829\n",
      "\n",
      "Clf: Ridge Classifier                                   0.840        0.848   0.843\n",
      "\n",
      "Clf: Multinomial NB                                     0.881        0.695   0.792\n",
      "\n",
      "Clf: Passive Aggressive Classifier                      0.845        0.857   0.850\n",
      "\n",
      "Clf: SVM                                                0.843        0.875   0.856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_test_runs = 10\n",
    "scores = np.zeros((3,))\n",
    "\n",
    "print(f\"Clf: {'':<50} Precision   Recall  Accuracy \")\n",
    "for clf in clfs:\n",
    "    for test_run in range(num_test_runs):\n",
    "        scores += split_vectorize_and_fit(revs,lbls,clfs[clf])\n",
    "    p_score,r_score,a_score = scores/num_test_runs\n",
    "    print(f\"Clf: {clf:<50} {p_score:.3f}        {r_score:.3f}   {a_score:.3f}\")\n",
    "    scores = np.zeros((3,))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multinomial Naive Bayes classifier had the best Precision Score, but it also had the worst accuracy\n",
    "and by far the worst recall.  This is an excellent example of the precision-recall trade-off. The classifier\n",
    "achieves its high precision at the cost of a lot of false negatives, lowering its recall and accuracy.\n",
    "\n",
    "The highest Accuracy and the best recall was achieved by the Passive Aggressive Classifier, with the\n",
    "Ridge Classifier tying for best recall and just a smidge behind in accuracy. Except for the Multinomial Naive Bayes\n",
    "classifier, the precision scores all lagged behind their recall scores, meaning most of the classifiers\n",
    "traded in a little precision for better recall.\n",
    "\n",
    "**Note:  Your mileage may vary.  It doesn't mean much if you couldn't reproduce these numbers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Addendum**: Note the results discussed above aren't **stable**.  On this next run of 10 test runs,  the Passive Agressive Classifier got the best precision.   That's okay.  Just discuss the results you got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:                                                    Precision   Recall  Accuracy \n",
      "Clf: Logistic Regression                                0.807        0.827   0.816\n",
      "\n",
      "Clf: Ridge Classifier                                   0.815        0.866   0.837\n",
      "\n",
      "Clf: Multinomial NB                                     0.848        0.752   0.805\n",
      "\n",
      "Clf: Passive Aggressive Classifier                      0.850        0.862   0.855\n",
      "\n",
      "Clf: SVM                                                0.845        0.863   0.853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_test_runs = 10\n",
    "scores = np.zeros((3,))\n",
    "\n",
    "print(f\"Clf: {'':<50} Precision   Recall  Accuracy \")\n",
    "for clf in clfs:\n",
    "    for test_run in range(num_test_runs):\n",
    "        scores += split_vectorize_and_fit(revs,lbls,clfs[clf])\n",
    "    p_score,r_score,a_score = scores/num_test_runs\n",
    "    print(f\"Clf: {clf:<50} {p_score:.3f}        {r_score:.3f}   {a_score:.3f}\")\n",
    "    scores = np.zeros((3,))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus:\n",
    "\n",
    "A brief (\"truncated\") demo of using dimensionality reduction on textual data.  The \n",
    "technique is known as LSI (Latent Semantic Indexing), because the SVD\n",
    "axes are latent features,  linear combinations of the original word features.\n",
    "See the dimensionality reduction NB. \n",
    "\n",
    "It doesn't do badly. One reason this is attractive is because it is a whole\n",
    "lot faster than it used to be with classic eigenvector computation methods.\n",
    "Try to articulate a way in which the LSI helps classifier performance, at\n",
    "least in this experiment vis a vis the results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.804 Prec: 0.776 Rec: 0.816\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition as dec\n",
    "from sklearn import linear_model\n",
    "from sklearn import naive_bayes as nb\n",
    "\n",
    "X_train, X_test, y_train, y_test, tf = split_and_vectorize(revs,lbls)\n",
    "\n",
    "#Going to try what's known as LSI (Latent Semantic Indexing)\n",
    "reducer = dec.TruncatedSVD(n_components=200)\n",
    "# Using output of vectorizer\n",
    "X_train_reduced = reducer.fit_transform(X_train)\n",
    "clf_red = linear_model.LogisticRegression(solver='liblinear')\n",
    "clf_red.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Test\n",
    "X_test_reduced = reducer.transform(X_test)\n",
    "y_predicted = clf_red.predict(X_test_reduced)\n",
    "\n",
    "# Optional but convenient below\n",
    "y_test = np.array(y_test)\n",
    "pos_label='pos'\n",
    "acc, prec, rec = accuracy_score(y_test,y_predicted),\\\n",
    "                  precision_score(y_test,y_predicted,pos_label=pos_label), \\\n",
    "                    recall_score(y_test,y_predicted,pos_label=pos_label)\n",
    "\n",
    "print(f\"Acc: {acc:.3f} Prec: {prec:.3f} Rec: {rec:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition as dec\n",
    "\n",
    "n_components=2\n",
    "reducer = dec.PCA(n_components=n_components)\n",
    "X_r = reducer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important features (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the way two-class problems work with svms there is only one set\n",
    "of weights to look at.  So you need the **lowest** weighted features\n",
    "if you want to look at the more fun word set that best characterized **bad**\n",
    "reviews.  You will have to modify `get_topn` from the Insult Detection Notebook,\n",
    "to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos: great fun life quite hilarious memorable terrific overall excellent especially seen job perfectly trek performances true different matrix family good perfect american definitely performance gives truman effective wonderful enjoyed enjoyable frank bulworth titanic best pulp cameron rocky pace wonderfully mulan war people political bowfinger mamet solid works mike bit dark\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm as svm\n",
    "\n",
    "def get_feature_weights (clf):\n",
    "    \"\"\"\n",
    "    Return weight vector and weight dimensionality\n",
    "    \"\"\"\n",
    "    global clf0\n",
    "    if hasattr(clf,\"coef_\"):\n",
    "        (wt_dims,feats) = clf.coef_.shape\n",
    "        return wt_dims, clf.coef_\n",
    "    elif hasattr(clf,'feature_log_prob_'):\n",
    "        (wt_dims,feats) = clf.feature_log_prob_.shape\n",
    "        return wt_dims, clf.feature_log_prob_\n",
    "    else:\n",
    "        clf0 = clf\n",
    "        raise Exception(f\"Can\\'t find weights for classifier clf0: {clf0}\")\n",
    " \n",
    "\n",
    "def get_topn(clf, vectorizer=None, feature_names=None, top_n=10, class_labels=(True,), verbose=True):\n",
    "    \"\"\"\n",
    "    Prints features with the highest/lowest coefficient values, per class if appropriate.\n",
    "    \n",
    "    Assumptions: In a Binary class problem the positive class is labeled `\"Pos\"` or `True`,\n",
    "    the neg class `\"Neg\"` or `False`.  Generalizing from this case needs some more parameterization.    \n",
    "    \n",
    "    For the multiclass case the entire set of class labels is used. User supplied\n",
    "    class_labels are ignored.  Also in the case of a classifier that uses more than one\n",
    "    weight vector for a 2-class problem (e.g., Naive Bayes):\n",
    "    \n",
    "    If no vectorizer is supplied, the parameter `feature_names` must be a 1D numpy array with the names\n",
    "    aligned with the data matrix columns.\n",
    "    \"\"\"\n",
    "    ## NB: feature_names is a numpy array\n",
    "    if feature_names is None:\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "    ## Look at the feature weights the classifier learned.\n",
    "    wt_dims,weights = get_feature_weights (clf)\n",
    "    if wt_dims > 1:\n",
    "        # Must go through all classes in the right order when there's more than one weight vector\n",
    "        class_labels = clf.classes_\n",
    "    word_lists = []\n",
    "    ## ArgSort the feature weights row by row (that is, class by class): feature indices sorted from\n",
    "    ## lowest weighted feature to highest-weighted feature.\n",
    "    word_importance = weights.argsort(axis=1)\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        ## Th value-sorted indices of the feature weights the classifier learned for class i.\n",
    "        word_indices = word_importance[i]\n",
    "        ## Get the topn WORDS using fancy indexing on feature_names\n",
    "        if wt_dims==1 and class_label in [\"Pos\",True]:\n",
    "            # Reorder so most important comes first\n",
    "            Words = feature_names[word_indices[-top_n:]][::-1] \n",
    "        elif wt_dims==1 and class_label in [\"Neg\",False]:\n",
    "            Words = feature_names[word_indices[:top_n]]\n",
    "        else:\n",
    "            # NB clf case: each class has its own logprob vector\n",
    "            # Multiclass SVM: each class has its own coefficient vector\n",
    "            Words = feature_names[word_indices[-top_n:]][::-1] \n",
    "        if verbose:\n",
    "            WordsStr = \" \".join(Words)\n",
    "            print(f\"{class_label}: {WordsStr}\",end=\"\\n\\n\")\n",
    "        word_lists.append(Words)\n",
    "    return word_lists\n",
    "\n",
    "tf = text.TfidfVectorizer(stop_words=\"english\")\n",
    "#revs, lbls\n",
    "X_train = tf.fit_transform(revs)\n",
    "est = svm.LinearSVC()\n",
    "est.fit(X_train, lbls)\n",
    "# Now find the most heavily weighted features [= words]\n",
    "Words = get_topn(est, tf, class_labels=(\"Pos\",), top_n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-1.7519762264209062)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.decision_function(X_train).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.4920838699931187)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.decision_function(X_train).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0009f', ..., 'zwigoff', 'zycie', 'zzzzzzz'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tf.get_feature_names_out()\n",
    "#feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg: bad plot worst supposed unfortunately boring script waste awful poor reason looks ridiculous stupid attempt dull harry wasted mess terrible tv lame pointless cheap carpenter poorly maybe better fails jakob filmmakers attempts minute given worse adam guess director west point joke potential seagal falls material write didn bland make hurlyburly saved flat tries minutes eve tedious disappointing obvious embarrassing laughable charlie unfunny audience naked badly joan save talent predictable grade beast idea weak data alessa actors sloppy clich tired francis iii godzilla metro designed superior headed apparently dutch sports pay bore franklin annoying movie nbsp lifeless interesting problem figure random\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BadWords = get_topn(est,tf, class_labels=(\"Neg\",), top_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code note**: Demonstrating how `.argsort()` works on a 2D array `A`: `A.argsort()` always returns an array of \n",
    "the same shape as `A` but it's an array of indices, and we can sort either rows or columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[[ 0.  1.  2.  3.  4.  5.]\n",
      " [ 6.  7.  8.  9. 10. 11.]\n",
      " [12. 11. 10.  9.  8.  7.]\n",
      " [ 6.  5.  4.  3.  2.  1.]]\n",
      "argsort Axis 0 (sorted columns, indices are row indices)\n",
      "[[0 0 0 0 3 3]\n",
      " [1 3 3 3 0 0]\n",
      " [3 1 1 1 2 2]\n",
      " [2 2 2 2 1 1]]\n",
      "Example: Col 1 Sorted\n",
      "[ 1.  5.  7. 11.]\n",
      "argsort axis 1 (sorted rows,  indices are column indices)\n",
      "[[0 1 2 3 4 5]\n",
      " [0 1 2 3 4 5]\n",
      " [5 4 3 2 1 0]\n",
      " [5 4 3 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "A1 = np.arange(12,dtype=float).reshape((2,6))\n",
    "A2 = np.arange(12,0,-1,dtype=float).reshape((2,6))\n",
    "A = np.concatenate([A1,A2])\n",
    "print(\"A\")\n",
    "print(A)\n",
    "print(\"argsort Axis 0 (sorted columns, indices are row indices)\")\n",
    "col_sort = A.argsort(axis=0)\n",
    "print(col_sort)\n",
    "print(\"Example: Col 1 Sorted\")\n",
    "sorted_indices = col_sort[:,1]\n",
    "# Use fancy indexing \n",
    "print(A[sorted_indices,1])\n",
    "print(\"argsort axis 1 (sorted rows,  indices are column indices)\")\n",
    "print(A.argsort(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, sort order is smallest to biggest.  In other words, if we've argsorted the rows (axis=1),\n",
    "then pick a row, say, the third row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entire row: [12. 11. 10.  9.  8.  7.]\n",
      "The index of the second smallest element in the third row is: 4\n",
      "The value of the second smallest element in the third row is: 8.\n"
     ]
    }
   ],
   "source": [
    "sorted_indices = A.argsort(axis=1)\n",
    "#third row\n",
    "row_idx = 2\n",
    "print(\"The entire row:\",A[row_idx,:])\n",
    "col_idx = sorted_indices[row_idx,1]\n",
    "print(\"The index of the second smallest element in the third row is:\", col_idx)\n",
    "print(f\"The value of the second smallest element in the third row is: {A[row_idx,col_idx]:.0f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting always happens along an axis for a multidimensional array; the default\n",
    "is sorting along axis 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4, 5],\n",
       "       [0, 1, 2, 3, 4, 5],\n",
       "       [5, 4, 3, 2, 1, 0],\n",
       "       [5, 4, 3, 2, 1, 0]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.argsort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mnemonic.  Axis 0 is the row axis; axis 1 is the column axis.  If you want the indices\n",
    "in the sort to be row indices, use a row-axis (axis=0) argsort; if you want the indices\n",
    "in the sort to be column indices, use a column-axis (axis=1) argsort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addendum  (not part of the assignment):  Trying this out with NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole strategy doesn't work very well with NB because NB uses a kind of cumulative evidence strategy.\n",
    "All very common words are positive indicators of both classes but as we sum the log probs for all the words, the evidence gradually tilts in one direction or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg: film movie like just time good plot make character bad story way characters director little does don really doesn people know scene films scenes man end better movies best new big work gets script going life isn makes audience look long thing action think actually come real love plays old say things fact year great acting did role seen played point comes minutes screen ve comedy goes funny right years cast actors course interesting away lot didn performance world far trying dialogue takes watching john hard set instead looks star pretty ll want watch reason making half having guy given\n",
      "\n",
      "pos: film movie like just time good story character way does make life best characters little people films man really director new great scene doesn makes scenes don plot end work movies world know love seen performance years role right year real long audience gets old better things takes big comes going fact come ve young say plays cast look screen isn quite think actually thing played making away set day actors bad action place lot goes far times acting job did performances watch john funny course comedy help gives especially star interesting fun high instead family wife picture bit point\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uset stop words; reduce the tendency of NB to make the most important feat list be a list of very common words.\n",
    "tf = text.TfidfVectorizer(stop_words=\"english\")\n",
    "X_train = tf.fit_transform(revs)\n",
    "est = nb.BernoulliNB()\n",
    "est.fit(X_train, lbls)\n",
    "top_n=100\n",
    "# Now find the most heavily weighted features [= words]\n",
    "Words = get_topn(est, tf, top_n=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reasonable thing to try is sort the highest-weighted words by their IDF score.  The more\n",
    "docs a word has occurred in, the lower the IDF score; so a high-IDF \n",
    "word is more likely to be a content word. Let's print those first. \n",
    "\n",
    "It so happens the TFIDF Vectorizer stores the vocabulary\n",
    "IDF-scores in an attribute (in part because that's the part of the TFIDF score\n",
    "that can be computed once for each word for the entire document set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: neg\n",
      "reason looks half pretty dialogue given guy watching ll didn having trying hard want instead star watch john interesting minutes making far set course lot point funny comedy away actors goes did acting takes script played cast screen ve action actually comes fact plays say thing think right performance things come old world years look role year isn real going seen long audience gets big love better great makes work movies end know new bad scenes scene man life don doesn films best people really plot director little does characters way make character story good time just like movie film\n",
      "\n",
      "Class: pos\n",
      "family performances especially gives picture wife job bit high fun help times instead star watch john place quite day interesting making far set course lot point funny comedy young away actors goes did acting takes played cast screen ve action actually comes fact plays say thing think right performance things come old world years look role year isn real going seen long audience gets big love better great makes work movies end know new bad scenes scene man life don doesn films best people really plot director little does characters way make character story good time just like movie film\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As before\n",
    "top_n = 100\n",
    "feature_names = tf.get_feature_names_out()\n",
    "_wt_dims, weights = get_feature_weights (est)\n",
    "word_importance = weights.argsort(axis=1)\n",
    "\n",
    "# New: argsort idf scores for vocab\n",
    "idf_idx = tf.idf_.argsort()[::-1]\n",
    "\n",
    "\n",
    "for (i, this_class) in enumerate(est.classes_):\n",
    "    topn_idxs = word_importance[i][-top_n:]\n",
    "    # Present the top_n idxs in idf score order\n",
    "    idf_idx0 = [idx for idx in idf_idx if idx in topn_idxs]\n",
    "    print(f\"Class: {this_class}\")\n",
    "    print(' '.join(feature_names[idf_idx0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Looking at confidence scores (functional margins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classifier, prepare test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce labeling convention used for SVMs\n",
    "lbl_set = set(lbls)\n",
    "if lbl_set == {\"pos\",\"neg\"}:\n",
    "    lbls = np.array([1 if l == \"pos\" else -1 for l in lbls])\n",
    "elif lbl_set != {-1,1}:\n",
    "    raise Exception(\"Unknown labeling!\")\n",
    "T_train,T_test, y_train,y_test = train_test_split(revs,lbls)\n",
    "y_train, y_test = np.array(y_train),np.array(y_test)\n",
    "clf = LinearSVC()\n",
    "tf = text.TfidfVectorizer()\n",
    "X_train = tf.fit_transform(T_train)\n",
    "\n",
    "\n",
    "clf.fit(X_train,  y_train)\n",
    "X_test = tf.transform(T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When given one argument (the condition) np.where(condition)\n",
    "# returns a 1-tuple whose first member is the array of indexes \n",
    "# satisfying the condition\n",
    "fn_idxs = np.where((y_test > 0) & (predictions < 0))[0] \n",
    "fp_idxs = np.where((y_test < 0) & (predictions > 0))[0]\n",
    "\n",
    "# Take an example of each type\n",
    "fn_idx = fn_idxs[0]\n",
    "fp_idx = fp_idxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = clf.decision_function(X_train)\n",
    "lbls_svm_train = clf.predict(X_train)\n",
    "# Guarantee margins are positive\n",
    "functional_margins = lbls_svm_train * train_scores\n",
    "\n",
    "test_scores = clf.decision_function(X_test)\n",
    "predictions = clf.predict(X_test)\n",
    "test_margins = predictions * test_scores\n",
    "\n",
    "# Margins are all positive so this does what we want\n",
    "cmax_idx, cmin_idx = functional_margins.argmax(),functional_margins.argmin()\n",
    "cmax, cmin= functional_margins[cmax_idx],functional_margins[cmin_idx]\n",
    "\n",
    "def get_confidence_level(idx,test_margins,cmax,cmin):\n",
    "    \"\"\"\n",
    "    Note that this is a u-function.  That is, idx can be an array\n",
    "    of idxs and an array of confidence values will be returned.\n",
    "    \"\"\"\n",
    "    return (test_margins[idx]-cmin)/(cmax-cmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the fp item and the fn item  were low-confidence predictions, but the false positive\n",
    "prediction is significantly more confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 fp 0.13\n",
      "5 fn 0.01\n"
     ]
    }
   ],
   "source": [
    "for (tp,idx) in [(\"fp\",fp_idx),(\"fn\",fn_idx)]:\n",
    "    print(f\"{idx} {tp} {get_confidence_level(idx,test_margins,cmax,cmin):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, the fns and fps are low-confidence, suggesting confidence is not generally misplaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg fn margin: 0.14\n",
      "Avg fp margin: 0.11\n"
     ]
    }
   ],
   "source": [
    "fn_conf_mn = get_confidence_level(fn_idxs,test_margins,cmax,cmin).mean()\n",
    "print(f\"Avg fn margin: {fn_conf_mn:.2f}\")\n",
    "\n",
    "\n",
    "fp_conf_mn = get_confidence_level(fp_idxs,test_margins,cmax,cmin).mean()\n",
    "print(f\"Avg fp margin: {fp_conf_mn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the largest misplaced confidence margins are signifcantly less than .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 fn items; max conf: 0.39\n",
      "47 fp items; max conf: 0.34\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(fn_idxs)} fn items; max conf: {get_confidence_level(fn_idxs,test_margins,cmax,cmin).max():.2f}\")\n",
    "print(f\"{len(fp_idxs)} fp items; max conf: {get_confidence_level(fp_idxs,test_margins,cmax,cmin).max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addendum  (not part of the assignment):  Demonstrating use of `get_topn` on a multiclass nonlinguistic classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "data = load_iris()\n",
    "features = data['data']\n",
    "feature_names = np.array(data['feature_names'])\n",
    "target = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "# Rerun prediction on just the training data.\n",
    "X = features[:,:]\n",
    "Y = target\n",
    "# we create an instance of a Logistic Regression Classifier.\n",
    "logreg = linear_model.LogisticRegression(C=1e5,solver='lbfgs',multi_class='auto')\n",
    "#logreg = LinearSVC(C=1e5)\n",
    "\n",
    "logreg.fit(X, Y)\n",
    "predicted = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the weights on many sklearn classifiers will look like on\n",
    "a multiclass problem.  \n",
    "\n",
    "One weight vector for each of the iris class; Weights for four features.\n",
    "That yields a 3x4 array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  3.93510528,   9.17853868, -12.37698229,  -5.92989148],\n",
       "       [ -0.73525102,  -1.25242607,   1.47764768,  -6.16849281],\n",
       "       [ -3.19985426,  -7.92611261,  10.89933461,  12.09838429]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(feature_names))\n",
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out each class has a different \"Most important feature\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: sepal width (cm) sepal length (cm) petal width (cm) petal length (cm)\n",
      "\n",
      "1: petal length (cm) sepal length (cm) sepal width (cm) petal width (cm)\n",
      "\n",
      "2: petal width (cm) petal length (cm) sepal length (cm) sepal width (cm)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feat_array = get_topn(logreg,top_n=4,feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "pos_pairs = [(review, 'pos') for review in pos_file_reviews]\n",
    "neg_pairs = [(review, 'neg') for review in neg_file_reviews]\n",
    "review_data = pos_pairs + neg_pairs\n",
    "shuffle(review_data)\n",
    "reviews, labels = zip(*review_data)\n",
    "#print(labels)\n",
    "df_data = [reviews, labels]\n",
    "df = pd.DataFrame.from_records(df_data, index=[\"Review\",\"Opinion\"]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Opinion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bruce barth's mellow piano plays in the backgr...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>almost a full decade before steven spielberg's...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adam sandler vehicles are never anything speci...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" the red violin \" is a cold , sterile featur...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the calendar year has not even reached its mid...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>\" the endurance : shackleton's legendary anta...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>i've never written a review for a movie i have...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>edward zwick's \" the siege \" raises more quest...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>how do films like mouse hunt get into theatres...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>birthdays often cause individuals to access th...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review Opinion\n",
       "0     bruce barth's mellow piano plays in the backgr...     pos\n",
       "1     almost a full decade before steven spielberg's...     pos\n",
       "2     adam sandler vehicles are never anything speci...     neg\n",
       "3      \" the red violin \" is a cold , sterile featur...     neg\n",
       "4     the calendar year has not even reached its mid...     pos\n",
       "...                                                 ...     ...\n",
       "1995   \" the endurance : shackleton's legendary anta...     pos\n",
       "1996  i've never written a review for a movie i have...     neg\n",
       "1997  edward zwick's \" the siege \" raises more quest...     pos\n",
       "1998  how do films like mouse hunt get into theatres...     neg\n",
       "1999  birthdays often cause individuals to access th...     neg\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"Review\":reviews,\"Opinion\":labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Opinion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bruce barth's mellow piano plays in the backgr...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>almost a full decade before steven spielberg's...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adam sandler vehicles are never anything speci...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" the red violin \" is a cold , sterile featur...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the calendar year has not even reached its mid...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>\" the endurance : shackleton's legendary anta...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>i've never written a review for a movie i have...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>edward zwick's \" the siege \" raises more quest...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>how do films like mouse hunt get into theatres...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>birthdays often cause individuals to access th...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review Opinion\n",
       "0     bruce barth's mellow piano plays in the backgr...     pos\n",
       "1     almost a full decade before steven spielberg's...     pos\n",
       "2     adam sandler vehicles are never anything speci...     neg\n",
       "3      \" the red violin \" is a cold , sterile featur...     neg\n",
       "4     the calendar year has not even reached its mid...     pos\n",
       "...                                                 ...     ...\n",
       "1995   \" the endurance : shackleton's legendary anta...     pos\n",
       "1996  i've never written a review for a movie i have...     neg\n",
       "1997  edward zwick's \" the siege \" raises more quest...     pos\n",
       "1998  how do films like mouse hunt get into theatres...     neg\n",
       "1999  birthdays often cause individuals to access th...     neg\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Insults_with_Naive_Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "94px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
