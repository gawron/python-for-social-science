{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b95e841",
   "metadata": {
    "id": "0b95e841"
   },
   "source": [
    "## Text salad: WordNet, word lists, unicode, and compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746fd5d",
   "metadata": {
    "id": "2746fd5d"
   },
   "source": [
    "### Wordnet Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926d93e",
   "metadata": {
    "id": "f926d93e"
   },
   "source": [
    "WordNet (WN) is a large multilingual database pairing words and meanings.\n",
    "\n",
    "WordNet implements two key ideas.  The **senses** (or meanings) of a word are language\n",
    "independent concepts represented in a **very large** concept graph. We'll draw some concept network picture when we get to networks.  For now,\n",
    "some basics of WordNet.\n",
    "\n",
    "Concepts (or **synsets** in WN) are linked to **lemmas**, which are language\n",
    "particular ways of expresing a concept: a single concept links\n",
    "to *dog* in English and *chien* in French. Technically,\n",
    "the lemma is a lexical entry, which may have multiple forms;\n",
    "the aforementioned English lemma has two forms, *dog*\n",
    "and *dogs*.  So we refer to the string that captures the dictionary \n",
    "form of the lemma as the **lemma name**. The lemma  and the lemma\n",
    "name are distinct.  The lemma is a \n",
    "class instance with properties\n",
    "like a language, a name, hypernyms, antonyms, and so on.\n",
    "The lemma name is a string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f403e3",
   "metadata": {
    "id": "33f403e3"
   },
   "source": [
    "There are three different ways of spelling (expressing in writing) the first (most imprtant) sense of the word *dog*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f40b8",
   "metadata": {
    "id": "8e3f40b8",
    "outputId": "f2986739-f53a-484a-9fed-99f4ca63917a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'domestic_dog', 'Canis_familiaris']"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ln for ln in wn.synsets('dog')[0].lemma_names(lang='eng')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924f1b5",
   "metadata": {
    "id": "0924f1b5"
   },
   "source": [
    "Below, just for fun, we implement a more general function than we need, `get_active_words` which collects all words of a given length\n",
    "in a given language (if the language is in WordNet!).\n",
    "\n",
    "For example, let's collect all 5-letter French words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd731256",
   "metadata": {
    "id": "bd731256"
   },
   "outputs": [],
   "source": [
    "f_wds = get_active_words_wn (lang='fra')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561a2c25",
   "metadata": {
    "id": "561a2c25"
   },
   "source": [
    "\n",
    "Since `f_wds` is a set, we can't just look at the first 20 elements, so instead we look at a random sample.\n",
    "\n",
    "```\n",
    "from random import choice,sample\n",
    "from string import ascii_lowercase\n",
    "\n",
    "\n",
    ">>> sample(f_wds,20)\n",
    "['reine',\n",
    " 'mikv√©',\n",
    " 'geste',\n",
    " 'orgue',\n",
    " 'luire',\n",
    " 'anode',\n",
    " '√©luer',\n",
    " 'jaune',\n",
    " 'kobus',\n",
    " 'hindi',\n",
    " 'dingo',\n",
    " 'osier',\n",
    " 'lupin',\n",
    " 'gruau',\n",
    " 'ajuga',\n",
    " 'rumen',\n",
    " 'prise',\n",
    " 'unix‚Ñ¢',\n",
    " 'axial',\n",
    " 'gecko']\n",
    "```\n",
    "\n",
    "If you know French the set contains some pretty oddball words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6462ddb9",
   "metadata": {
    "id": "6462ddb9"
   },
   "source": [
    "It's a set so we can't grab the first 20 elements.  \n",
    "We'll just randomly sample 20 words plus one other we know for know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf1ef1",
   "metadata": {
    "id": "d2bf1ef1",
    "outputId": "f468e62f-ff65-4cc3-960d-b81ddc576c30",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mucor',\n",
       " 'lyc√©e',\n",
       " 'paire',\n",
       " 'combe',\n",
       " 'kogia',\n",
       " 'carvi',\n",
       " 'score',\n",
       " 'd√©vas',\n",
       " 'ch√©ri',\n",
       " 'n√®fle',\n",
       " 'pr√™le',\n",
       " 'nanti',\n",
       " 'heurt',\n",
       " 'priv√©',\n",
       " 'nuire',\n",
       " 'ador√©',\n",
       " 'golfe',\n",
       " 'slave',\n",
       " 'aotus',\n",
       " 'curry',\n",
       " 'unix‚Ñ¢']"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "add_on = [w for w in f_wds if w.startswith('unix')]\n",
    "sample(f_wds,20) + add_on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6853fe",
   "metadata": {
    "id": "fb6853fe"
   },
   "source": [
    "We note in passing that \"Unix\" with the trademark symbol counts as a 5-letter word.  Just one of many surprises you will experience once you start working with Unicode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a72bd56",
   "metadata": {
    "id": "7a72bd56"
   },
   "source": [
    "Here's code for implementing the building a wordnet word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c7000",
   "metadata": {
    "id": "289c7000",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "# For a more natural list of words than nltk.words()\n",
    "from string import ascii_lowercase,digits\n",
    "digits = set(digits)\n",
    "\n",
    "\n",
    "def get_active_words_wn (lang='eng'):\n",
    "    return {ln for w in wn.all_synsets() for ln in w.lemma_names(lang=lang) \n",
    "                 if ln.islower()  and len(ln) == 5 and  '_' not in ln\n",
    "                 and digits.intersection(ln) == set()}\n",
    "\n",
    "def get_definitions(word_set,language = None):\n",
    "    \"\"\"\n",
    "    Need to check that synset has at least one lemma in the given langugae.\n",
    "    \"\"\"\n",
    "    for wd in word_set:\n",
    "        print(wd)\n",
    "        for (i,ss) in enumerate(wn.synsets(wd)):\n",
    "            print(f'{i+1}. {ss.definition()}.',end= '  ')\n",
    "            print()\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "active_words = get_active_words_wn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7e8b7",
   "metadata": {
    "id": "14b7e8b7",
    "outputId": "5ee8d900-f4d6-4425-8a79-79384006c629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4158"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(active_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23060fae",
   "metadata": {
    "id": "23060fae"
   },
   "source": [
    "`active_words` is a set so we can't just look at the first 20 elements. \n",
    "\n",
    "Let's look at a random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc77f9a",
   "metadata": {
    "id": "bcc77f9a",
    "outputId": "3059b837-56aa-472e-f2fc-7853ccce5b1d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['merit',\n",
       " 'zesty',\n",
       " 'nitid',\n",
       " 'bract',\n",
       " 'so-so',\n",
       " 'fryer',\n",
       " 'urban',\n",
       " 'scoot',\n",
       " 'dimly',\n",
       " 'aroid',\n",
       " 'three',\n",
       " 'tower',\n",
       " 'snare',\n",
       " 'wheal',\n",
       " 'chivy',\n",
       " 'gummy',\n",
       " 'angry',\n",
       " 'glare',\n",
       " 'edged',\n",
       " 'fovea']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import sample\n",
    "L = sample(active_words,20)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f477c",
   "metadata": {
    "id": "a00f477c",
    "outputId": "060fe999-0b3a-449f-850e-f92af8f6ddd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merit\n",
      "1. any admirable quality or attribute.  \n",
      "2. the quality of being deserving (e.g., deserving assistance).  \n",
      "3. be worthy or deserving.  \n",
      "\n",
      "\n",
      "zesty\n",
      "1. having an agreeably pungent taste.  \n",
      "2. marked by spirited enjoyment.  \n",
      "\n",
      "\n",
      "nitid\n",
      "1. bright with a steady but subdued shining.  \n",
      "\n",
      "\n",
      "bract\n",
      "1. a modified leaf or leaflike part just below and protecting an inflorescence.  \n",
      "\n",
      "\n",
      "so-so\n",
      "1. being neither good nor bad.  \n",
      "2. in an acceptable (but not outstanding) manner.  \n",
      "\n",
      "\n",
      "fryer\n",
      "1. flesh of a medium-sized young chicken suitable for frying.  \n",
      "\n",
      "\n",
      "urban\n",
      "1. relating to or concerned with a city or densely populated area.  \n",
      "2. located in or characteristic of a city or city life.  \n",
      "\n",
      "\n",
      "scoot\n",
      "1. run or move very quickly or hastily.  \n",
      "\n",
      "\n",
      "dimly\n",
      "1. in a dim indistinct manner.  \n",
      "2. in a manner lacking interest or vitality.  \n",
      "3. with a dim light.  \n",
      "\n",
      "\n",
      "aroid\n",
      "1. any plant of the family Araceae; have small flowers massed on a spadix surrounded by a large spathe.  \n",
      "2. relating to a plant of the family Araceae.  \n",
      "\n",
      "\n",
      "three\n",
      "1. the cardinal number that is the sum of one and one and one.  \n",
      "2. one of four playing cards in a deck having three pips.  \n",
      "3. being one more than two.  \n",
      "\n",
      "\n",
      "tower\n",
      "1. a structure taller than its diameter; can stand alone or be attached to a larger building.  \n",
      "2. anything that approximates the shape of a column or tower.  \n",
      "3. a powerful small boat designed to pull or push larger ships.  \n",
      "4. appear very large or occupy a commanding position.  \n",
      "\n",
      "\n",
      "snare\n",
      "1. something (often something deceptively attractive) that catches you unawares.  \n",
      "2. a small drum with two heads and a snare stretched across the lower head.  \n",
      "3. a surgical instrument consisting of wire hoop that can be drawn tight around the base of polyps or small tumors to sever them; used especially in body cavities.  \n",
      "4. strings stretched across the lower head of a snare drum; they make a rattling sound when the drum is hit.  \n",
      "5. a trap for birds or small mammals; often has a slip noose.  \n",
      "6. catch in or as if in a trap.  \n",
      "7. entice and trap.  \n",
      "\n",
      "\n",
      "wheal\n",
      "1. a raised mark on the skin (as produced by the blow of a whip); characteristic of many allergic reactions.  \n",
      "\n",
      "\n",
      "chivy\n",
      "1. annoy continually or chronically.  \n",
      "\n",
      "\n",
      "gummy\n",
      "1. having the sticky properties of an adhesive.  \n",
      "2. covered with adhesive gum.  \n",
      "\n",
      "\n",
      "angry\n",
      "1. feeling or showing anger.  \n",
      "2. (of the elements) as if showing violent anger.  \n",
      "3. severely inflamed and painful.  \n",
      "\n",
      "\n",
      "glare\n",
      "1. a light within the field of vision that is brighter than the brightness to which the eyes are adapted.  \n",
      "2. an angry stare.  \n",
      "3. a focus of public attention.  \n",
      "4. look at with a fixed gaze.  \n",
      "5. be sharply reflected.  \n",
      "6. shine intensely.  \n",
      "\n",
      "\n",
      "edged\n",
      "1. advance slowly, as if by inches.  \n",
      "2. provide with a border or edge.  \n",
      "3. lie adjacent to another or share a boundary.  \n",
      "4. provide with an edge.  \n",
      "5. having a specified kind of border or edge.  \n",
      "6. (of speech) harsh or hurtful in tone or character.  \n",
      "7. having a cutting edge or especially an edge or edges as specified; often used in combination.  \n",
      "\n",
      "\n",
      "fovea\n",
      "1. area consisting of a small depression in the retina containing cones and where vision is most acute.  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_definitions(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6afedb",
   "metadata": {
    "id": "ca6afedb"
   },
   "source": [
    "### Unicode and unicode code points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92589d8f",
   "metadata": {
    "id": "92589d8f"
   },
   "source": [
    "To go from a numerical code point to the corresponding unicode character,\n",
    "use `chr` (look up the emoji codes [here](https://unicode.org/emoji/charts/full-emoji-list.html)\n",
    "and the chess piece code and other codes in the same unicode neighborhood [here](https://www.unicode.org/charts/PDF/U2600.pdf).\n",
    "\n",
    "For a list of unicode charts, look [here](http://www.unicode.org/charts/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84bba449",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84bba449",
    "outputId": "25354dfc-dae5-4f8a-eedf-c8a5281c2364"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 'üòÄ‚ôû')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0x prefix for a number means we are using hexadecimal (standard in unicode tables)\n",
    "smiley = chr(0x1F600)#.decode(encoding='utf8')\n",
    "knight = chr(0x265E)#.decode(encoding='utf8')\n",
    "len(smiley + knight), smiley + knight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed45116",
   "metadata": {
    "id": "1ed45116",
    "outputId": "b6179e04-49f1-4a65-e14b-2c7b10c263f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128512, 'üòÄ')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0x1F600, chr(0x1F600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e39005",
   "metadata": {
    "id": "88e39005",
    "outputId": "8ab4d276-456f-4e34-bfb4-7ceec36bdf18",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0x1F600 == 128512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3392b6",
   "metadata": {
    "id": "de3392b6"
   },
   "source": [
    "Hence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce3c1f",
   "metadata": {
    "id": "06ce3c1f",
    "outputId": "7521ce83-12cb-409a-d660-f43ab0f767f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üòÄ'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(128512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de81a85f",
   "metadata": {
    "id": "de81a85f"
   },
   "source": [
    "To go from character  to code point, use ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586b9a8",
   "metadata": {
    "id": "a586b9a8",
    "outputId": "a05db117-cb61-4539-aa85-20d98c82d00a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128512"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(smiley)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b02c8f",
   "metadata": {
    "id": "d7b02c8f"
   },
   "source": [
    "You will usually want to see this in hex, so do some string formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d38e4",
   "metadata": {
    "id": "c26d38e4",
    "outputId": "a87cf8c8-d93d-43f3-d01f-71b3f058f7b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x01f600'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{ord(smiley):#08x}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c30b3e",
   "metadata": {
    "id": "f2c30b3e"
   },
   "source": [
    "Note that using \"#\" in the formating code just produces a string that advertises\n",
    "the fact that it represents a hexadecimal number (prefix \"0x\"). Compare:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c86b8",
   "metadata": {
    "id": "546c86b8",
    "outputId": "29919c8d-c7cd-437f-f178-853e43f713fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0001f600'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{ord(smiley):08x}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae12b4b",
   "metadata": {
    "id": "6ae12b4b"
   },
   "source": [
    "`ord` requires a single character argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b37a5",
   "metadata": {
    "id": "739b37a5",
    "outputId": "ae215da2-b559-4f6e-9de9-61a46a3f67c8"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ord() expected a character, but string of length 2 found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-249-9369e418c775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: ord() expected a character, but string of length 2 found"
     ]
    }
   ],
   "source": [
    "ord('xy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1fa48f",
   "metadata": {
    "id": "fe1fa48f"
   },
   "source": [
    "To maintain a consistent implementation of\n",
    "this idea, `ord` does not support the extended notion of\n",
    "Unicode character which admits some characters that require **two** unicode code points\n",
    "(flags, some emoji). See [Unicode org docs](https://unicode.org/Public/emoji/4.0/emoji-sequences.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d4499",
   "metadata": {
    "id": "f95d4499",
    "outputId": "0fbf8f72-1d2c-49a5-fb2e-a7a31d849246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 üá¶üá©\n",
      "2 üáµüá±\n"
     ]
    }
   ],
   "source": [
    "flag_of_ascension_island = '\\U0001F1E6\\U0001F1E9'\n",
    "flag_of_poland = '\\U0001F1F5\\U0001F1F1'\n",
    "print(len(flag_of_ascension_island),flag_of_ascension_island)\n",
    "print(len(flag_of_poland),flag_of_poland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9bed7",
   "metadata": {
    "id": "34e9bed7",
    "outputId": "907188ed-6291-4708-f76c-8d395995dbae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üá¶üá©\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ord() expected a character, but string of length 2 found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-56527cc8f618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mflag_of_ascension_island\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\U0001F1E6\\U0001F1E9'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag_of_ascension_island\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag_of_ascension_island\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: ord() expected a character, but string of length 2 found"
     ]
    }
   ],
   "source": [
    "#TypeError: ord() expected a character, but string of length 2 found\n",
    "#ord(flag_of_ascension_island)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80472da4",
   "metadata": {
    "id": "80472da4"
   },
   "source": [
    "To use Unicode code points in strings.  Use \\U and \\u escapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "640e5ac0",
   "metadata": {
    "id": "640e5ac0",
    "outputId": "87ed9b53-5f95-416b-c1fc-f249a3a185fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'üòÄ', 1, '‚ôû')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\\U0001F600\"),\"\\U0001F600\",len(\"\\u265E\"),\"\\u265E\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d06ae629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòÄ\n"
     ]
    }
   ],
   "source": [
    "x = \"\\U0001F600\"\n",
    "print(f'{x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1474e3",
   "metadata": {
    "id": "ee1474e3"
   },
   "source": [
    "Note: because of the escapes, the strings above are characters, not representations\n",
    "of hexadecimal numbers.  Compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d584a0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945 03b1\n"
     ]
    }
   ],
   "source": [
    "print(f\"{ord('Œ±')} {ord('Œ±'):04x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac98fc",
   "metadata": {
    "id": "42ac98fc",
    "outputId": "01c569c2-b5bf-406f-b859-02f9b20ebd6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0001f600 1 üòÄ\n"
     ]
    }
   ],
   "source": [
    "hex_str = f'{ord(smiley):08x}'\n",
    "unicode_char = \"\\U0001F600\"\n",
    "print(len(hex_str),hex_str,len(unicode_char),unicode_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2322d",
   "metadata": {
    "id": "1fa2322d",
    "outputId": "5b88876a-61d5-4521-9c49-2dea445b6f31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíØ\n"
     ]
    }
   ],
   "source": [
    "print('\\U0001f4af')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "JLw-HoSwZKo-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLw-HoSwZKo-",
    "outputId": "b5c3c7f9-e78a-41e2-85f5-17e08ab4d096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶ú\n"
     ]
    }
   ],
   "source": [
    "print('\\U0001F99C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9cb16fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129436"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0x1F99C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2fd836",
   "metadata": {
    "id": "cb2fd836"
   },
   "source": [
    "A lot of times you will have access to a keyboard that will let you \n",
    "do \"literal\" unicode character entry.  Use it! \n",
    "\n",
    "Or cut and paste from a window where you can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681ff4bc",
   "metadata": {
    "id": "681ff4bc",
    "outputId": "c7a8632a-aaf8-4a14-cb00-81575ea786ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 'Œ±Œ≤Œ≥Œ¥ŒµŒ∂Œ∑Œ∏ŒπŒ∫ŒªŒºŒΩŒæŒøœÄœÅœÇœÉœÑœÖœÜœáœà')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = 'Œ±Œ≤Œ≥Œ¥ŒµŒ∂Œ∑Œ∏ŒπŒ∫ŒªŒºŒΩŒæŒøœÄœÅœÇœÉœÑœÖœÜœáœà'\n",
    "len(alphabet),alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79448def",
   "metadata": {
    "id": "79448def",
    "outputId": "2ddc5eeb-b839-4aa7-d6e3-cc3000ceed48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "946"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(alphabet[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953f8bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'œÜ'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet[-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a69d9e",
   "metadata": {},
   "source": [
    "The Œ± in the code cell below was entered by typing \\alpha and then hitting Tab.\n",
    "    \n",
    "Try this in the following code cell (Note: it must be a code cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Œ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af86fd3",
   "metadata": {},
   "source": [
    "Have to hit Tab twice to enter this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7034baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Œ±Œ≤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87a29c",
   "metadata": {},
   "source": [
    "And we have just changed the subject.  We're no longer talking about strings, not even unicode strings.  \n",
    "\n",
    "We're talking about what can be a **name** in Python.  And that's a completely\n",
    "different thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e694785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "œï = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "717ea807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "œï"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac388a06",
   "metadata": {},
   "source": [
    "Forgot to hit Tab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70de5f60",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (1782519148.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/w9/bx4mylnd27g_kqqgn5hrn2x40000gr/T/ipykernel_65591/1782519148.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    \\phi\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "\\phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66239d4",
   "metadata": {
    "id": "e66239d4"
   },
   "source": [
    "### Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c431dca",
   "metadata": {
    "id": "1c431dca"
   },
   "source": [
    "To represent unicode characters in a file or in a data stream traveling from\n",
    "computer to computer, we need some conventions about how to **encode** characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e561a50",
   "metadata": {
    "id": "6e561a50"
   },
   "source": [
    "Two different encodings of the same character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ccfa6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1ccfa6a",
    "outputId": "e7dc6868-8a56-4617-e8e5-9c938a7308a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'\\xf0\\x9f\\x98\\x80', b'\\xff\\xfe=\\xd8\\x00\\xde')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1, b2 = smiley.encode(encoding='utf8'),smiley.encode(encoding='utf16')\n",
    "b1,b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817acb6",
   "metadata": {
    "id": "7817acb6"
   },
   "source": [
    "The results of such encodings in Python are a new sequence type\n",
    "called **bytes**.  The same type we get when we read in a file\n",
    "in binary mode (for example, a compiled program).  It's just\n",
    "data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a976c5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a976c5b",
    "outputId": "fc3dd7ba-515c-4d5c-96e0-719531cb71fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(bytes, bytes, False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b1),type(b2),b1 == b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd50dd",
   "metadata": {
    "id": "3fdd50dd",
    "outputId": "86903ece-e857-4592-9ba6-c869b8bb51f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b1),len(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6cf4e",
   "metadata": {
    "id": "b0f6cf4e"
   },
   "source": [
    "When a bytes instance represents a unicode text string, it is very\n",
    "hard to do anything with it unless we know the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219105b",
   "metadata": {
    "id": "b219105b",
    "outputId": "dfb7d4b2-5856-4303-cce1-b2b96ea2193c",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-540bfa1d069f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Using the wrong encoding.  Utf8 impossibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "#Using the wrong encoding.  Utf8 impossibility\n",
    "b2.decode(encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f8595",
   "metadata": {
    "id": "192f8595"
   },
   "source": [
    "And that's not the worst thing that can happen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "585e12bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "585e12bf",
    "outputId": "379832fd-352f-4844-b1c3-989dd4046c76"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\u9ff0ËÇò'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the wrong encoding.  Nonsense!\n",
    "b1.decode(encoding='utf16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79268f51",
   "metadata": {
    "id": "79268f51"
   },
   "source": [
    "For a great discussion of the design features of encodings \n",
    "and what representing textual information on a computer means,\n",
    "see [real Python docs.](https://realpython.com/python-encodings-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d9aeb",
   "metadata": {
    "id": "519d9aeb"
   },
   "source": [
    "### The unicode sandwich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b11a3",
   "metadata": {
    "id": "070b11a3"
   },
   "source": [
    "The basic principle in Python: Treat all unicode strings as sequences of unicode characters, each represented by a unicode code point.  Support unicode code point entry, unicode code point requests.\n",
    "\n",
    "This was demonstrated above.  \n",
    "\n",
    "Encoded strings are bytes: binary data.  A different type.  Textual unicode\n",
    "data belongs on one side of a great divide; bytes strings on another.\n",
    "\n",
    "This was illustrated above.\n",
    "\n",
    "But what about the real world?  The world outside of python.  The world\n",
    "in which operating system vendors can disagree on  what encoding to use, in which\n",
    "particular language communities, for instance users of Japanese and Chinese\n",
    "writing, may have established their\n",
    "own encodings before UTF8 became a de facto default?\n",
    "\n",
    "The way to deal with this is the **unicode sandwich**.\n",
    "\n",
    "```\n",
    "encoded representation\n",
    "\n",
    "      \\|/   Input: decode (somebody told you the encoding!)\n",
    "      /|\\\n",
    "      \n",
    "  unicode string     [your program goes here!]\n",
    "  \n",
    "      \\|/   Output: encode (somebody, maybe your OS, told you what encoding to use!)\n",
    "      /|\\   \n",
    "   \n",
    "encoded  representation\n",
    "```\n",
    "\n",
    "In other words the only parts of your program that know\n",
    "anything about encodings are the Input/Output (IO) parts (a little bit\n",
    "of an idealization, but that's the goal!).\n",
    "\n",
    "You don't need to care about whether you are dealing with\n",
    "Greek, Latin, Tamil, Tibetan, or Tagbanwa characters,\n",
    "or emoji.  All can be freely combined.  All will \n",
    "increase a string length by 1.\n",
    "\n",
    "Adapted from \n",
    "the [Python unicode How To doc:](https://docs.python.org/3/howto/unicode.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37f4aed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001f600 üòÄ GRINNING FACE\n",
      "0001f601 üòÅ GRINNING FACE WITH SMILING EYES\n",
      "0001f602 üòÇ FACE WITH TEARS OF JOY\n",
      "0001f603 üòÉ SMILING FACE WITH OPEN MOUTH\n",
      "0001f604 üòÑ SMILING FACE WITH OPEN MOUTH AND SMILING EYES\n",
      "0001f605 üòÖ SMILING FACE WITH OPEN MOUTH AND COLD SWEAT\n",
      "0001f606 üòÜ SMILING FACE WITH OPEN MOUTH AND TIGHTLY-CLOSED EYES\n",
      "0001f607 üòá SMILING FACE WITH HALO\n",
      "0001f608 üòà SMILING FACE WITH HORNS\n",
      "0001f609 üòâ WINKING FACE\n",
      "0001f60a üòä SMILING FACE WITH SMILING EYES\n",
      "0001f60b üòã FACE SAVOURING DELICIOUS FOOD\n",
      "0001f60c üòå RELIEVED FACE\n",
      "0001f60d üòç SMILING FACE WITH HEART-SHAPED EYES\n",
      "0001f60e üòé SMILING FACE WITH SUNGLASSES\n",
      "0001f60f üòè SMIRKING FACE\n",
      "0001f610 üòê NEUTRAL FACE\n",
      "0001f611 üòë EXPRESSIONLESS FACE\n",
      "0001f612 üòí UNAMUSED FACE\n",
      "0001f613 üòì FACE WITH COLD SWEAT\n",
      "0001f614 üòî PENSIVE FACE\n",
      "0001f615 üòï CONFUSED FACE\n",
      "0001f616 üòñ CONFOUNDED FACE\n",
      "0001f617 üòó KISSING FACE\n",
      "0001f618 üòò FACE THROWING A KISS\n",
      "0001f619 üòô KISSING FACE WITH SMILING EYES\n",
      "0001f61a üòö KISSING FACE WITH CLOSED EYES\n",
      "0001f61b üòõ FACE WITH STUCK-OUT TONGUE\n",
      "0001f61c üòú FACE WITH STUCK-OUT TONGUE AND WINKING EYE\n",
      "0001f61d üòù FACE WITH STUCK-OUT TONGUE AND TIGHTLY-CLOSED EYES\n",
      "0001f61e üòû DISAPPOINTED FACE\n",
      "0001f61f üòü WORRIED FACE\n",
      "0001f620 üò† ANGRY FACE\n",
      "0001f621 üò° POUTING FACE\n",
      "0001f622 üò¢ CRYING FACE\n",
      "0001f623 üò£ PERSEVERING FACE\n",
      "0001f624 üò§ FACE WITH LOOK OF TRIUMPH\n",
      "0001f625 üò• DISAPPOINTED BUT RELIEVED FACE\n",
      "0001f626 üò¶ FROWNING FACE WITH OPEN MOUTH\n",
      "0001f627 üòß ANGUISHED FACE\n",
      "0001f628 üò® FEARFUL FACE\n",
      "0001f629 üò© WEARY FACE\n",
      "0001f62a üò™ SLEEPY FACE\n",
      "0001f62b üò´ TIRED FACE\n",
      "0001f62c üò¨ GRIMACING FACE\n",
      "0001f62d üò≠ LOUDLY CRYING FACE\n",
      "0001f62e üòÆ FACE WITH OPEN MOUTH\n",
      "0001f62f üòØ HUSHED FACE\n",
      "0001f630 üò∞ FACE WITH OPEN MOUTH AND COLD SWEAT\n",
      "0001f631 üò± FACE SCREAMING IN FEAR\n",
      "0001f632 üò≤ ASTONISHED FACE\n",
      "0001f633 üò≥ FLUSHED FACE\n",
      "0001f634 üò¥ SLEEPING FACE\n",
      "0001f635 üòµ DIZZY FACE\n",
      "0001f636 üò∂ FACE WITHOUT MOUTH\n",
      "0001f637 üò∑ FACE WITH MEDICAL MASK\n",
      "0001f638 üò∏ GRINNING CAT FACE WITH SMILING EYES\n",
      "0001f639 üòπ CAT FACE WITH TEARS OF JOY\n",
      "0001f63a üò∫ SMILING CAT FACE WITH OPEN MOUTH\n",
      "0001f63b üòª SMILING CAT FACE WITH HEART-SHAPED EYES\n",
      "0001f63c üòº CAT FACE WITH WRY SMILE\n",
      "0001f63d üòΩ KISSING CAT FACE WITH CLOSED EYES\n",
      "0001f63e üòæ POUTING CAT FACE\n",
      "0001f63f üòø CRYING CAT FACE\n",
      "0001f640 üôÄ WEARY CAT FACE\n",
      "0001f641 üôÅ SLIGHTLY FROWNING FACE\n",
      "0001f642 üôÇ SLIGHTLY SMILING FACE\n",
      "0001f643 üôÉ UPSIDE-DOWN FACE\n",
      "0001f644 üôÑ FACE WITH ROLLING EYES\n",
      "0001f645 üôÖ FACE WITH NO GOOD GESTURE\n",
      "0001f646 üôÜ FACE WITH OK GESTURE\n",
      "0001f647 üôá PERSON BOWING DEEPLY\n",
      "0001f648 üôà SEE-NO-EVIL MONKEY\n",
      "0001f649 üôâ HEAR-NO-EVIL MONKEY\n",
      "0001f64a üôä SPEAK-NO-EVIL MONKEY\n",
      "0001f64b üôã HAPPY PERSON RAISING ONE HAND\n",
      "0001f64c üôå PERSON RAISING BOTH HANDS IN CELEBRATION\n",
      "0001f64d üôç PERSON FROWNING\n",
      "0001f64e üôé PERSON WITH POUTING FACE\n",
      "0001f64f üôè PERSON WITH FOLDED HANDS\n"
     ]
    }
   ],
   "source": [
    "for code in range(0x1f600,+0x1f600+80):\n",
    "    print(f\"{code:08x} {chr(code)} {unicodedata.name(chr(code))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ceb024c",
   "metadata": {
    "id": "1ceb024c",
    "outputId": "c6b18514-6fdc-4197-c987-a17f8aff8db0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 characters in search of a word: Œ≤√©‡Ø≤‡æÖ·ù∞üòÄ\n",
      "\n",
      "0  Œ≤  000003b2 Ll GREEK SMALL LETTER BETA\n",
      "1  √©  000000e9 Ll LATIN SMALL LETTER E WITH ACUTE\n",
      "2  ‡Ø≤  00000bf2 No TAMIL NUMBER ONE THOUSAND\n",
      "3  ‡æÖ  00000f85 Po TIBETAN MARK PALUTA\n",
      "4  ·ù∞  00001770 Lo TAGBANWA LETTER SA\n",
      "5  üòÄ  0001f600 So GRINNING FACE\n",
      "\n",
      "Tamil ‡Ø≤ is 1000.0\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "u = chr(946) + chr(233) + chr(0x0bf2) + chr(3973) + chr(6000) + smiley\n",
    "\n",
    "print(f'{len(u)} characters in search of a word: {u}')\n",
    "print()\n",
    "for i, c in enumerate(u):\n",
    "    print(f'{i} {c:^3} {ord(c):08x} {unicodedata.category(c)}', end=\" \")\n",
    "    print(unicodedata.name(c))\n",
    "\n",
    "# Get numeric value of numeric character\n",
    "print()\n",
    "print(f'Tamil {u[2]} is {unicodedata.numeric(u[2])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197257b7",
   "metadata": {
    "id": "197257b7"
   },
   "source": [
    "We use the `unicodedata` module to access the database info on each\n",
    "character, displaying its name/description and its unicode category.\n",
    "\n",
    "Tagbanwa is one of the scripts indigenous to the Philippines used by the Tagbanwa people and the Palawan people.\n",
    "\n",
    "The unicode categories used above are\n",
    "\n",
    "```\n",
    "L1 Letter lowercase\n",
    "No Numerical other\n",
    "Po Punctuation other\n",
    "Lo Letter other\n",
    "So Symbol other\n",
    "```\n",
    "\n",
    "See [compart.com unicode docs](https://www.compart.com/en/unicode/category) \n",
    "for more discussion of Unicode categories.\n",
    "\n",
    "There's a lot of well-thought out discussion of Unicode concepts such as Unicode categories, planes, and blocks on [compart.com.](https://www.compart.com/en/unicode/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894fc17",
   "metadata": {
    "id": "9894fc17"
   },
   "source": [
    "Things are only slightly more complicated with 2-character symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e860e23",
   "metadata": {
    "id": "9e860e23",
    "outputId": "ae0ab50b-5ba9-4af5-d0f9-bbfbad48c9ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üáµüá±   So So REGIONAL INDICATOR SYMBOL LETTER P REGIONAL INDICATOR SYMBOL LETTER L\n"
     ]
    }
   ],
   "source": [
    " v = '\\U0001F1F5\\U0001F1F1'\n",
    "print(f'{v:^3}  {unicodedata.category(v[0])} {unicodedata.category(v[1])}', end=\" \")\n",
    "print(unicodedata.name(v[0]),unicodedata.name(v[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82dfe6",
   "metadata": {
    "id": "4c82dfe6"
   },
   "source": [
    "Here \"PL\" is short for Poland."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c61e02",
   "metadata": {
    "id": "59c61e02"
   },
   "source": [
    "### Reading a large compressed file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fbff2a",
   "metadata": {
    "id": "56fbff2a"
   },
   "source": [
    "The idea here is to present code that lets you process a large compressed\n",
    "file without having (a) to uncompress the whole thing in memory;\n",
    "(b) read the entire compressed file into memory.\n",
    "\n",
    "Decide first what you want to do with the file contents.  We will use\n",
    "the example of the Google Books word list.  We will read and uncompress it up\n",
    "to a frequency threshhold, and then stop.\n",
    "\n",
    "We will iterate through line by line.  The inner loop\n",
    "needs to  process a single line.  Here's what that looks like\n",
    "for this example file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9cb79d",
   "metadata": {
    "id": "7d9cb79d"
   },
   "outputs": [],
   "source": [
    "def process_line(line,freq_dict,threshhold,debug=False,format2=False):\n",
    "    try:\n",
    "        # Expected line format goes here\n",
    "        if format2:\n",
    "             (wd, count) = line.split()\n",
    "        else:\n",
    "            (rank, wd, count, pct, cum_pct) = line.split()\n",
    "    except ValueError:\n",
    "        # Abort!  This aint happening.  Not necessarily an error.\n",
    "        # Could be last line of a well-formatted file.\n",
    "        print(line)\n",
    "        return False\n",
    "    try:\n",
    "        # replace does not raise an Exception if there is no \",\".\n",
    "        # `int(...)` will for any uncoerceable string.\n",
    "        int_ct = int(count.replace(\",\",''))\n",
    "    except Exception as e:\n",
    "        print(line)\n",
    "        raise e\n",
    "    # We'll stop processing when we get to low frequency stuff.\n",
    "    if int_ct < threshhold:\n",
    "        return False\n",
    "    else:\n",
    "        if not debug:\n",
    "            freq_dict[wd] = int_ct\n",
    "        else:\n",
    "            freq_dict[wd] = (rank, count, pct, cum_pct)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a1f9a",
   "metadata": {
    "id": "2e2a1f9a"
   },
   "source": [
    "### the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79456550",
   "metadata": {
    "id": "79456550"
   },
   "source": [
    "The file loaded below is a truncated version of \n",
    "[Hacker9b's wordlist repository.](https://github.com/hackerb9/gwordlist)\n",
    "This in turn is derived from  [Google corpus V3 20200217](https://storage.googleapis.com/books/ngrams/books/datasetsv3.html)\n",
    "\n",
    "Have a look at that webpage, if you want an idea of the \n",
    "value of the service Hacker9b is providing. It describes what the line by line\n",
    "values for a Google ngram file are.  An extract:\n",
    "\n",
    "```\n",
    "As an example, here are the 3,000,000th and 3,000,001st lines from the a file of the English 1-grams (googlebooks-eng-all-1gram-20120701-a.gz):\n",
    "\n",
    "circumvallate   1978   335    91\n",
    "circumvallate   1979   261    91\n",
    "```\n",
    "\n",
    "This is [the\n",
    "page with just the unigram data for that release.](http://storage.googleapis.com/books/ngrams/books/20200217/eng/eng-1-ngrams_exports.html)\n",
    "\n",
    "Kaggle has released a page with links to a subset of this, which is more\n",
    "difficult to work with for building a word list, [a bigram corpus derived from\n",
    "Google data of the same date.](https://www.kaggle.com/ketchupduck/google-2grams-20200217-english-fiction)\n",
    "\n",
    "With no filtering, Hacker B9's wordlist derived from \n",
    "Google's ngram corpora contains 8 million words.  That's too many.\n",
    "The rarest \"words\" have count 40.  Here are some of those:\n",
    "\n",
    "```\n",
    "0-7923-3178-8 \n",
    "0-7923-2968-6\n",
    "0-7923-1344-5\n",
    "07-1499\n",
    "```\n",
    "So this list of 8 M words is obviously too long if you want **just**\n",
    "words, according to some  \"ordinary\" definition of word.\n",
    "Various attempts to work out a better threshhold than 8M are shown beklow.\n",
    "\n",
    "The 8M words are surely too many in many ways,\n",
    "but they may also be too few in other ways.  For example,\n",
    "\"butch\"  does not make the frequency cutoff in\n",
    "this mega-corpus, supposedly including a representative\n",
    "sample of modern texts.  Is this something\n",
    "Hackerb9 did, or is it Google's sampling method?\n",
    "An open question I havent the time to resolve.\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f36258",
   "metadata": {
    "id": "e4f36258"
   },
   "source": [
    "### Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf5887",
   "metadata": {
    "id": "9adf5887"
   },
   "source": [
    "The file in the example is a gzip file; gzip is one of several common ways\n",
    "to compress.  The discussion of compression is placed next to the\n",
    "discussion of unicode encodings, because what is compressed is some encoding\n",
    "of a file.  Therefore what is uncompressed will still need to\n",
    "be decoded before becoming unicode.  We illustrate this below.\n",
    "\n",
    "The idea is this: Unzip using `gzip` module; gievn a file path,\n",
    "`gzip.open` returns a text file handle, an iterator that\n",
    "allows us to loop through the uncompressed file contents one \n",
    "line at a time. This is a very transparent way of doing\n",
    " decompression.  `open(path)` becomes `gzip.open(path)`.\n",
    "\n",
    "Let's illustrate two different situations, one a little\n",
    "more complicated than the other.\n",
    "\n",
    "First suppose this file exists as a file on your\n",
    "machine.  The code looks like this;\n",
    "\n",
    "```\n",
    "line 20: gzip.open(path) returns a file stream we \n",
    "         can iterate through line by line, the\n",
    "         lines are uncompressed.\n",
    "Line 21:  for line in ... We begin iterating line by line\n",
    "Line 27:  We decode the line, which is (uncompressed) bytes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff582d",
   "metadata": {
    "id": "6dff582d",
    "outputId": "5a9bb11d-855d-44e1-f7cb-b2acc2223bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting on line 7,919,827\n",
      "7,919,826 words in Frequency Dict\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os.path\n",
    "\n",
    "####  File Location info\n",
    "wd = '/Users/gawron/Desktop/src/sphinx/python_for_ss/colab_notebooks/' \\\n",
    "         'python-for-social-science-drafts'\n",
    "# Google books\n",
    "fn,format2 = 'gwordlist/gwordlist-master/frequency-all.txt.gz',False\n",
    "# Google ngrams  The two dont seem very different apart for format\n",
    "#fn2,format2 = 'gwordlist/gwordlist-master/1gramsbyfreq.txt.gz',True\n",
    "####  End File Location info\n",
    "\n",
    "### Code parameters\n",
    "path = os.path.join(wd, fn2)\n",
    "## If for example we use threshhold freq of 5,000, then\n",
    "## a \"word\" must be used at least 5000 times to make the vocab list.\n",
    "## This is a very high number, but this is also Google Books!\n",
    "## using 40 means all words in the list will be used.\n",
    "freq_dict_40,threshhold,encoding,debug = dict(),40,'utf8',False\n",
    "#debug_stopper = 19\n",
    "debug_stopper = None\n",
    "\n",
    "with gzip.open(path) as go:\n",
    "    for (i,ln) in enumerate(go):\n",
    "        if i==0 :\n",
    "            continue\n",
    "        elif i == debug_stopper:\n",
    "            break\n",
    "        else:\n",
    "            line = ln.decode(encoding=encoding)\n",
    "            if process_line(line,freq_dict_40,threshhold,debug=debug,\n",
    "                           format2=format2):\n",
    "                continue\n",
    "            else:\n",
    "                print(f'Exiting on line {i:,}')\n",
    "                break\n",
    "print(f'{len(freq_dict_40):,} words in Frequency Dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085f2bf",
   "metadata": {
    "id": "b085f2bf",
    "outputId": "3c82c6be-4bf6-473b-b4c5-c4bcdaeef2e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "838432998263"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(freq_dict_40.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b59d3",
   "metadata": {
    "id": "c10b59d3"
   },
   "source": [
    "Situation 2.  The file is on the web.\n",
    "\n",
    "The problem is the gzip module wants\n",
    "to be handed either a string which gives\n",
    "the path to a file in the current file\n",
    "system, or a file like object, which\n",
    "provides a binary data stream to an already opened file.\n",
    "How do we connect this pipeline to a stream \n",
    "from the web?\n",
    "\n",
    "Solution.  We use `urlib.urlopen` to\n",
    "download the raw bytes from the file on the web.\n",
    "Then we use the `BytesIO` module to go from\n",
    "raw bytes to a file like object containing\n",
    "those bytes, suitable for a `gzip.GzipFile`\n",
    "instance to decompress.\n",
    "\n",
    "```\n",
    "Line 8: resp (short for response) is a stream\n",
    "Line 11: resp.read() is the downloaded bytes.  The sad thing is we now load\n",
    "         the entire compressed file into memory all at once. That's one problem\n",
    "         with this solution.  No workaround found yet.\n",
    "Line 11: bts is an iterator, a file like object (file stream)\n",
    "         that we can pass to the gzip.GzipFile class.  It is\n",
    "         still binary data. You can't iterate line by line!\n",
    "line 12: The gzip.GzipFile instance consumes bts to create an iterable (gzipfile) that \n",
    "         is uncompressed text bytes, and can be iterated through line by line.\n",
    "         So as we loop we uncompress. Note the uncompressed file does\n",
    "         not have to reside in memory all at once.\n",
    "Line 13:  We begin iterating line by line\n",
    "Line 17:  We decode ln, which is a line of(uncompressed) bytes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362fcb39",
   "metadata": {
    "id": "362fcb39",
    "outputId": "bab27403-dc87-42b8-c440-e8aed1d4a559",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting on line 676232\n",
      "676,231 words in Frequency Dict\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "# Must visit raw version of the github repository for single file downloads\n",
    "raw_github = 'https://raw.githubusercontent.com/hackerb9/' \\\n",
    "             'gwordlist/master/frequency-all.txt.gz'\n",
    "# The Google unigram min freq is 40.  Let's try 5000\n",
    "freq_dict_5000,threshhold = dict(),5_000\n",
    "resp = urlopen(raw_github)\n",
    "\n",
    "# Create a FileLike Object usable by a Gzipfile instance\n",
    "with BytesIO(resp.read()) as bts:\n",
    "    with gzip.GzipFile(fileobj=bts) as gzipfile:\n",
    "        for (i,ln) in enumerate(gzipfile):\n",
    "            if i==0:\n",
    "                continue\n",
    "            else:\n",
    "                line = ln.decode(encoding=encoding)\n",
    "                if process_line(line,freq_dict_5000,threshhold):\n",
    "                    continue\n",
    "                else: \n",
    "                    print(f'Exiting on line {i}')\n",
    "                    break\n",
    "\n",
    "def pfreq(wd,freq_dict=freq_dict_40):\n",
    "    print(f'{freq_dict[wd]:,}')\n",
    "\n",
    "print(f'{len(freq_dict_5000):,} words in Frequency Dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e47fc",
   "metadata": {
    "id": "225e47fc",
    "outputId": "e57f4b03-dd2a-4253-a6ed-22301e4a2048"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676231"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_dict_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26cbbec",
   "metadata": {
    "id": "d26cbbec",
    "outputId": "470cd613-6119-401e-e6c5-795355224b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454,777,905\n",
      "312,387,136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfreq('men'),pfreq('women')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f84066",
   "metadata": {
    "id": "93f84066"
   },
   "source": [
    "### Eyeballing the rarest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5432d94c",
   "metadata": {
    "id": "5432d94c"
   },
   "outputs": [],
   "source": [
    "#il = sorted(freq_dict_40.items(),key=lambda x:x[1], reverse=True)\n",
    "il = freq_dict_40.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9285927",
   "metadata": {
    "id": "c9285927",
    "outputId": "2b9a79de-78ca-4dca-a458-0b5640bc41b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7919827,\n",
       " 2852598,\n",
       " [('acciderc', 40),\n",
       "  ('accesj', 40),\n",
       "  ('accepte_', 40),\n",
       "  ('acceptance.This', 40),\n",
       "  (\"a'ccept\", 40),\n",
       "  ('accempany', 40),\n",
       "  ('accedervi', 40),\n",
       "  ('acceae', 40),\n",
       "  ('acagainst', 40),\n",
       "  ('abuse.f', 40),\n",
       "  ('abundantlyclear', 40),\n",
       "  ('abtiut', 40),\n",
       "  ('absurdissimas', 40),\n",
       "  ('absurd.f', 40),\n",
       "  ('absoVOL', 40),\n",
       "  ('absolverc', 40),\n",
       "  ('absoluttly', 40),\n",
       "  ('absolte', 40),\n",
       "  ('abscindatur_.', 40),\n",
       "  ('abrupl', 40),\n",
       "  ('abrasionem', 40),\n",
       "  ('above.That', 40),\n",
       "  ('aboveshown', 40),\n",
       "  ('aboveone', 40),\n",
       "  ('aboutedly', 40),\n",
       "  ('abouli', 40),\n",
       "  ('aborderait', 40),\n",
       "  ('abominablv', 40),\n",
       "  ('abolut', 40),\n",
       "  (\"Abolish'n\", 40),\n",
       "  (\"abo'\", 40),\n",
       "  ('abl.e', 40),\n",
       "  ('abieast', 40),\n",
       "  ('abgetrotzt', 40),\n",
       "  ('abgeschildert', 40),\n",
       "  ('abdi_', 40),\n",
       "  ('abcjut', 40),\n",
       "  ('ABCjCLIO', 40),\n",
       "  ('abbracciarono', 40),\n",
       "  ('abbraccia_.', 40),\n",
       "  ('abandonnat', 40),\n",
       "  ('aavant', 40),\n",
       "  ('aarred', 40),\n",
       "  ('aapers', 40),\n",
       "  ('aamission', 40),\n",
       "  ('aakcd', 40),\n",
       "  ('aafr', 40),\n",
       "  ('aadin', 40),\n",
       "  ('aaati', 40),\n",
       "  ('aaani', 40)])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_list,min_ct = il,40\n",
    "# Still getting some ighty strange \"words\" with 5K cutoff!\n",
    "filtered_il = [(wd,ct) for (wd,ct) in in_list if #ct >= min_ct and \\\n",
    "                digits.intersection(wd) == set() and \\\n",
    "                wd.istitle() == False and wd.isupper() == False]\n",
    "len(il), len(filtered_il), filtered_il[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9c76f",
   "metadata": {
    "id": "32c9c76f"
   },
   "source": [
    "### Using/ Evaluating the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5765d7f0",
   "metadata": {
    "id": "5765d7f0"
   },
   "source": [
    "Complaint:  Our WordNet-generated list of active words has a lot of odd balls on it.\n",
    "\n",
    "Let's use the Google books word list to filter our Wordnet derived list\n",
    "of active 5-letter words, built earlier in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e183a",
   "metadata": {
    "id": "eb2e183a"
   },
   "source": [
    "Here are some oddbvall WordNet 5-ltter words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01bc9a",
   "metadata": {
    "id": "4f01bc9a",
    "outputId": "121ec060-d3f3-455f-dcb1-c9d9aaed0133"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oxlip',\n",
       " 'grume',\n",
       " 'moray',\n",
       " 'felon',\n",
       " 'queue',\n",
       " 'spark',\n",
       " 'moody',\n",
       " 'knave',\n",
       " 'clank',\n",
       " 'tears',\n",
       " 'speak',\n",
       " 'flout',\n",
       " 'lxxiv',\n",
       " 'dopey',\n",
       " 'judas',\n",
       " 'snort',\n",
       " 'draba',\n",
       " 'hunch',\n",
       " 'rumba',\n",
       " 'galax']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(active_words,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c05d63",
   "metadata": {
    "id": "c0c05d63",
    "outputId": "af03d1e2-a95d-4e62-c178-720150627e3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4158"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(active_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4ad96",
   "metadata": {
    "id": "12b4ad96"
   },
   "source": [
    "How rare can our Google words be and still provide useful filtering?  Let's try 2,000 as k\n",
    "(words below frequency k in the gynormous Google Books corpus are ignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7577bd5",
   "metadata": {
    "id": "c7577bd5",
    "outputId": "6591af36-991b-4057-f693-972219993f59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3344"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_words2 = {w for w in active_words if w in freq_dict_2000}\n",
    "\n",
    "len(active_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24a01a",
   "metadata": {
    "id": "6b24a01a"
   },
   "source": [
    "Here's the bathwater:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27791971",
   "metadata": {
    "id": "27791971"
   },
   "outputs": [],
   "source": [
    "eliminated = active_words - active_words2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc7009",
   "metadata": {
    "id": "06fc7009"
   },
   "source": [
    "Let's examine this bathwater for babies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bfc946",
   "metadata": {
    "id": "51bfc946",
    "outputId": "995c2f7d-f961-4e86-8df2-e7b627bf825e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abele',\n",
       " 'sprat',\n",
       " 'hippo',\n",
       " 'gulch',\n",
       " 'picot',\n",
       " 'dotty',\n",
       " 'donna',\n",
       " 'swami',\n",
       " 'orach',\n",
       " 'sulla',\n",
       " 'lyssa',\n",
       " 'pappa',\n",
       " 'verso',\n",
       " 'redux',\n",
       " 'rubel',\n",
       " 'egest',\n",
       " 'krone',\n",
       " 'sigma',\n",
       " 'skive',\n",
       " 'minge',\n",
       " 'henry',\n",
       " 'annex',\n",
       " 'benne',\n",
       " 'savin',\n",
       " 'arras',\n",
       " 'jinks',\n",
       " 'ravel',\n",
       " 'brome',\n",
       " 'psalm',\n",
       " 'titty',\n",
       " 'butte',\n",
       " 'kraft',\n",
       " 'amort',\n",
       " 'bruin',\n",
       " 'sadhu',\n",
       " 'molly',\n",
       " 'goody',\n",
       " 'daddy',\n",
       " 'aegir',\n",
       " 'bosie',\n",
       " 'athar',\n",
       " 'chile',\n",
       " 'daisy',\n",
       " 'immix',\n",
       " 'costa',\n",
       " 'rugby',\n",
       " 'cager',\n",
       " 'midge',\n",
       " 'dhava',\n",
       " 'welch',\n",
       " 'plage',\n",
       " 'poyou',\n",
       " 'galax',\n",
       " 'knawe',\n",
       " 'testa',\n",
       " 'piper',\n",
       " 'roble',\n",
       " 'scoke',\n",
       " 'butch',\n",
       " 'lally']"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(eliminated,60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0abd16",
   "metadata": {
    "id": "2e0abd16"
   },
   "source": [
    "Some definite babies found when k = 2,000!\n",
    "\n",
    "```\n",
    "annex\n",
    "daisy\n",
    "bruin\n",
    "rugby\n",
    "psalm\n",
    "butch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295cd61b",
   "metadata": {
    "id": "295cd61b"
   },
   "source": [
    "Try lowering k to 1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bf7ab",
   "metadata": {
    "id": "7c1bf7ab",
    "outputId": "e219d1e2-f5f8-4fd5-9320-83785b632e32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4158, 3367)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_words1 = {w for w in active_words if w in freq_dict_1000}\n",
    "eliminated1 = active_words - active_words2\n",
    "len(active_words), len(active_words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5f63f",
   "metadata": {
    "id": "34f5f63f"
   },
   "source": [
    "Notice the very small increase in vocab size 3344 to 3367.\n",
    "\n",
    "The bathwater:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3bfec",
   "metadata": {
    "id": "c2c3bfec",
    "outputId": "dd6444d5-412a-4b5d-a249-6c50f9206845",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kasha',\n",
       " 'hakim',\n",
       " 'seine',\n",
       " 'xcvii',\n",
       " 'osier',\n",
       " 'tabor',\n",
       " 'selva',\n",
       " 'hello',\n",
       " 'ilama',\n",
       " 'testa',\n",
       " 'baron',\n",
       " 'coney',\n",
       " 'so-so',\n",
       " 'spawl',\n",
       " 'jemmy',\n",
       " 'tiyin',\n",
       " 'cirio',\n",
       " 'judas',\n",
       " 'pokey',\n",
       " 'benne',\n",
       " 'co-ed',\n",
       " 'lemma',\n",
       " 'pinto',\n",
       " 'cypre',\n",
       " 'burry',\n",
       " 'butch',\n",
       " 'lynch',\n",
       " 'quint',\n",
       " 'mamba',\n",
       " 'table',\n",
       " 'kempt',\n",
       " 'salat',\n",
       " 'beery',\n",
       " 'vouge',\n",
       " 'baboo',\n",
       " 'fatso',\n",
       " 'cytol',\n",
       " 'cohoe',\n",
       " 'bravo',\n",
       " 'spiff',\n",
       " 'reccy',\n",
       " 'creel',\n",
       " 'chino',\n",
       " 'clegg',\n",
       " 'corps',\n",
       " 'rouge',\n",
       " 'typic',\n",
       " 'colly',\n",
       " 'carol',\n",
       " 'lough',\n",
       " 'lanai',\n",
       " 'tubby',\n",
       " 'whish',\n",
       " 'xlvii',\n",
       " 'sally',\n",
       " 'daisy',\n",
       " 'stoke',\n",
       " 'barde',\n",
       " 'pipit',\n",
       " 'sigma']"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(eliminated1,60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf5537",
   "metadata": {
    "id": "36cf5537"
   },
   "source": [
    "Still some babies!\n",
    "\n",
    "```\n",
    "hello (hello!)\n",
    "judas (probably established a common noun)\n",
    "beery\n",
    "corps\n",
    "rouge\n",
    "tubby\n",
    "sigma\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d45a47",
   "metadata": {
    "id": "e7d45a47"
   },
   "source": [
    "How about 500?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9a33b",
   "metadata": {
    "id": "adc9a33b",
    "outputId": "5d8c614e-4923-4b58-8799-29d743552392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4158 3375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['deist',\n",
       " 'wedel',\n",
       " 'natty',\n",
       " 'pavis',\n",
       " 'marri',\n",
       " 'raita',\n",
       " 'savin',\n",
       " 'punic',\n",
       " 'scone',\n",
       " 'hi-fi',\n",
       " 'jawan',\n",
       " 'xlvii',\n",
       " 'hello',\n",
       " 'thane',\n",
       " 'viola',\n",
       " 'brail',\n",
       " 'dicer',\n",
       " 'leone',\n",
       " 'hadji',\n",
       " 'eggar',\n",
       " 'barde',\n",
       " 'agama',\n",
       " 'sabra',\n",
       " 'draba',\n",
       " 'hewer',\n",
       " 'etude',\n",
       " 'kiley',\n",
       " 'delta',\n",
       " 'south',\n",
       " 'china',\n",
       " 'gigue',\n",
       " 'benny',\n",
       " 'rouge',\n",
       " 'liger',\n",
       " 'lotte',\n",
       " 'paseo',\n",
       " 'allis',\n",
       " 'braky',\n",
       " 'annex',\n",
       " 'kalif',\n",
       " 'caddy',\n",
       " 'boner',\n",
       " 'xcvii',\n",
       " 'imaum',\n",
       " 'phlox',\n",
       " 'boney',\n",
       " 'liege',\n",
       " 'marsh',\n",
       " 'xliii',\n",
       " 'ganef',\n",
       " 'armet',\n",
       " 'eblis',\n",
       " 'momma',\n",
       " 'liman',\n",
       " 'oxbow',\n",
       " 'letch',\n",
       " 'laver',\n",
       " 'verso',\n",
       " 'deism',\n",
       " 'agave']"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_words500 = {w for w in active_words if w in freq_dict_500}\n",
    "eliminated500 = active_words - active_words500\n",
    "print(len(active_words), len(active_words500))\n",
    "random.sample(eliminated500,60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d77cdc",
   "metadata": {
    "id": "26d77cdc"
   },
   "source": [
    "Babies galore! This is getting tedious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7e541",
   "metadata": {
    "id": "24d7e541",
    "outputId": "c29f1216-ccfd-468c-a0c5-83988e54d366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4158 3381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['doyly',\n",
       " 'groak',\n",
       " 'braky',\n",
       " 'valse',\n",
       " 'boule',\n",
       " 'pommy',\n",
       " 'orpin',\n",
       " 'butch',\n",
       " 'dawah',\n",
       " 'nance',\n",
       " 'jagua',\n",
       " 'vista',\n",
       " 'pater',\n",
       " 'skeet',\n",
       " 'belle',\n",
       " 'tubby',\n",
       " 'mimer',\n",
       " 'pruno',\n",
       " 'roman',\n",
       " 'indri',\n",
       " 'whang',\n",
       " 'grail',\n",
       " 'chine',\n",
       " 'clxxx',\n",
       " 'hydra',\n",
       " 'gonzo',\n",
       " 'fleck',\n",
       " 'pavan',\n",
       " 'bruin',\n",
       " 'villa',\n",
       " 'momma',\n",
       " 'press',\n",
       " 'cypre',\n",
       " 'khadi',\n",
       " 'cohoe',\n",
       " 'pacha',\n",
       " 'argal',\n",
       " 'hogan',\n",
       " 'salat',\n",
       " 'bazar',\n",
       " 'whish',\n",
       " 'brant',\n",
       " 'leech',\n",
       " 'spiff',\n",
       " 'armet',\n",
       " 'vroom',\n",
       " 'lanai',\n",
       " 'halma',\n",
       " 'lazar',\n",
       " 'arras',\n",
       " 'codex',\n",
       " 'hippo',\n",
       " 'laver',\n",
       " 'blitz',\n",
       " 'cronk',\n",
       " 'hi-fi',\n",
       " 'henry',\n",
       " 'jello',\n",
       " 'tench',\n",
       " 'emery']"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_words100 = {w for w in active_words if w in freq_dict_100}\n",
    "eliminated100 = active_words - active_words100\n",
    "print(len(active_words), len(active_words100))\n",
    "random.sample(eliminated100,60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7600dd2",
   "metadata": {
    "id": "e7600dd2"
   },
   "source": [
    "There are many babies here:\n",
    "\n",
    "```\n",
    "bruin\n",
    "villa\n",
    "momma\n",
    "press\n",
    "gonzo\n",
    "fleck\n",
    "hi-fi\n",
    "jello (!)\n",
    "emery\n",
    "\n",
    "```\n",
    "Lowering the threshold from 100 to 40 adds exactly one word to our word list. \n",
    "\n",
    "Still babies. For example the mysteriously absent \"butch\" is still absent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4e4ed",
   "metadata": {
    "id": "f6c4e4ed",
    "outputId": "f68c4a9f-7727-4aa1-adad-98336e3b37e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4158 3382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['boule',\n",
       " 'muser',\n",
       " 'roper',\n",
       " 'islay',\n",
       " 'aleph',\n",
       " 'molly',\n",
       " 'serin',\n",
       " 'pichi',\n",
       " 'anele',\n",
       " 'hello',\n",
       " 'jawan',\n",
       " 'plyer',\n",
       " 'bosie',\n",
       " 'pruno',\n",
       " 'roach',\n",
       " 'minty',\n",
       " 'quint',\n",
       " 'stein',\n",
       " 'elver',\n",
       " 'queen',\n",
       " 'hadji',\n",
       " 'gamba',\n",
       " 'indri',\n",
       " 'paseo',\n",
       " 'nacho',\n",
       " 'sigeh',\n",
       " 'fermi',\n",
       " 'braky',\n",
       " 'bodge',\n",
       " 'tenno',\n",
       " 'dixie',\n",
       " 'sissy',\n",
       " 'nicad',\n",
       " 'boeuf',\n",
       " 'pavan',\n",
       " 'grail',\n",
       " 'leppy',\n",
       " 'blanc',\n",
       " 'gulch',\n",
       " 'blitz',\n",
       " 'vanda',\n",
       " 'deism',\n",
       " 'serge',\n",
       " 'ottar',\n",
       " 'caddy',\n",
       " 'etude',\n",
       " 'babka',\n",
       " 'liger',\n",
       " 'munja',\n",
       " 'gemma',\n",
       " 'no-go',\n",
       " 'osier',\n",
       " 'monas',\n",
       " 'terry',\n",
       " 'randy',\n",
       " 'lxvii',\n",
       " 'titan',\n",
       " 'lemma',\n",
       " 'south',\n",
       " 'lapin']"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_words40 = {w for w in active_words if w in freq_dict_40}\n",
    "eliminated40 = active_words - active_words40\n",
    "print(len(active_words), len(active_words40))\n",
    "random.sample(eliminated40,60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a140b6",
   "metadata": {
    "id": "14a140b6"
   },
   "source": [
    "An incomplete list of babies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac0c5c",
   "metadata": {
    "id": "63ac0c5c",
    "outputId": "ea305638-655f-40c2-9084-a304a0c04746",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "butch True False\n",
      "bruin True False\n",
      "villa True False\n",
      "momma True False\n",
      "press True False\n",
      "gonzo True False\n",
      "fleck True False\n",
      "hi-fi True False\n",
      "jello True False\n",
      "emery True False\n",
      "metro True False\n",
      "carol True False\n",
      "curry True False\n",
      "roach True False\n",
      "lemma True False\n",
      "hello True False\n",
      "south True False\n"
     ]
    }
   ],
   "source": [
    "for wd in \"butch bruin villa momma press gonzo fleck hi-fi \"\\\n",
    "          \"jello emery metro carol curry roach lemma hello south\".split():\n",
    "    print(f'{wd} {wd in active_words} {wd in active_words40}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7de7e0",
   "metadata": {
    "id": "0e7de7e0"
   },
   "source": [
    "Bottom line using frequency in Google ngrams as a criterion for filtering\n",
    "out some of WordNet's odder words is not working very well yet.\n",
    "\n",
    "This could be because of issues with hackerB9's filtering code.\n",
    "It could be because of Google.  Based on looking at **butch** it appears to be \n",
    "something about hackerB9's methods.\n",
    "\n",
    "Facts below;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449bd653",
   "metadata": {
    "id": "449bd653"
   },
   "source": [
    "### Why this may be a problem with the word set, not Google Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930b04f",
   "metadata": {
    "id": "2930b04f"
   },
   "source": [
    "Here's one very odd omission from the freq dict for the whole word set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c9f0d",
   "metadata": {
    "id": "688c9f0d",
    "outputId": "92903850-e7b3-4a65-d237-80575dc1c283",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'butch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-458652e97476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfreq_dict_40\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'butch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'butch'"
     ]
    }
   ],
   "source": [
    "freq_dict_40['butch']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966eba83",
   "metadata": {
    "id": "966eba83"
   },
   "source": [
    "If it's a Google problem, that word should just have a very small frequency in the original set.\n",
    "\n",
    "Let;s check.\n",
    "\n",
    "First some background numbers.  Using *the* with pct figure to calculate size of corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba3a51",
   "metadata": {
    "id": "a7ba3a51"
   },
   "source": [
    "This entry was created in debug mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2d498",
   "metadata": {
    "id": "40f2d498",
    "outputId": "0cf6980c-b560-461b-b2c8-849d521ca858",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2', '53,097,503,134', '5.937009%', '12.189017%')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict_00['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78407843",
   "metadata": {
    "id": "78407843",
    "outputId": "4d8ba9ae-8f02-429c-d93b-7fb88ec75fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894,349,050,597.9451\n"
     ]
    }
   ],
   "source": [
    "# 53097503134 = .05937 x N\n",
    "# 53097503134/.05937 = Nm\n",
    "N = 53_097_503_134/.05937\n",
    "# Nearly 1 trillion words\n",
    "print(f'{N:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc68bed",
   "metadata": {
    "id": "7bc68bed"
   },
   "source": [
    "Another way, another answer.  He gives a reason for this discrepancy, filtering,\n",
    "for example, punctuation.  I believe\n",
    "the percentages are closer to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5297210",
   "metadata": {
    "id": "d5297210",
    "outputId": "1d785804-184c-4840-a475-40d6096bdccb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "838432998263"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(freq_dict_40.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a12e7",
   "metadata": {
    "id": "219a12e7"
   },
   "outputs": [],
   "source": [
    "# The frequency of \"butch\" in its peak year (c. 1998) \n",
    "# according to the Google Ngrams viewer\n",
    "X = .0000414287 * .01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afad471",
   "metadata": {
    "id": "8afad471"
   },
   "source": [
    "The frequency of X, the max freq of *butch*, were maintained throughout all the years in the corpus (N is the aggregate of all texts for all years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890593d8",
   "metadata": {
    "id": "890593d8",
    "outputId": "1872c282-dacc-4aa2-d0b6-19ca62c351ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370517.18512507086"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N*X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29657754",
   "metadata": {
    "id": "29657754"
   },
   "source": [
    "The frequency if Y, the min freq of *butch*, were maintained throughout all the years in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3297e5",
   "metadata": {
    "id": "ac3297e5",
    "outputId": "4968e2e5-fc08-40a3-8dad-3828c93e436a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16456.02253100219"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = .00000184 * .01\n",
    "N*Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdab7b6",
   "metadata": {
    "id": "6fdab7b6"
   },
   "source": [
    "Both numbers are well above the Google Ngram curoff of 40.  So this argues there is something going on with hackerb9's filtering methods which has filtered out the word **butch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653858a",
   "metadata": {
    "id": "9653858a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "wordnet_unicode_compression_word_lists.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
