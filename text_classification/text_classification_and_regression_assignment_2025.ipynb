{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1147747",
   "metadata": {},
   "source": [
    "## Millikan electron experiment (Regression, Newman Computational Physics, Exc 3.8  p.123 )\n",
    "\n",
    "This is worth 3 points.\n",
    "\n",
    "The whole source of excitement about the photolectric effect was that it is governed by a very simple equation, whose verification turned out to be very strong evidence that light consists of particles (let's call them photons).\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{V} = \\frac{h}{e} \\nu - \\Phi\n",
    "$$\n",
    "\n",
    "Read this as V (Voltage) = $\\frac{h}{e}$ (Planck's Constant divided by the charge of an electron) times\n",
    "$\\nu$ (the frequency of the light) minus $\\Phi$ (known as the work function of the surface).\n",
    "So an electron gets kicked up from the surface by a photon, loses a little energy ($\\Phi$)\n",
    "because it takes some work to pry it loose, and then has energy (V) proportional to the\n",
    "light's wavelength.\n",
    "\n",
    "The experimental setup was such that the work function of the surface was constant throughout  the experiment.\n",
    "\n",
    "Here is Millikan's data, measuring the voltage produced by single photons of light\n",
    "at various frequencies. He won the Nobel Prize for these results, which confirmed\n",
    "predictions made in Einstein's 1905 work on the photoelectric effect, for which Einstein also\n",
    "won the Nobel Prize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3212d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "github_url ='https://raw.githubusercontent.com/gawron/python-for-social-science/master/'\n",
    "data_path = 'text_classification/data/'\n",
    "url_dir = github_url + data_path\n",
    "data_file = \"millikan.txt\"\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "#                    D a t a\n",
    "#\n",
    "###############################################################################\n",
    "e_charge = 1.602e-19\n",
    "df = pd.read_csv(url_dir + data_file,header=None,delimiter= \" \",names=(\"Frequency\",\"Voltage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62948822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Voltage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.487400e+14</td>\n",
       "      <td>0.53090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.931000e+14</td>\n",
       "      <td>1.08420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.430700e+14</td>\n",
       "      <td>1.27340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.219300e+14</td>\n",
       "      <td>1.65980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.607400e+14</td>\n",
       "      <td>2.19856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.184000e+15</td>\n",
       "      <td>3.10891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Frequency  Voltage\n",
       "0  5.487400e+14  0.53090\n",
       "1  6.931000e+14  1.08420\n",
       "2  7.430700e+14  1.27340\n",
       "3  8.219300e+14  1.65980\n",
       "4  9.607400e+14  2.19856\n",
       "5  1.184000e+15  3.10891"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81cb910",
   "metadata": {},
   "source": [
    "1.  The equation says the Frequency and the voltage should be related by a linear relation. Assuming the data do obey this linear relation, use Scikit learn's linear regression implementation to find $\\frac{h}{e}$ and $\\Phi$ from these data points.  \n",
    "\n",
    "2.  Then given that the charge of an electron  is $1.602e^{-19}$, estimate Planck's constant. This is one of the most important constants in Physics.  Look it up to see how good your estimate is.  Compute your percentage of error.\n",
    "\n",
    "3.  Draw a scatter plot of the 6 data points.  Use the results of your linear regression to plot the  line that is the best fit to these points in the same plot.  You will need to make a fairly arbitrary decision about the aspect of the figure (ratio of height to width), since the x and y axis represent different units with very different orders of magnitude.\n",
    "\n",
    "4.  What is the value of $\\Phi$?  Hint:  It's work.  It shouldn't be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87581c8",
   "metadata": {},
   "source": [
    "## 1985 Auto Imports Database  Exercise  (Regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa83a09",
   "metadata": {},
   "source": [
    "This problem is worth 7 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560b278",
   "metadata": {},
   "source": [
    "UCI Data sets\n",
    "\n",
    "Schlimmer, J. (1985). Automobile Dataset. UCI Machine Learning Repository. https://doi.org/10.24432/C5B01C.\n",
    "\n",
    "-- Creator/Donor: Jeffrey C. Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)\n",
    "-- Date: 19 May 1987\n",
    "-- Sources:\n",
    "\n",
    "     1) 1985 Model Import Car and Truck Specifications, 1985 Ward's\n",
    "        Automotive Yearbook.\n",
    "     2) Personal Auto Manuals, Insurance Services Office, 160 Water\n",
    "        Street, New York, NY 10038 \n",
    "     3) Insurance Collision Report, Insurance Institute for Highway\n",
    "        Safety, Watergate 600, Washington, DC 20037\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b037b83b",
   "metadata": {},
   "source": [
    "This exercise involves a technique called **cross-validation**: k-fold cross-validation means\n",
    "that you do k different train test splits, each time using a different portion of the data for the\n",
    "test set.  The way this typically works is that by the end of the k splits, each item in the dataset\n",
    "has had a chance to be in the test set (that's the way the scikit_learn `KFold` function works).\n",
    "Intuitively this means that if there are items in the data that are particularly hard, the learner will\n",
    "get a crack at predicting those items without having seen them during training. \n",
    "\n",
    "Here is how cross validation on a small dataset of size 7 works if we require there to \n",
    "be 7 splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57559705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: indices = [1 2 3 4 5 6]\n",
      "  Test:  indices = [0]\n",
      "Fold 1:\n",
      "  Train: indices = [0 2 3 4 5 6]\n",
      "  Test:  indices = [1]\n",
      "Fold 2:\n",
      "  Train: indices = [0 1 3 4 5 6]\n",
      "  Test:  indices = [2]\n",
      "Fold 3:\n",
      "  Train: indices = [0 1 2 4 5 6]\n",
      "  Test:  indices = [3]\n",
      "Fold 4:\n",
      "  Train: indices = [0 1 2 3 5 6]\n",
      "  Test:  indices = [4]\n",
      "Fold 5:\n",
      "  Train: indices = [0 1 2 3 4 6]\n",
      "  Test:  indices = [5]\n",
      "Fold 6:\n",
      "  Train: indices = [0 1 2 3 4 5]\n",
      "  Test:  indices = [6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6],[7,5]])\n",
    "y = np.array([1, 2, 1, 2, 1, 2, 2])\n",
    "rs = KFold(n_splits=7)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(rs.split(X)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: indices = {train_index}\")\n",
    "    print(f\"  Test:  indices = {test_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425edf9",
   "metadata": {},
   "source": [
    "**General idea of the assignment: Reproduce the regression result from Kibler et al.**\n",
    "\n",
    "Kibler, D., Aha, D. W., & Albert, M. (1989).  Instance-based prediction of real-valued attributes.  *Computational Intelligence 5* pp.  51--57.\n",
    "\t\n",
    "**Predicted price of car using all Numeric and Boolean attributes.**\n",
    "\n",
    "Their Method: an instance-based learning (IBL) algorithm derived from a localized k-nearest neighbor algorithm. You won't build the IBL system.  You will just implement the system they  compared it with: a standard  linear regression model. Note that they discarded all instances with missing attribute values.  You should do the same.  This resulted in a training set of 159 instances.\n",
    "\n",
    "The Idea: Train 159 models. In each case, train with 158 instances.  Predict the price of  the  159th instance and save that in an array called `predictions`.  Each time you train,. a different row of the data should be held out. to be used as test data; scikit learn's KFold should help.\n",
    "\n",
    "\n",
    "Results: \n",
    "\n",
    "Percent Average Deviation Error, also known as MAPE (\"mean absolute percentage error\")  of Prediction from Actual\n",
    "\n",
    "1. 11.84% for the IBL algorithm\n",
    "2. 14.12% for the resulting linear regression equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb9caf",
   "metadata": {},
   "source": [
    "#### Attribute information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b21ef4",
   "metadata": {},
   "source": [
    " 1. symboling:                -3, -2, -1, 0, 1, 2, 3.\n",
    "2. normalized-losses:        continuous from 65 to 256.\n",
    "3. make:                     alfa-romero, audi, bmw, chevrolet, dodge, honda,\n",
    "                           isuzu, jaguar, mazda, mercedes-benz, mercury,\n",
    "                           mitsubishi, nissan, peugot, plymouth, porsche,\n",
    "                           renault, saab, subaru, toyota, volkswagen, volvo\n",
    "4. fuel-type:                diesel, gas.\n",
    "5. aspiration:               std, turbo.\n",
    "6. num-of-doors:             four, two.\n",
    "7. body-style:               hardtop, wagon, sedan, hatchback, convertible.\n",
    "8. drive-wheels:             4wd, fwd, rwd.\n",
    "9. engine-location:          front, rear.\n",
    "10. wheel-base:               continuous from 86.6 120.9.\n",
    "11. length:                   continuous from 141.1 to 208.1.\n",
    "12. width:                    continuous from 60.3 to 72.3.\n",
    "13. height:                   continuous from 47.8 to 59.8.\n",
    "14. curb-weight:              continuous from 1488 to 4066.\n",
    "15. engine-type:              dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\n",
    "16. num-of-cylinders:         eight, five, four, six, three, twelve, two.\n",
    "17. engine-size:              continuous from 61 to 326.\n",
    "18. fuel-system:              1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\n",
    "19. bore:                     continuous from 2.54 to 3.94.\n",
    "20. stroke:                   continuous from 2.07 to 4.17.\n",
    "21. compression-ratio:        continuous from 7 to 23.\n",
    "22. horsepower:               continuous from 48 to 288.\n",
    "23. peak-rpm:                 continuous from 4150 to 6600.\n",
    "24. city-mpg:                 continuous from 13 to 49.\n",
    "25. highway-mpg:              continuous from 16 to 54.\n",
    "26. price:                    continuous from 5118 to 45400."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c079a7",
   "metadata": {},
   "source": [
    "#### Missing values info\n",
    "\n",
    "Missing values are denoted by '?'. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74737672",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c48afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8555a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "#import os\n",
    "#print(os.getcwd())\n",
    "\n",
    "\n",
    "\n",
    "cols = \"symboling normalized-losses make fuel-type aspiration num-of-doors body-style\" +\\\n",
    "       \" drive-wheels engine-location wheel-base length width height curb-weight\" +\\\n",
    "       \" engine-type num-of-cylinders engine-size fuel-system bore stroke\" +\\\n",
    "       \" compression-ratio horsepower peak-rpm city-mpg highway-mpg price\"\n",
    "cols = cols.split()\n",
    "\n",
    "data_file = \"imports-85.data\"\n",
    "df = pd.read_csv(url_dir + data_file,sep=\",\",header=None,names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899f5eb",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "Build a regression model predict car price from other numeric and Boolean variables.  Here are some specific things to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b59c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>convertible</th>\n",
       "      <th>hardtop</th>\n",
       "      <th>hatchback</th>\n",
       "      <th>sedan</th>\n",
       "      <th>wagon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     convertible  hardtop  hatchback  sedan  wagon\n",
       "0              1        0          0      0      0\n",
       "1              1        0          0      0      0\n",
       "2              0        0          1      0      0\n",
       "3              0        0          0      1      0\n",
       "4              0        0          0      1      0\n",
       "..           ...      ...        ...    ...    ...\n",
       "200            0        0          0      1      0\n",
       "201            0        0          0      1      0\n",
       "202            0        0          0      1      0\n",
       "203            0        0          0      1      0\n",
       "204            0        0          0      1      0\n",
       "\n",
       "[205 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"body-style\"].str.get_dummies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d2bf6c",
   "metadata": {},
   "source": [
    "1.  Select the Numeric and Boolean attributes you will use.  Note that the data types  you get when you read in the data are inferred by pandas and they may not be correct. Think about whether the data types pandas uses for each column are appropriate for the information being represented in that column.  Also note: There are obviously no Boolean data types directly represented in the data, but you can create some Boolean type columns using `.str.get_dummies()`. For example, try executing `df[\"body-style\"].str.get_dummies()`.  This returns a Dataframe with the Boolean columns derived from the categorial `\"body_style\"` column. You use `pd.merge` to merge these new columns into `df`.\n",
    "2.  Do any data type conversions you need to do to use the attributes sensibly.  Note:\n",
    "    One of the best ways to do your type conversions is to do them while\n",
    "    reading in the data with `read_csv` (called above).  It is recommended that you \n",
    "    read the docs for function by googling \"pandas read_csv\".\n",
    "3.  Discard instances with missing values. Note that missing values are denoted by \n",
    "    '?', not a convention known to pandas.   Again \"read_csv\" can help.\n",
    "4.  Build a linear regression model to predict price. Since you are trying to reproduce \n",
    "    Kibler at al. you must do the training/test splits the way they do: Use all but one \n",
    "    row of data for training on each train-test split, with the held out row\n",
    "    used as test data. Do N train-test splits where N is the number of rows. A different\n",
    "    row should be held out each time (so you can't choose the held out row by random\n",
    "    selection).\n",
    "5.  Evaluation.  Compute root mean squared error.  Also compute Average \n",
    "    Deviation Error of Prediction from Actual (for comparison with Kibler et al.)  You can \n",
    "    use scikit_learn's `mean_absolute_percentage_error` for this.  Please pay attention to \n",
    "    the right argument order.  Call the 1D array of the predictions you made during your \n",
    "    159 train-test splits `predictions`. Conceptually, `predictions` and `df[\"price\"]` are \n",
    "    the arguments of `mean_absolute_percentage_error` (MAPE) and \n",
    "    `root_mean_squared_error` (RSME).  Note You should be able to build a linear regression\n",
    "    model that beats the 14.12% MAPE score for a regression model reported by\n",
    "    Kibler et al.\n",
    "6.  Try different feature sets. Is it sometimes a good idea to leave a feature out? \n",
    "    Yes! Report the results for your best model.  Note: you don't have to and should not\n",
    "    try all possible subsets of the columns of `df`.  Just try a model with all possible\n",
    "    numerical and Boolean columns as features and then look at ways to improve it by\n",
    "    pruning some columns/features.  You don't have to show all your experiments.\n",
    "    Just your best model. The code that produced it.  And the MAPE and RSME\n",
    "    scores for 159 predictions you made with that model (one model, two numbers).\n",
    "7.  For your best model report your most important positive feature (most likely to inflate price) and your\n",
    "    most important negative feature (most likely to lower price). Find the 5 features that matter the least\n",
    "    in determining price.\n",
    "8.  To help you interpret your results, implement and evaluate a baseline model \n",
    "    for comparison.  The baseline should always predict\n",
    "    the mean price of the entire data set.  Find\n",
    "    the MAPE and RSME for the baseline model.  How does the RSME of this baseline model\n",
    "    compare with the STD of the price column?\n",
    "9.  Optionally, try out sklearn's `linear_model.LogisticRegression` on the problem.  \n",
    "    To do this you   will need to scale the data using sklearn's\n",
    "    `preprocessing.StandardScaler`. Read [the \n",
    "    docs](https://scikit-learn.org/1.5/modules/preprocessing.html) \n",
    "    because this is implemented as an sklearn Transformer.  What effect does using \n",
    "    Logistic Regression have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b9c1ba",
   "metadata": {},
   "source": [
    "Data types as read in by `read_csv`.  Recall that columns containing strings \n",
    "will by default be given the data type `object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd7c1722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 205 entries, 0 to 204\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   symboling          205 non-null    int64  \n",
      " 1   normalized-losses  205 non-null    object \n",
      " 2   make               205 non-null    object \n",
      " 3   fuel-type          205 non-null    object \n",
      " 4   aspiration         205 non-null    object \n",
      " 5   num-of-doors       205 non-null    object \n",
      " 6   body-style         205 non-null    object \n",
      " 7   drive-wheels       205 non-null    object \n",
      " 8   engine-location    205 non-null    object \n",
      " 9   wheel-base         205 non-null    float64\n",
      " 10  length             205 non-null    float64\n",
      " 11  width              205 non-null    float64\n",
      " 12  height             205 non-null    float64\n",
      " 13  curb-weight        205 non-null    int64  \n",
      " 14  engine-type        205 non-null    object \n",
      " 15  num-of-cylinders   205 non-null    object \n",
      " 16  engine-size        205 non-null    int64  \n",
      " 17  fuel-system        205 non-null    object \n",
      " 18  bore               205 non-null    object \n",
      " 19  stroke             205 non-null    object \n",
      " 20  compression-ratio  205 non-null    float64\n",
      " 21  horsepower         205 non-null    object \n",
      " 22  peak-rpm           205 non-null    object \n",
      " 23  city-mpg           205 non-null    int64  \n",
      " 24  highway-mpg        205 non-null    int64  \n",
      " 25  price              205 non-null    object \n",
      "dtypes: float64(5), int64(5), object(16)\n",
      "memory usage: 41.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f9ffb",
   "metadata": {},
   "source": [
    "A look at the regression variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e861f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      13495\n",
       "1      16500\n",
       "2      16500\n",
       "3      13950\n",
       "4      17450\n",
       "       ...  \n",
       "200    16845\n",
       "201    19045\n",
       "202    21485\n",
       "203    22470\n",
       "204    22625\n",
       "Name: price, Length: 205, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b27b7d",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3b4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vectorize_and_fit(docs,labels,clf,**params):\n",
    "    \"\"\"\n",
    "    Given labeled data (docs, labels) and a classifier,\n",
    "    do the training test split.  Train the vectorizer and the classifier.\n",
    "    Transform the test data and return a set of preducted labels\n",
    "    for the test data,\n",
    "    \"\"\"\n",
    "    T_train,T_test, y_train,y_test = train_test_split(docs,labels)\n",
    "    tf = text.TfidfVectorizer(**params)\n",
    "    X_train = tf.fit_transform(T_train)\n",
    "    clf_inst = clf()\n",
    "    clf_inst.fit(X_train, y_train)\n",
    "    X_test = tf.transform(T_test)\n",
    "    return clf_inst.predict(X_test), y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57aeca",
   "metadata": {},
   "source": [
    "### General idea\n",
    "\n",
    "Work through the Insults Detection notebook about text classification and \n",
    "insult detection. Focus on the use of `scikit_learn`, especially the \n",
    "`TfidfVectorizer`. For this assignment you will be turning in the Python notebook (extension `.ipynb`, **not** a `.py` file).  Turn in this notebook with all the code needed to run your classifier.  If it doesn't run, your score will suffer.\n",
    "\n",
    "Try two different classifiers on the movie review data discussed below, the one used in the textbook, an SVM called `LinearSVC`, and  the Bernoulli Naive Bayes model used above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bdce15",
   "metadata": {},
   "source": [
    "### Instructions and point values\n",
    "\n",
    "1.  Be sure to get the average of at runs  least 10 runs for **both** classifiers.  2 points\n",
    "2.  Be sure to get average accuracy, precision, and recall for both classifiers on those multiple runs. You will probably find `split_vectorize_and_fit` defined above useful, but you will need to modify it.  2 points.\n",
    "3.  Discuss which of the two classifiers does better.  Discuss which metric the best classifier does the worst at and speculate as to why (this will require reviewing the definitions of precision and recall and thinking about what they mean in a movie review setting). 3 points.\n",
    "4. Do a new training/test split on the data and train and test an SVM model.  Choose one false positive and one false negative from the test set.  Call these documents $j$ and $k$ and call their functional margins $c_j$ and $c_k$ (see the SVM notebook).  Find \n",
    "\n",
    "$$\n",
    "\\frac{c_{j}}{c_{max}-c_{min}}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{c_{k}}{c_{max}-c_{min}},\n",
    "$$\n",
    "\n",
    "where $c_{max}$ and $c_{min}$ are the maximum and minimum functional margins for the training set.  Are documents $j$ and $k$ misclassified with high confidence?  Of course getting credit for this part means submitting the code you used to compute these quantities.  For the computation of functional margins, it will be convenient to relabel positive and negative classes 1 and -1 respectively. 5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ce392",
   "metadata": {},
   "source": [
    "#### Help with getting the movie reviews data.\n",
    "\n",
    "Execute the next two cells to get the movie review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c02c3c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/gawron/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1add2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "\n",
    "def get_file_strings (corpus, file_ids):\n",
    "    return [corpus.raw(file_id) for file_id in file_ids]\n",
    "\n",
    "data = dict(pos = mr.fileids('pos'),\n",
    "            neg = mr.fileids('neg'))\n",
    "\n",
    "pos_file_ids = data['pos']\n",
    "neg_file_ids = data['neg']\n",
    "\n",
    "# Get all the positive and negative reviews.\n",
    "pos_file_reviews = get_file_strings (mr, pos_file_ids)\n",
    "neg_file_reviews = get_file_strings (mr, neg_file_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f2701d",
   "metadata": {},
   "source": [
    "Each review is a string.  In principle, a list of strings like `pos_file_reviews`  can be passed to `text.TfidfVectorizer()` via the `fit_transform` method to train a vectorizer for machine learning.\n",
    "You could code that up.\n",
    "\n",
    "What you'd really like to do is use `split_vectorize_and_fit`, defined above, which does a lot of the work for you.\n",
    "\n",
    "But hold on. You have a coding problem. You don't have  a sequence of documents and labels.  Instead you have\n",
    "one sequence of positive documents  and another sequence of negative documents.  \n",
    "\n",
    "So you will need to turn those two sequences into a sequence of documents and a sequence of labels\n",
    "because that's what `split_vectorize_and_fit` wants.  You also want the doc sequence\n",
    "to contain a random mixture of positive and negative documents, because some machine\n",
    "learning algorithms are sensitive to the order in which training data is presented to\n",
    "them.\n",
    "\n",
    "The next cell does **not** do that for you.  But it illustrates an approach using \n",
    "two sets of English letters in place of two sets of English documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28749c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklm\n",
      "nopqrstuvwxyz\n",
      "('c', 'r', 'i', 's', 'z', 'b', 'g', 'h', 'q', 'o', 'w', 'j', 'p', 'y', 'd', 'n', 'x', 'l', 'k', 't', 'e', 'v', 'a', 'm', 'f', 'u')\n",
      "('f', 'l', 'f', 'l', 'l', 'f', 'f', 'f', 'l', 'l', 'l', 'f', 'l', 'l', 'f', 'l', 'l', 'f', 'f', 'l', 'f', 'l', 'f', 'f', 'f', 'l')\n"
     ]
    }
   ],
   "source": [
    "# Lets work on letters instead of documents\n",
    "# There are 2 classes, letters from the first half of the\n",
    "# alphabet ('f') and letters frmm the last half ('l')\n",
    "\n",
    "from random import shuffle\n",
    "from string import ascii_lowercase\n",
    "\n",
    "#Class 1 of the letters: the f_lets \n",
    "f_lets = ascii_lowercase[:13]\n",
    "print(f_lets)\n",
    "#Class2 of the letters: the l_lets\n",
    "l_lets = ascii_lowercase[13:]\n",
    "print(l_lets)\n",
    "\n",
    "# Now get pairs of letters and labels\n",
    "f_pairs = [(let,'f') for let in f_lets]\n",
    "l_pairs = [(let,'l') for let in l_lets]\n",
    "\n",
    "###########  Shuffling  ###########################\n",
    "# Way too orderly, the classes arent mixed yet.\n",
    "data = f_pairs + l_pairs\n",
    "shuffle(data)\n",
    "###################  Now they're shuffled! ###############\n",
    "\n",
    "# Separate the letters from their labels\n",
    "lets, lbls = zip(*data)\n",
    "print(lets)\n",
    "print(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7f741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
