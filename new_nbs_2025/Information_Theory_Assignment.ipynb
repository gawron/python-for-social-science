{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c3e652",
   "metadata": {},
   "source": [
    "## Some Information Theory background\n",
    "\n",
    "$$\n",
    "\\begin{array}{|l|l|l|l|}\n",
    "\\hline\n",
    "(a) & \\text{Information/surprisal carried by event x} & \\text{I}_{p}\\,(x) & - \\log_{2} p\\,(x) \\\\\n",
    "(b) & \\text{Entropy of distribution p} & \\text{H}(p) & \\sum\\limits_{x} \\, -p\\,(x) \\log_{2} p\\,(x)\\\\\n",
    "    & \\text{Expected information under p} &          &\\sum\\limits_{x\\in \\chi}  p\\,(x) \\, \\text{I}_{p}\\,(x) \\\\ \n",
    "(c) & \\text{Excess surprisal replacing p with q} & \\Delta_{p,q}(x) & \\log_{2} \\frac{p\\,(x)}{q\\,(x)}\\\\\n",
    "    &                         &                         & \\log_{2}  p\\,(x) - \\log_{2}  q\\,(x)  \\\\\n",
    "    &                         &                         & \\text{I}_{q}\\,(x) - \\text{I}_{p}\\,(x)\\\\\n",
    "(d) & \\text{KL Divergence of q from p}   & \\text{D}_{\\text{KL}}(p\\mid\\mid q) & \\sum\\limits_{x} \\, p\\,(x) \\,\\Delta_{p,q}\\,(x) \\\\\n",
    "\\hline\n",
    "    \\end{array}\n",
    "$$\n",
    "The entropy of $p$ is equivalent to Expected information (or surprisal) under $p$.\n",
    "Ther KL Divergence of $q$ from $p$ is equivalent to expected probability ratio \n",
    "of p to q or the expected excess surprisal when replacing p with q.\n",
    "It can be thought of as the distance of q from p, but it's better\n",
    "to call it the **divergence** of $q$ from $p$, because it's not symmetric;\n",
    "its official name is **Kullback-Leibler Divergence**;  it is also called\n",
    "**Relative Entropy** and **I-Divergence**.\n",
    "\n",
    "Example A reindeer race.  Suppose a racing handicapper $p$ rates the reindeer as follows:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l|rrc}\n",
    "               & p(x) & \\text{Odds} &\\text{I}_{p}(x) \\\\\n",
    "               \\hline\n",
    "\\text{Prancer} & 1/4 & 3:1 & 2\\\\\n",
    "\\text{Blitzen} & 1/4 & 3:1 & 2 \\\\\n",
    "\\text{Dancer} & 1/8 & 7:1 &3  \\\\\n",
    "\\text{Dasher} & 1/8  & 7:1 &3  \\\\\n",
    "\\text{Donner} & 1/16 & 15:1 & 4 \\\\\n",
    "\\text{Vixen}& 1/16  & 15:1 & 4\\\\\n",
    "\\text{Comet}& 1/16  & 15:1 & 4 \\\\\n",
    "\\text{Cupid}& 1/16  & 15:1 &4\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The unit for $\\text{I}_{p}(x)$ is **bits**.  The intuition is that if one of the favorites -- say Prancer -- wins\n",
    "that carries less surprisal (2 bits) than if one of the long shots wins (4 bits). Now for the entropy calculation\n",
    "(computing the average amount of surprise):\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "\\text{H}(p) & = & - \\,\\lbrack\\,2 \\cdot 1/4\\cdot\\log\\,1/4 \\,+\\, 2\\cdot 1/8\\cdot\\log\\, 1/8\\, +\\, 4 \\cdot 1/16\\cdot \\log\\, 1/16\\,\\rbrack\\\\\n",
    "            & = & - \\,\\lbrack\\,2 \\cdot 1/4\\cdot (-2) \\,+\\, 2\\cdot 1/8\\cdot (-3)\\, +\\, 4 \\cdot 1/16\\cdot (-4)\\,\\rbrack\\\\\n",
    "            &=&  - \\,\\lbrack\\,2 \\cdot (-1/2)\\, + \\, 2 \\cdot (-3/8) \\, +\\, 4 \\cdot (-1/4)\\,\\rbrack\\\\\n",
    "            &=& - \\,\\lbrack \\,-1 \\,+ \\,-3/4 \\,+ \\,-1\\,\\rbrack \\\\\n",
    "            &=& 2.75 \\text{ bits}\n",
    "\\end{array}\n",
    "$$ \n",
    "\n",
    "\n",
    "Suppose racing expert q tells we have complete parity: All the reindeer are equally likely to win:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "H(q) & = & -\\, \\lbrack\\, 8 \\cdot 1/8 \\cdot \\log_{2} 1/8 \\,\\rbrack\\\\\n",
    "     & = & 3 \\text{ bits}\n",
    "\\end{array}\n",
    "$$ \n",
    "\n",
    "The **uniform distribution** is always the **maximum entropy** distribution over any given\n",
    "sample space.  It represents the case in which our expert has no relevant information\n",
    "distinguishing any of the horses.  He won't stay in business long if he keeps doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0814bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(2.75)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "log2 = np.log(2)\n",
    "\n",
    "\n",
    "def dist (p):\n",
    "    \"\"\"\n",
    "    A test that this is a valid probability distribution.\n",
    "    \n",
    "    The == here should be replaced with close to (up to a tolerance).\n",
    "    \"\"\"\n",
    "    return ((p>=0) & (p <=1)).all() and p.sum() ==1\n",
    "\n",
    "def H (p):\n",
    "    \"\"\"\n",
    "    p is a 1D array representing a valid probability distribution.\n",
    "    \n",
    "    Return the entropy of p.\n",
    "    \"\"\"\n",
    "    assert dist(p), \"p must be a valid probability distribution!\"\n",
    "    I = - np.log(p)\n",
    "    # np.log is natural log; convert to bits\n",
    "    return (p * I).sum()/log2\n",
    "\n",
    "p = np.array([.25,.25,.125,.125,.0625,.0625,.0625,.0625,])\n",
    "print(dist(p))\n",
    "H(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc5a72",
   "metadata": {},
   "source": [
    "## Cross-entropy (Model evaluation)\n",
    "\n",
    "We're interesting in evaluating various models of the world with respect to the\n",
    "**truth**.  Or maybe -- to make it sound like this is a **doable** thing --\n",
    "in evaluating various models with respect to the accuracy of the predications they make.\n",
    "Since models are often used to assign probabilities to things, **Information Theory**\n",
    "can be of help.\n",
    "\n",
    "The original definition of information-theoretic definition of cross-entropy comes \n",
    "from (Shannon and Weaver), where it\n",
    "used as measure of the expected amount of surprise if probability distribution $\\hat{y}$ is used in place\n",
    "of $y$.  \n",
    "\n",
    "$$\n",
    "%\\begin{array}{lcl}\n",
    "\\text{H}\\,(\\hat{y},\\,y)=\\text{CE}\\,(\\hat{y},\\,y) =  - \\sum\\limits_{x \\in \\chi}  y\\,[x] \\log \\hat{y}\\,[x]\\\\\n",
    "%\\end{array}\n",
    "$$\n",
    "\n",
    "So in our model evaluation task,  we can think of $y$ as \"the truth\" and\n",
    "$\\hat{y}$ as our model's estimate of the truth, and $\\text{CE}\\,(\\hat{y},\\,y)$\n",
    "weights the surprise-value of each $x$ (as given by $\\hat{y}$) with its true probability\n",
    "(as given by $y$).\n",
    "\n",
    "From which it follows that\n",
    "\n",
    "$$\n",
    " \\text{CE}\\,(\\hat{y},\\,y)=               \\sum\\limits_{x \\in \\chi}  y\\,[x] \\,\\text{I}_{\\hat{y}} [x]\\\\\n",
    "%                       &=& \\text{I}\\,(y) + \\text{D}_{\\text{KL}}(\\hat{y}\\mid\\mid y) \n",
    "$$\n",
    "\n",
    "or the expectation (according to $y$) of the information assigned by $\\hat{y}$.  And it is an interesting\n",
    "exercise to show that \n",
    "\n",
    "$$\n",
    "\\text{CE}\\,(\\hat{y},\\,y)= \\text{H}(y) +  \\text{D}_{\\text{KL}}(\\hat{y} \\mid\\mid y) \n",
    "$$\n",
    "\n",
    "That is, the cross entropy is the sum of of the entropy of $y$ and the divergence\n",
    "of $\\hat{y}$ from $y$.  Since entropy and divergence are always non-negative,\n",
    "that means the cross-entropy of $y$ with any other distribution is\n",
    "always strictly greater than the entropy of $y$. That makes sense:\n",
    "modeling the true distribution $y$ with any other distribution ought\n",
    "generate more surprises.\n",
    "\n",
    "In LLM pre-training we concentrate on predicting the **next** word\n",
    "given all the previous words. We are going to use **cross-entropy** as our loss\n",
    "function.  But what do we use for $y$ (the correct distribution)?  We use the data\n",
    "itself. We assume the correct distribution has perfect knowledge of what\n",
    "the next word is given the context of all the previous words, so\n",
    "for any given word $w$ in its context, $y[w]=1$. Therefore,\n",
    "all the terms in the sum above are 0 except for the term for the actual next word,\n",
    "and we can write the cross entropy (which we'll call\n",
    "$\\text{Loss}_{\\text{CE}}$) as:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{CE}}\\,(\\hat{y}_{t},\\,y_{t}) =   \\log \\hat{y}\\,[w_{t+1}]\n",
    "$$\n",
    "\n",
    "For example, suppose at word position $t$, $\\hat{y}\\,[w_{t+1}] = .25$ (the model assigns the\n",
    "next word a probability of 1/4).  Then the loss is:\n",
    "\n",
    "$$\n",
    "-\\,\\log_{2}(1/4) = - (-2) = 2\n",
    "$$\n",
    "\n",
    "On the other hand, suppose it is 3/8:\n",
    "\n",
    "$$\n",
    "-\\,\\log_{2}(3/8) \\approx - (-1.415) = 1.415\n",
    "$$\n",
    "\n",
    "So the higher the probability the model assigns to the next word, the less the loss. To achieve a loss\n",
    "of 0 it must assign to each succeeding word a probability of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a6c36",
   "metadata": {},
   "source": [
    "##  Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2081e",
   "metadata": {},
   "source": [
    "**Exc 1**.  Compute the entropy of the distribution of heads and tails for a fair coin (p(h)=.5\n",
    "    and p(t) = .5). [H(p) = 1] **Done for you.**\n",
    "\n",
    "**Exc 2**. Compute the entropy of the distribution of heads and Tails for a biased coin (p(h)=.75).\n",
    "    [H(p) $\\approx$ .811]  **Done for you.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "58158958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exc 1 code H defined above\n",
    "H(np.array([.5,.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2e7c9d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8112781244591328)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exc 2 code\n",
    "H(np.array([.75,.25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d622a9",
   "metadata": {},
   "source": [
    "**Exc 3a.** Plot H(p) for coins ranging from p(H)=0 to p(H) =1. The plotting code is easy. What's more interesting is producing the x- and y-values you are going to plot. Approach this as an exercise in array programming.  Produce an array of probability distributions.  Apply H to that array using `np.apply_along_axis` to get a 1D array of H-values.\n",
    "\n",
    "**Exc 3b.**  Where is the maximum value for H(p)?\n",
    "\n",
    "[Place an answer to 3b) here.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8dc4c",
   "metadata": {},
   "source": [
    "##  Your plotting code answer (3a) goes in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d703efc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'H(x)')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "(fig, ax ) = plt.subplots(1,1,figsize=(6,6))\n",
    "ax.set_title(\"H(x)\")\n",
    "\n",
    "## A block of plotting code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbddfd6",
   "metadata": {},
   "source": [
    "4.  Write a python function that computes KL-Divergence.  Compute the divergence of expert $q$ from \n",
    "    expert $p$ in the reindeer handicapping example above (that is, $q$ is the \"model\" and $p$ is the truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d133d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "log2 = np.log(2)\n",
    "\n",
    "def KLD (p,q):\n",
    "    assert dist(p), \"p must be a valid probability distribution!\"\n",
    "    assert dist(q), \"q must be a valid probability distribution!\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "p = np.array([.25,.25,.125,.125,.0625,.0625,.0625,.0625,])\n",
    "q = np.ones((8,))*(1/8)\n",
    "#KLD(p,q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea70a0",
   "metadata": {},
   "source": [
    "5.  Write a python function that computes Cross entropy.  Compute the cross entropy of expert $q$ from \n",
    "    expert $p$ in the reindeer handicapping example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b21b491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "def CE (p,q):\n",
    "    assert dist(p), \"p must be a valid probability distribution!\"\n",
    "    assert dist(q), \"q must be a valid probability distribution!\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "print(CE(p,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea672813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
