{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e17f15d",
   "metadata": {},
   "source": [
    "## The XOR example (Fig. 6.6)\n",
    "\n",
    "Here is an implementation of the xor Neural Net in Python using the network drawn in Ch. 6 in Figure 6.6.\n",
    "\n",
    "Work through the discussion that follows the code,  then answer the questions at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ba63f",
   "metadata": {},
   "source": [
    "The `xor` function in the cell below implements the xor NN in Fig. 6.6 (the same weights,\n",
    "the same number of layers, but only one ReLU unit, which seems to be all that's\n",
    "needed).\n",
    "\n",
    "The `xor2` function in the cell below is equivalent, but uses\n",
    "python arrays (computational implementations of the matrices in your \n",
    "Linear Algebra class) to contain and apply the weights and biases.  The transition\n",
    "from `xor` to `xor2` requires some minor changes to input and output illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23400a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    \"\"\"\n",
    "    Note: if z is an array or sequence, this will broadcast 0 to the same shape and\n",
    "    zero out the negative elements, as desired for ReLU.\n",
    "    \n",
    "    Example: np.maximum([1,-1],0) returns [1,0]\n",
    "    \"\"\"\n",
    "    return np.maximum(z,0)\n",
    "    # The following variant works only for scalar z\n",
    "    #return max (z,0)\n",
    "    \n",
    "#######################################################\n",
    "#\n",
    "#  X O R    N e u r a l    N e t  (Version 1)\n",
    "#\n",
    "#######################################################\n",
    "\n",
    "# Map to hidden layer\n",
    "w1 = np.array([1,1])\n",
    "w2 = np.array([1,1])\n",
    "b = np.array([0,-1])\n",
    "# Map to output layer\n",
    "o = np.array([1,-2])\n",
    "b2 = 0\n",
    "\n",
    "def xor (x,verbose=0):\n",
    "    \"\"\"\n",
    "    The caption of Fig. 6.6 says to use 3 ReLUs but one \n",
    "    ReLU on the input layer implements two of those.\n",
    "    \n",
    "    And a ReLU on the output layer is unneeded.\n",
    "    \"\"\"\n",
    "    # The hidden layer:  2 dot products with biases added\n",
    "    #h[0] = w1[0]*x[0] + w1[1]*x[1] + b[0]\n",
    "    #h[1] = w2[0]*x[0] + w2[1]*x[1] + b[1]\n",
    "    h = np.array([w1.dot(x), w2.dot(x)]) + b\n",
    "    h = ReLU(h)\n",
    "    if verbose > 1:\n",
    "        print(\"*** x: \",x[0],x[1])\n",
    "    if verbose > 0:\n",
    "        print(\"*** h: \",h[0],h[1])\n",
    "    # (o[0]*h[0]) + (o[1*h[1]) + b2\n",
    "    return  o.dot(h) + b2\n",
    "\n",
    "\n",
    "#######################################################\n",
    "#\n",
    "#  X O R    N e u r a l    N e t  (Matrix Version)\n",
    "#\n",
    "#######################################################\n",
    "# A 2D array whose rows are w1 and  w2\n",
    "# b will be unchanged\n",
    "H = np.array([w1,w2])\n",
    "# Output layer\n",
    "# O is 1x2 2D array whose single row is 1D array o\n",
    "# This way all our LEARNED parameters are in 2D arrays.  \n",
    "# b2 is unchanged.\n",
    "O = np.array([o])\n",
    "\n",
    "def xor2 (x,verbose=0,linear=False):\n",
    "    \"\"\"\n",
    "    This version represent the hidden layer weights +bias and\n",
    "    the output weights + bias as matrices.\n",
    "    \"\"\"\n",
    "    # Hidden layer matrix\n",
    "    h = H@x + b\n",
    "    if not linear:\n",
    "        # Add the teensy bit of non linearity here\n",
    "        h = ReLU(h)\n",
    "    if verbose > 1:\n",
    "        print(\"*** x: \",x[0],x[1])\n",
    "    if verbose > 0:\n",
    "        print(\"*** h: \",h[0],h[1])\n",
    "    return O@h + b2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "df296d31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** x:  1 1\n",
      "*** h:  2 1\n",
      "0\n",
      "*** x:  1 0\n",
      "*** h:  1 0\n",
      "1\n",
      "*** x:  0 1\n",
      "*** h:  1 0\n",
      "1\n",
      "*** x:  0 0\n",
      "*** h:  0 0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Our data is generally also going to be in an array\n",
    "X = np.array([[1,1],[1,0],[0,1],[0,0]])\n",
    "# Verbose=2 prints out x (the input) and the hidden layer output\n",
    "for x in X:\n",
    "    # Iterate through the ROWS of X\n",
    "    print(xor(x,verbose=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d878be4",
   "metadata": {},
   "source": [
    "The function  `xor2` is equivalent in behavior \n",
    "to `xor` but outputs a 1D array with shape (1,) instead of a scalar,\n",
    "\n",
    "Have a look at its definition and make sure you understand how it's equivalent, because it is written\n",
    "much more in the matrix-programming style that is typical for NNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c5238289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** x:  1 1\n",
      "*** h:  2 1\n",
      "[0]\n",
      "*** x:  1 0\n",
      "*** h:  1 0\n",
      "[1]\n",
      "*** x:  0 1\n",
      "*** h:  1 0\n",
      "[1]\n",
      "*** x:  0 0\n",
      "*** h:  0 0\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "for x in X:\n",
    "    print(xor2(x,verbose=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b10a37c",
   "metadata": {},
   "source": [
    "We need our ReLU to get the right result.  Here's what happens if we take it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1e58daf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** x:  1 1\n",
      "*** h:  2 1\n",
      "[0]\n",
      "*** x:  1 0\n",
      "*** h:  1 0\n",
      "[1]\n",
      "*** x:  0 1\n",
      "*** h:  1 0\n",
      "[1]\n",
      "*** x:  0 0\n",
      "*** h:  0 -1\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "for x in X:\n",
    "    print(xor2(x,verbose=2,linear=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407c98f",
   "metadata": {},
   "source": [
    "Because of the use of ReLU, the transformation to the first layer ($\\mathbf{x} \\mapsto \\mathbf{h}$) is not a linear map. Let's call that transformation L."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9712530",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1.  [2 points] Implement the `and` and `or` NNs shown in Fig. 6.4 as matrices and biases.  All you need to do is \n",
    "    consult the figure and define the matrix so that the code loop in the solution stub for this exercise works. \n",
    "    For example using the matrix  if  `and_M` is the matrix for `and` and `and_b` is the corresponding bias:\n",
    "    \n",
    "    ```python\n",
    "    x = [1,0]\n",
    "    and_M@x + and_b\n",
    "    ```\n",
    "    \n",
    "    returns \n",
    "    \n",
    "    ```python\n",
    "    -1\n",
    "    ```\n",
    "    \n",
    "    After applying equation 6.7 this is converted to the right value, 0.\n",
    "    Use the solution stub below to test it on all 4 combinations of 1 and 0.\n",
    " \n",
    "2.  [2 points] Define `nand` as a matrix and bias: `a nand b` ($\\text{not} (a \\& b)$) ---\n",
    "    written as $a \\mid b$ -- flips the 1s and 0s in the truth table of `&`, so\n",
    "    it behaves as follows:\n",
    "    \n",
    "    $$\n",
    "    \\begin{array}{cc|c}\n",
    "    a  &  b &  a\\mid b\\\\\n",
    "    \\hline\n",
    "    1 & 1 & 0\\\\\n",
    "    1 & 0 &  1\\\\\n",
    "    0 & 1 &  1\\\\\n",
    "    0 & 0 &  1\n",
    "    \\end{array}\n",
    "    $$\n",
    "    \n",
    "    Your `nand` matrix and `nand` bias should work just the way `and` and `or` did.\n",
    "    Use a copy of the solution stub for Exc (1) for testing.\n",
    "    \n",
    "3.  [2 points] The following  is a fact from a logic class: Using `|` for `nand` (as we did in the  previous \n",
    "    problem), and $\\vee$ for `or`, `xor` can be defined as \n",
    "\n",
    "    $$\n",
    "    (3.1) \\; a\\text{ xor } b = (a\\,\\vee \\,b) \\;\\& \\;(a \\,\\mid\\, b)\n",
    "    $$\n",
    "\n",
    "    You should check that this has the right truth table.  We have linear definitions for all the operators on \n",
    "    the right hand side ($&$, $\\vee$, $\\mid$).  Don't we now have a linear definition for `xor`?  Define a \n",
    "    function that implements the idea of definition 3.1 above, using the\n",
    "    matrices and biases for $&$, $\\vee$, and $\\mid$ that you already have.  Test it. What happens? \n",
    "    You can fix your `xor` function by asking just what values is `and` getting from `or` and `nand` and \n",
    "    adjusting them with a nonlinear activation function $\\tau$.\n",
    "    \n",
    "    \n",
    "    $$\n",
    "    (3.2)\\; a\\text{ xor } b = \\tau(a\\,\\vee \\,b) \\;\\& \\;\\tau(a \\,\\mid\\, b)\n",
    "    $$\n",
    "    \n",
    "    Hint:  The easiest answer is not to use a standard activation function\n",
    "    (like the ones in the textbook or in the pytorch `functionals` module)\n",
    "    but to choose one that makes the `or` and `nand` models return exactly\n",
    "    what they returned in Exc. 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ff15b",
   "metadata": {},
   "source": [
    "4.  **a.**  [6 points] Code up a `numpy` version of the toy language model in \n",
    "    slide 67 of the Jurasky and Martin slides for the NN chapter \n",
    "    (or, for the same diagram, see Figure 6.14 in the Jurafksy and Martin reading for the NN module).\n",
    "    This diagram defines a **language model**, a model that predicts the next word on the basis\n",
    "    of the words before it.  Language models have been our focus from the first day \n",
    "    of the course.\n",
    "    \n",
    "    To help you get started in implementing the diagram, a solution-stub cell has been provided below.  It \n",
    "    defines a vocabulary, an example to run, and a function `get_input_layer` that maps from an example \n",
    "    string to the input layer in the diagram.\n",
    "    \n",
    "    The solution stub defines your vocabulary words as\n",
    "    \n",
    "    ```python\n",
    "    vocab = np.array('a across and boy dog his run runs the yard'.split())\n",
    "    ```\n",
    "    \n",
    "    The model in the diagram uses 1-hot encoding vectors for input words.  \n",
    "    \n",
    "    The context used to predict the next word is N words long. Therefore the input to the learner is a \n",
    "    sequence of N one-hot vectors,  which means that the layer labeled `input_layer` in the diagram\n",
    "    is an NxV array (execute the code in the solution stub and the example cell below it to see this).\n",
    "    For concreteness, we assume that N =4.\n",
    "    \n",
    "    To follow the diagram, you must define  a separate 2D array `E` which contains embeddings for all\n",
    "    the words in the vocabulary.  Given the one-hot encodings, the mapping from the\n",
    "    `input_layer` to the `embedding_layer` can be partially accomplished via a matrix multiplication with `E`\n",
    "    (Fig. 6.11 in Jurafsky and Martin). This will return the 4 embeddings for the input words in a 2D array. \n",
    "    Let's call that 2D array `input_embeddings`.  To construct `embedding_layer` we concatenate  the N vectors \n",
    "    in `input_embeddings`.  To code this, use:\n",
    "    \n",
    "    ```python\n",
    "    embedding_layer =  np.concat(input_embeddings)\n",
    "    ```\n",
    "   \n",
    "    Write the NN as a function called `LM_NN` (Language Model Neural Net),\n",
    "    which takes the input layer as its argument and  returns the corresponding output layer.  \n",
    "    It should use the architecture of the NN in the diagram, with these modifications to the hyperparameters:\n",
    "    \n",
    "    **Assume the embedding dimensionality used in the embedding layer is 5 ($d$ = $5$), the diagram uses 3),\n",
    "    the hidden layer has dimensionality 9 ($d_h$ = $9$), the size of the input sequence is 4 words ($N$ = $4$, the\n",
    "    diagram uses 3).**\n",
    "    \n",
    "    You should define 2D arrays of the right shape for `E`, `W`, and `U`, as in the diagram.  \n",
    "    Initialize these  array with random numbers.  To do that, and yet have reproduceable results,\n",
    "    use the `numpy` random number generator, which supports a number of random-generation tasks:\n",
    "    \n",
    "    ```python\n",
    "    seed = 48\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    rng.random((m, n))\n",
    "    ```\n",
    "    \n",
    "    The last line returns an array of shape m x n filled with random numbers.\n",
    "    \n",
    "    In writing `LM_NN`, make your work clear by using the layer names in the diagram.  Assign appropriate values\n",
    "    for `embedding_layer`, `hidden_layer`, and `output_layer`.  The value of `input_layer` has\n",
    "    already been computed for you for an example (in the example cell following the solution stub).\n",
    "    \n",
    "    In order to define `output_layer` correctly, you will need to do a **softmax**.  You can use  \n",
    "    `scipy.special.softmax` for your softmax function or write your own.\n",
    "    \n",
    "    **b.** [1 point] For fun,  apply this utterly untrained language model to the appropriate input\n",
    "    sequence for `a boy and his`. That input has been supplied in the solution stub.\n",
    "    \n",
    "    **c.** [3 points] Write a function to compute the loss, using **Negative Log Likelihood** as your loss \n",
    "    function.  See the slides entitled *Information_theory.pdf*.  Compute the loss for the `a boy and his`-input\n",
    "    when the actual next word is *dog*.  \n",
    "    \n",
    "    **c.** [1 point] Use the output layer returned by `LM_NN` to compute the predicted next word for the \n",
    "    `a boy and his`-input.\n",
    "    \n",
    "    **d.** [1 point] If the LM predicts the next word correctly, is the loss always 0? If not, is it ever 0?\n",
    "    If the loss can ever be 0, under what circumstances can it be 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166ee4ba",
   "metadata": {},
   "source": [
    "##  Solution stub for Excs 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ed239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define M and b with the appropriate values for the Boolean operator `and` or `or` or `nand`\n",
    "# Then test your definition here. Use one copy of this cell for each Boolean\n",
    "# operator you've defined.\n",
    "M,b = ??,??\n",
    "X = np.array([[1,1],[1,0],[0,1],[0,0]])\n",
    "\n",
    "def PosU (x):\n",
    "    \"\"\"\n",
    "    This is Eqn 6.7\n",
    "    \"\"\"\n",
    "    return int(x>0)\n",
    "\n",
    "for x in X:\n",
    "        v = (M@x+b)[0]   ## Put your matrix and bias calculation here\n",
    "        # This implements the output rule in Eqn 6.7\n",
    "        o = PosU(v)\n",
    "        print(f\"{x} {v:> 2} {o:>2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18075bea",
   "metadata": {},
   "source": [
    "## Solution stub for Exc 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "34b2f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########  Vocab and utilities ##########################\n",
    "def get_input_layer (doc):\n",
    "    \"\"\"\n",
    "    This constructs the 2D one-hit encoding matrix\n",
    "    for the input doc.  It assumes the input sequence\n",
    "    length N is defined at run time.\n",
    "    \"\"\"\n",
    "    wds = doc.split()\n",
    "    M = len(wds)\n",
    "    assert M==N, f\"The input sequence must be of length {N}, not {M}!\"\n",
    "    input_layer = np.zeros((N,V))\n",
    "    for (i,wd) in enumerate(wds):\n",
    "        input_layer[i,word2index[wd]] = 1\n",
    "    return input_layer\n",
    "\n",
    "vocab = np.array('a across and boy dog his runs runs the yard'.split())\n",
    "word2index = {w:i for (i,w) in enumerate(vocab)}\n",
    "##########  Vocab and utilities ##########################\n",
    "\n",
    "\n",
    "#############  LEARNING PARAMS OF THE MODEL  ###############\n",
    "##  Your initializations of E, W, and U go here\n",
    "##  Defining these depends on certain hyperparameters\n",
    "##  discussed in the instructions, such as N and d.\n",
    "\n",
    "############################################################\n",
    "\n",
    "def LM_NN (input_layer):\n",
    "    \"\"\"\n",
    "    Given the inout `input_layer`\n",
    "    operates on globals E, W, and U\n",
    "    to produce and return the\n",
    "    output `output_layer`.\n",
    "    \"\"\"\n",
    "    ##  YOUR CODE GOES HERE\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9fa9b0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "######### Example input ###################################\n",
    "N=4\n",
    "input_doc,next_word = \"a boy and his\",\"dog\"\n",
    "input_layer = get_input_layer (input_doc)\n",
    "print(input_layer)\n",
    "###########   Example input ################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717ab23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
