{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"ngram_assignment.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 1:**  Use the ngram assignment code help notebook to find the necessary unigram and bigram counts to estimate $P(\\text{the}\\mid \\text{of})$ in the  lower-cased version of the Brown corpus. Assign the unigram count to `unigram_ct` and the bigram count to `bigram_ct`. Assign the probability value to `p_the_given_of`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "unigram_ct = ...\n",
    "bigram_ct = ...\n",
    "p_the_given_of = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 2:**  Use the ngram assignment code help notebook to find the necessary unigram and bigram counts to estimate $P(\\text{player}\\mid \\text{football})$  in the lower-cased version of the Brown corpus. Assign the unigram count to `unigram_ct_2` and the bigram count to `bigram_ct_2`. Assign the probability value to `p_player_given_football`. Next, find any additional counts you need to estimate the probability of the bigram *football player*. Assign the bigram probability to `p_football_player`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "bigram_ct_2 = ...\n",
    "unigram_ct_2 = ...\n",
    "p_player_given_football = ...\n",
    "p_football_player = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 3:**  This question refers to `p_player_given_football` and `p_football_player` computed in question 2.  According to the **Chain Rule**, dividing one of these probabilities by the other should give a third probability. Call that third probability `p_chained`.  You can estimate `p_chained` independently using counts from the Brown corpus. This will allow you to check that your work in answering question 2 agrees with the Chain Rule.   Assign one of these counts to `count_1` and the other to `count_2`; assign `count_1` divided by `count_2` to `p_chained_2`; `p_chained_2` should  approximately equal (difference less than `1e11`) `p_chained`.\n",
    "\n",
    "Note:  To facilitate automatic grading, do not use names assigned values in previous questions in answering this one.  Your count assignments should have integers on the right hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "count_1 = ...\n",
    "count_2 = ...\n",
    "p_chained_2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 4:**  Given that a word token follows *of*, how likely is it to be one of the 20 most common words to follow *of*.  In other words, find the sum of the conditional probabilities $P(\\text{x}\\mid \\text{of})$ for the 20 x most likely to follow the word *of*.  Assign the value to `top_20_of_followers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "total_of_ct = ...\n",
    "sum_of_cts_of_top_20_of_followers = ...\n",
    "top_20_of_followers = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 5:**  Using the lower-cased version of the Brown corpus, find the most common trigram that contains neither a punctuation mark nor the word \"of\".  Assign the value to `most_common_lexical_trigram`. Hint: You are allowed to import more help from `nltk` and you may use `string.punctuation` (`string` is a module in the standard Python libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "most_common_lexical_trigram  = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 6**:  Suppose in the midst of a deadly worldwide plague your research group believes it has developed a test allowing early detection of those who have contracted the virus.  You test 100 random selected subjects and then quarantine them for 14 days to see which ones have the virus.  Using $\\hat{+}$ for those who have tested positive and $+$ for those who actually have the virus, and $\\hat{-}$ for those who have tested negative and $-$ for those who are actually free of the virus, here are the results expressed in the form of what is called a **confusion matrix**:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "         &  +  &  -  \\\\\n",
    "         \\hline\n",
    "\\hat{+}  &  30 &  40 \\\\\n",
    "\\hat{-}  &  20 &  10\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Answer the following questions\n",
    "\n",
    "(a) Compute $P(\\hat{+}\\mid +)$  and assign the value to $p0$. Compute the value of  $P(+ \\mid \\hat{+})$ and assign the value to $p1$. Show your work.\n",
    "\n",
    "(b)  Look up the definitions of **precision** and **recall**.  Do either of the probabilities in (a)  measure the precision of the test?  If so, set `precison` to that value.  Otherwise, set it to `None`.  Do either of these probabilities measure the recall of the test?  If so, set `recall` to that value.  Otherwise, set it to `None`. \n",
    "\n",
    "(c) Compute the maximum likelihood estimate of  $P(+)$ using this sample and set it to $p2$.\n",
    "\n",
    "(d) Determine which is more likely for this test, a false outcome for a positive test result or a false outcome for a negative test result.  Set `p_f_negative` to the probability of false outcome given a negative test outcome and `p_f_positive` to the probability of a false outcome given a positive test result.  Show your work. Does either of these probabilities show the test is informative in some way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p0 = ...\n",
    "p1 = ...\n",
    "p2 = ...\n",
    "recall = ...\n",
    "precision = ...\n",
    "p3 = ...\n",
    "p_f_negative = ...\n",
    "p_f_positive = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    " This is a gradescope assignment.\n",
    " Write your code and compute your values in the code help NB.\n",
    " Then assign those values to the appropriate variables in the solution cell of each question in this NB.\n",
    " Upload this notebook file ('.ipynb') with your solutions to Canvas.\n",
    " Save your code help NB for discussion, but you do not need to hand it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_of_ct(unigram_ct):\n...     return unigram_ct == 36412\n>>> test_of_ct(unigram_ct)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> def test_of_the_ct(bigram_ct):\n...     return bigram_ct == 9717\n>>> test_of_the_ct(bigram_ct)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> def test_p_the_given_of(p_the_given_of):\n...     return 0.26686 < p_the_given_of < 0.26687\n>>> test_p_the_given_of(p_the_given_of)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_bigram_ct_2(bigram_ct_2):\n...     return bigram_ct_2 == 1\n>>> test_bigram_ct_2(bigram_ct_2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> def test_unigram_ct_2(unigram_ct_2):\n...     return unigram_ct_2 == 36\n>>> test_unigram_ct_2(unigram_ct_2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> def test_p_football_player(p_football_player):\n...     return 0 < p_football_player < 8.7e-07\n>>> test_p_football_player(p_football_player)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> def test_p_player_given_football(p_player_given_football):\n...     return 0.0275 < p_player_given_football < 0.028\n>>> test_p_player_given_football(p_player_given_football)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> count_1 == 36\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> count_2 == 1161192 or count_2 == 1161191\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 3.10026248889e-05 < p_chained_2 < 3.1002624889e-05\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_of_followers(top_20_of_followers):\n...     return 0.4427 < top_20_of_followers < 0.4428\n>>> test_of_followers(top_20_of_followers)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> most_common_lexical_trigram in [('the', 'united', 'states'), 'the united states', ('The', 'United', 'States'), 'The United States', ('the', 'United', 'States'), 'the United States']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> p0 == 0.6\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0.4285 < p1 < 0.4286\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> precision == p1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> recall == p0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0.6666 < p_f_negative < 0.6667\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0.5714 < p_f_positive < 0.5715\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
