{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ASSIGNMENT CONFIG\n",
    "requirements: \n",
    "    requirements.txt\n",
    "solutions_pdf: true\n",
    "export_cell:\n",
    "    instructions: \" This is a gradescope assignment.\\n Write your code and compute your values in the code help NB.\\n Then assign those values to the appropriate variables in the solution cell of each question in this NB.\\n Upload this notebook file ('.ipynb') with your solutions to Canvas.\\n Save your code help NB for discussion, but you do not need to hand it in.\"\n",
    "generate: \n",
    "    pdf: true\n",
    "    filtering: true\n",
    "    pagebreaks: true\n",
    "    zips: false\n",
    "    points_possible: 20"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q1\n",
    "points: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:**  Use the ngram assignment code help notebook to find the necessary unigram and bigram counts to estimate $P(\\text{the}\\mid \\text{of})$ in the  lower-cased version of the Brown corpus. Assign the unigram count to `unigram_ct` and the bigram count to `bigram_ct`. Assign the probability value to `p_the_given_of`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unigram_ct = 36412   # SOLUTION\n",
    "bigram_ct = 9717   # SOLUTION\n",
    "p_the_given_of = bigram_ct/unigram_ct  # SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: .5\n",
    "def test_of_ct (unigram_ct):\n",
    "    return unigram_ct == 36412\n",
    "\n",
    "test_of_ct(unigram_ct) \n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: .5\n",
    "def test_of_the_ct (bigram_ct):\n",
    "    return bigram_ct == 9717\n",
    "\n",
    "test_of_the_ct(bigram_ct) \n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: 1\n",
    "def test_p_the_given_of(p_the_given_of):\n",
    "    return 0.26686 < p_the_given_of < 0.26687\n",
    "\n",
    "test_p_the_given_of (p_the_given_of) \n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q2\n",
    "points: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**  Use the ngram assignment code help notebook to find the necessary unigram and bigram counts to estimate $P(\\text{player}\\mid \\text{football})$  in the lower-cased version of the Brown corpus. Assign the unigram count to `unigram_ct_2` and the bigram count to `bigram_ct_2`. Assign the probability value to `p_player_given_football`. Next, find any additional counts you need to estimate the probability of the bigram *football player*. Assign the bigram probability to `p_football_player`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_ct_2 = 1  # SOLUTION\n",
    "unigram_ct_2 = 36 # SOLUTION\n",
    "p_player_given_football = bigram_ct_2/unigram_ct_2 # SOLUTION\n",
    "p_football_player = bigram_ct_2/1161191   # SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: 1\n",
    "# success_message: Correct!\n",
    "\n",
    "def test_bigram_ct_2(bigram_ct_2):\n",
    "    return bigram_ct_2 == 1\n",
    "test_bigram_ct_2 (bigram_ct_2) \n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: 1\n",
    "# success_message: Correct!\n",
    "\n",
    "def test_unigram_ct_2(unigram_ct_2):\n",
    "    return unigram_ct_2 == 36\n",
    "\n",
    "test_unigram_ct_2 (unigram_ct_2) \n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: 1\n",
    "# success_message: Correct!\n",
    "\n",
    "def test_p_football_player (p_football_player):\n",
    "    return 0 < p_football_player  < 8.7e-07\n",
    "\n",
    "test_p_football_player (p_football_player) \n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: 1\n",
    "# success_message: Correct!\n",
    "\n",
    "def test_p_player_given_football (p_player_given_football):\n",
    "    return .0275 < p_player_given_football < .028\n",
    "\n",
    "test_p_player_given_football (p_player_given_football) # IGNORE\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q3\n",
    "points: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:**  This question refers to `p_player_given_football` and `p_football_player` computed in question 2.  According to the **Chain Rule**, dividing one of these probabilities by the other should give a third probability. Call that third probability `p_chained`.  You can estimate `p_chained` independently using counts from the Brown corpus. This will allow you to check that your work in answering question 2 agrees with the Chain Rule.   Assign one of these counts to `count_1` and the other to `count_2`; assign `count_1` divided by `count_2` to `p_chained_2`; `p_chained_2` should  approximately equal (difference less than `1e11`) `p_chained`.\n",
    "\n",
    "Note:  To facilitate automatic grading, do not use names assigned values in previous questions in answering this one.  Your count assignments should have integers on the right hand side."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_1 = 36 # SOLUTION\n",
    "count_2 = 1161192 # SOLUTION\n",
    "p_chained_2 = count_1/count_2 # SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: .5\n",
    "# success_message: Correct!\n",
    "count_1 == 36 \n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: .5\n",
    "# success_message: Correct!\n",
    "count_2 == 1161192 or count_2 == 1161191\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: 1\n",
    "# success_message: Correct!\n",
    "3.10026248889e-05 < p_chained_2 < 3.10026248890e-05\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q4\n",
    "points: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:**  Given that a word token follows *of*, how likely is it to be one of the 20 most common words to follow *of*.  In other words, find the sum of the conditional probabilities $P(\\text{x}\\mid \\text{of})$ for the 20 x most likely to follow the word *of*.  Assign the value to `top_20_of_followers`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_of_ct = 36412 # SOLUTION\n",
    "sum_of_cts_of_top_20_of_followers = 16122 # SOLUTION\n",
    "top_20_of_followers = (sum_of_cts_of_top_20_of_followers/total_of_ct) # SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: 4\n",
    "\n",
    "\n",
    "def test_of_followers (top_20_of_followers):\n",
    "    return 0.4427 < top_20_of_followers < 0.4428\n",
    "    #return bool(np.isclose(top_20_of_followers,0.443,atol=.01))\n",
    "\n",
    "test_of_followers (top_20_of_followers) \n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q5\n",
    "points: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:**  Using the lower-cased version of the Brown corpus, find the most common trigram that contains neither a punctuation mark nor the word \"of\".  Assign the value to `most_common_lexical_trigram`. Hint: You are allowed to import more help from `nltk` and you may use `string.punctuation` (`string` is a module in the standard Python libraries)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_lexical_trigram  = ('the', 'united', 'states')  # SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: 4\n",
    "\n",
    "#def test_tri (most_common_lexical_trigram):\n",
    "#    return most_common_lexical_trigram in [('the', 'united', 'states'), \"the united states\",\n",
    "#                                ('The', 'United', 'States'), \"The United States\",\n",
    "#                                ('the', 'United', 'States'), \"the United States\",\n",
    "#                   \n",
    "#                                          ]\n",
    "#test_tri (most_common_lexical_trigram) \n",
    "most_common_lexical_trigram in [('the', 'united', 'states'), \"the united states\",\n",
    "                                ('The', 'United', 'States'), \"The United States\",\n",
    "                                ('the', 'United', 'States'), \"the United States\",]\n",
    "                   \n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q6\n",
    "points: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**:  Suppose in the midst of a deadly worldwide plague your research group believes it has developed a test allowing early detection of those who have contracted the virus.  You test 100 random selected subjects and then quarantine them for 14 days to see which ones have the virus.  Using $\\hat{+}$ for those who have tested positive and $+$ for those who actually have the virus, and $\\hat{-}$ for those who have tested negative and $-$ for those who are actually free of the virus, here are the results expressed in the form of what is called a **confusion matrix**:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "         &  +  &  -  \\\\\n",
    "         \\hline\n",
    "\\hat{+}  &  30 &  40 \\\\\n",
    "\\hat{-}  &  20 &  10\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Answer the following questions\n",
    "\n",
    "(a) Compute $P(\\hat{+}\\mid +)$  and assign the value to $p0$. Compute the value of  $P(+ \\mid \\hat{+})$ and assign the value to $p1$. Show your work.\n",
    "\n",
    "(b)  Look up the definitions of **precision** and **recall**.  Do either of the probabilities in (a)  measure the precision of the test?  If so, set `precison` to that value.  Otherwise, set it to `None`.  Do either of these probabilities measure the recall of the test?  If so, set `recall` to that value.  Otherwise, set it to `None`. \n",
    "\n",
    "(c) Compute the maximum likelihood estimate of  $P(+)$ using this sample and set it to $p2$.\n",
    "\n",
    "(d) Determine which is more likely for this test, a false outcome for a positive test result or a false outcome for a negative test result.  Set `p_f_negative` to the probability of false outcome given a negative test outcome and `p_f_positive` to the probability of a false outcome given a positive test result.  Show your work. Does either of these probabilities show the test is informative in some way?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p0 = 30/50  # SOLUTION\n",
    "p1 = 30/70  # SOLUTION\n",
    "p2 = 30/100 # SOLUTION\n",
    "recall = p0 # SOLUTION\n",
    "precision = p1 # SOLUTION\n",
    "p3 = 50/100  # SOLUTION\n",
    "p_f_negative = 20/30 # SOLUTION\n",
    "p_f_positive = 40/70 # SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: 1\n",
    "p0 == .6\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: a1\n",
    "0.4285 < p1 < 0.4286\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: .5\n",
    "precision == p1\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: .5\n",
    "recall == p0\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: .5\n",
    ".6666 < p_f_negative < .6667\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN TEST\n",
    "# points: .5\n",
    ".5714 < p_f_positive < .5715\n",
    "# END TEST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
